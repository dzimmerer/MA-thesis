\select@language {english}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces A sample Bayesian network with 5 nodes. (a) In the network RV $c$ is directly depended on $a$, $b$ and thus $c$ is the child of its parents $a$, $b$. The RVs $d$, $e$ are the children of $c$. Since there is a path from $a$ to $e$, $a$ is an ancestor of its descended $e$. (b) The conditional probability of $p(c |a , b)$ in a tabular form. Another variant of the conditional probability $p(c |a , b)$ is given in (c). The probability is defined by the parameters $w_{ac}$, $w_{bc}$, and due to the sigmoid activation function $\sigma $ a network with such probability functions is called sigmoid belief network.\relax }}{4}{figure.caption.3}
\contentsline {figure}{\numberline {2.2}{\ignorespaces (a) A Markov network with 5 nodes. The white node is depended on all connected nodes (blue nodes). Given its the blue nodes the white node is independent on any other node in the network. (b) Two cliques in a Markov network. The blue clique is maximal, since no vertex can be added, which is fully connected to all others in the blue clique. The green one is not maximal, since the node $x_3$ could be added.\relax }}{5}{figure.caption.4}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Sampling at discrete points $S_i$ in an simple distribution. The samples $S_i$ approximate the true density function. As more samples are drawn, the approximation will represent the function more exact.\relax }}{6}{figure.caption.5}
\contentsline {figure}{\numberline {2.4}{\ignorespaces A small section in the Brain. The neurons $a$ - $g$ are connected to other neurons in a complex network.\relax }}{8}{figure.caption.10}
\contentsline {figure}{\numberline {2.5}{\ignorespaces A schematic view of a natural neuron. Other neurons are pre-synaptic connected via the dendrites. The signals are then forwarded and accumulated in the soma and from there on via the in myelin sheath cover axon to the axon terminal and the out going synapses.\relax }}{9}{figure.caption.12}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Structure of a perceptron. The input $in(t)$ is set at the input variables $x_i$ and the multiplied with the corresponding synaptic weight $w_i$ and accumulated. In addition a threshold offset $\theta $ is added. On the sum the step-function is applied i.e. the output $out(t)$ is $1$ if the sum is greater $0$ and $0$ if the sum is smaller $0$.\relax }}{10}{figure.caption.18}
\contentsline {figure}{\numberline {2.7}{\ignorespaces The discrimination function of a perceptron. The discrimination function has the shape of a linear hyper plane in data space and it defined by the synaptic weight-vector $\textbf {w}$. It divides the data space and thus the data samples into two subspaces, the positive space $\textbf {x}^{\intercal }\textbf {w} > 0$ and the negative space $\textbf {x}^{\intercal }\textbf {w} < 0$.\relax }}{11}{figure.caption.20}
\contentsline {figure}{\numberline {2.8}{\ignorespaces A schematic multi layer perceptron with four layers.\relax }}{13}{figure.caption.24}
\contentsline {figure}{\numberline {2.9}{\ignorespaces The output of different activation functions plotted given the input.\relax }}{14}{figure.caption.26}
\contentsline {figure}{\numberline {2.10}{\ignorespaces Convolving or to be more exact a cross correlation of a $3\times 3$ image matrix with a $2\times 2$ kernel without stride and padding. The result is a $2\times 2$ feature map.\relax }}{16}{figure.caption.31}
\contentsline {figure}{\numberline {2.11}{\ignorespaces Typical architecture of a convolutional neural network with two convolution-pooling stages.\relax }}{17}{figure.caption.34}
\contentsline {figure}{\numberline {2.12}{\ignorespaces A blueprint of a Hopfield nets with 8 binary units. The units are connected with symmetric undirected connections.\relax }}{19}{figure.caption.38}
\contentsline {figure}{\numberline {2.13}{\ignorespaces A Boltzmann machine with 7 units. In contrast to a Hopfield nets, units are divided into visible and hidden/ unobserved units with stochastic activations.\relax }}{20}{figure.caption.42}
\contentsline {figure}{\numberline {2.14}{\ignorespaces A restricted Boltzmann machine is special kind of Boltzmann machine with no lateral connections in the hidden and visible layer. This eases sampling, since the visible are only dependent on the hidden units and the hidden units only on the visible units.\relax }}{22}{figure.caption.45}
\contentsline {figure}{\numberline {2.15}{\ignorespaces A temporal unrolling of the contrastive divergence algorithm with $k$ sampling step. The hidden units and the visible units are alternatingly sampled conditioned on the current state of the other.\relax }}{23}{figure.caption.47}
\contentsline {figure}{\numberline {2.16}{\ignorespaces Building up a deep belief network, by training RBMs greedily and stacking them up on top of each other. In the top layer "association" RBM the label information $y$ can be incooperated as well.\relax }}{23}{figure.caption.51}
\contentsline {figure}{\numberline {2.17}{\ignorespaces A LIF neuron as an electrical circuit. The capacitor $C_m$ corresponds to the potential at the membrane, $g_l$ the leakage potential and $E_l$ the resting potential. If the membrane potential is greater than $U_t$ a spike $\delta $ is emitted.\relax }}{24}{figure.caption.55}
\contentsline {figure}{\numberline {2.18}{\ignorespaces A Hodgkin-Huxley as an electrical circuit. The membrane potential corresponds to $C_m$, and the ion channels to $g_{Na}$, $g_{k}$, $g_{L}$ with their reversal potentials $E_{Na}$, $E_{}$, $E_{L}$.\relax }}{26}{figure.caption.57}
\contentsline {figure}{\numberline {2.19}{\ignorespaces Three samples of Ornstein\IeC {\textendash }Uhlenbeck processes. This can be seen as the membrane potential of neurons in a high conductance state (in comparison to the black doted line can be seen as a neuron without noisy input).\relax }}{28}{figure.caption.63}
\contentsline {figure}{\numberline {2.20}{\ignorespaces A membrane potential trajectory of a neuron in a high conductance state. (a) The spike train and the membrane potential. (b) The blue curve represents the actual membrane potential with refractory periods after each spike, while the red curve represents a membrane potential firing. It is apparent, that the blue curve returns the the hypothetical red potential without a "return from rest" time. \relax }}{28}{figure.caption.64}
\contentsline {figure}{\numberline {2.21}{\ignorespaces Three different PSP kernels. The green one has an alpha-shape, the blue one is exponential shaped and the red one is rectangular. \relax }}{29}{figure.caption.66}
\contentsline {figure}{\numberline {2.22}{\ignorespaces Different STDP curves observed in the Brain. The first curve shows only long term potentiation. The second show symmetric STDP. The fourth one shows a variant of the classic STDP Curve show in the center and the last one shows purely long term depression.\relax }}{31}{figure.caption.72}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces A Gibbs sampling step in a convolutional RBM. At first the visible data is convolved with the kernel to get the hidden activations. Afterwards the kernel matrix is flipped 180\textdegree . In the downward pass the hidden activations with padding are convolved with the flipped kernel to get the new visible activations.\relax }}{34}{figure.caption.73}
\contentsline {figure}{\numberline {3.2}{\ignorespaces The RBM layer architecture with probabilistic max pooling.\relax }}{34}{figure.caption.74}
\contentsline {figure}{\numberline {3.3}{\ignorespaces A spiking neural network as probabilistic model. The state $z_i$ of a neuron is set to $1$ for a time period $t_{on}$ after a spike. The complete state of the network $\textbf {z}$ is given by the individual state $(z_1, z_2, z_3, z_4)$ .\relax }}{35}{figure.caption.75}
\contentsline {figure}{\numberline {3.4}{\ignorespaces The artificial counter state $\xi $ of a neuron in a discrete time. The red state represent an active state $z_i = 1$. After a spike $\xi $ is set to the refactory period $\tau $. In each time step the refractory period counter $\xi $ is reduced by 1 until the neuron is inactive and can spike again.\relax }}{36}{figure.caption.76}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Comparison of the state probabilities with of neural sampling with Gibbs sampling in a four state Boltzmann machine. In (a) a probabilistic neuron model with a rectangular PSP is used and no discrepancies can be detected given a long enough sampling period. In (b) a more biological plausible alpha shaped kernel is used, which leads to some differences.\relax }}{36}{figure.caption.77}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Input output transfer function of a neuron in a high conductance state. The output rate is normalized by $\frac {1}{\tau _{ref}}$ to get a output probability. The transfer function is identical to shifted sigmoid function $\sigma $.\relax }}{37}{figure.caption.78}
\contentsline {figure}{\numberline {3.7}{\ignorespaces The proposed architectures to conversion between CNNs and SNNs. \relax }}{38}{figure.caption.79}
\contentsline {figure}{\numberline {3.8}{\ignorespaces An unrolled RBM with tied weights, which can be used to simulate the positive and negative phase and learn the weighs online.\relax }}{39}{figure.caption.80}
\contentsline {figure}{\numberline {3.9}{\ignorespaces Comparison between classical CD and event-based CD. In the classical k-CD (a) the weight update is determined by positive phase and negative phase, while in event-base CD (b) the weight update is determined by STDP and the sampling phase, determined by $g$.\relax }}{39}{figure.caption.81}
\contentsline {figure}{\numberline {3.10}{\ignorespaces Visualization of the phases of the event based contrastive divergence.\relax }}{40}{figure.caption.82}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Receptive fields over 4 neurons. In (a) the top layer neuron (red), is only connected to a some neurons in close vicinity to each other. In (b) four receptive fields are next to each other with a stride of two and shared weights. The top layer neurons have a similar topology as their receptive fields. Such a structure can give similar results to conventional more dimensional convolution. \relax }}{42}{figure.caption.83}
\contentsline {figure}{\numberline {4.2}{\ignorespaces A common structure of a deep belief network, build up of RBMs used for classification. The first layers transform the input data/ pixel image and can extract features and the top layer RBM is used for association the data with the correct label.\relax }}{43}{figure.caption.84}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Different converted structures of a CNN and a DBN. The CNN is converted to a spiking network, by simply using simple LIF neurons (white) (a). In the DBN (b) the LIF neurons are put in a high conductance state (black), by inserting high frequency poisson noise. The weights are scaled accordingly to fit the activations of the neurons.\relax }}{43}{figure.caption.85}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Properties of a conductance based LIF neuron, put into a high conductance state with high frequency input noise. The neuron fires with a probability of $0.5$ given no additional input (a). With input the input output transfer function approximates a sigmoid function (b). A network with such neurons is able to sample in a Boltzmann distribution similar to original distribution (c). \relax }}{45}{figure.caption.88}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Properties of a current based LIF neuron, put into a simulated high conductance state by high frequency input noise and a artificial high membrane conductance. The neuron also fires with a probability of $0.5$ given no additional input (a) and with input the input output transfer function approximates a sigmoid function (b). A network with such neurons can also to sample in a Boltzmann distribution similar to original distribution (c).\relax }}{46}{figure.caption.90}
\contentsline {figure}{\numberline {4.6}{\ignorespaces The five phases for a data sample in the adapted eCD algorithm. \relax }}{48}{figure.caption.91}
\contentsline {figure}{\numberline {4.7}{\ignorespaces A (restricted) Boltzmann machine with lateral inhibition in the top layer.\relax }}{49}{figure.caption.93}
\contentsline {figure}{\numberline {4.8}{\ignorespaces The structure of a deep belief network trained with eCD. The DBN conists of single Boltzmann machines stacked on top of each others. In contrast to analog DBNs, the Boltzmann machines are still bidirection connected, and two Boltzmann machines are stacked up by forwarding the activations of the hidden layer in the bottom RBM to the visible layer in the top RBM.\relax }}{50}{figure.caption.94}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces The two samples of the 1x4 dataset.\relax }}{55}{figure.caption.95}
\contentsline {figure}{\numberline {6.2}{\ignorespaces Samples from the stripe dataset. \relax }}{56}{figure.caption.96}
\contentsline {figure}{\numberline {6.3}{\ignorespaces Samples from the MNIST dataset.\relax }}{56}{figure.caption.97}
\contentsline {figure}{\numberline {6.4}{\ignorespaces Visualization of samples from the Poker-DVS dataset.\relax }}{57}{figure.caption.98}
\contentsline {figure}{\numberline {6.5}{\ignorespaces Visuallized filters $16 \times 16$ filters of the first layer convolutional RBM of a DBN trained on MNIST.\relax }}{58}{figure.caption.99}
\contentsline {figure}{\numberline {6.6}{\ignorespaces Activations in the features maps in a classical convolutional RBM. \relax }}{60}{figure.caption.101}
\contentsline {figure}{\numberline {6.7}{\ignorespaces Positive phase and negative phase of a diagonal stripe with 4 filters. In the first 4 images the stripe is learned, while during the last 4 images the model distribution is unlearned.\relax }}{61}{figure.caption.102}
\contentsline {figure}{\numberline {6.8}{\ignorespaces Positive phase and negative phase of a diagonal stripe with 4 filters. In the first 4 images the stripe is learned, while during the last 4 images the model distribution is unlearned.\relax }}{61}{figure.caption.104}
\contentsline {figure}{\numberline {6.9}{\ignorespaces Activations in the features maps in a as DBN converted RBM with current based LIF neurons. \relax }}{62}{figure.caption.105}
\contentsline {figure}{\numberline {6.10}{\ignorespaces $5 \times 5$ convolution filter matrices with and without lateral inhibitory connections in the top RBM layer on the stripe dataset. Whereas the filters without lateral connections look more similar the filters with lateral connections are more different and discriminative.\relax }}{62}{figure.caption.107}
\contentsline {figure}{\numberline {6.11}{\ignorespaces Weight of the first layers of the DBNs with and without convolutions.\relax }}{63}{figure.caption.109}
\contentsline {figure}{\numberline {6.12}{\ignorespaces The runtime of a learning step in dependence on the size of the Network.\relax }}{63}{figure.caption.111}
\contentsline {figure}{\numberline {6.13}{\ignorespaces Abstract architecture of the DBN for the stripe dataset.\relax }}{64}{figure.caption.113}
\contentsline {figure}{\numberline {6.14}{\ignorespaces $5 \times 5$ convolution filter matrices development during training with $1000$ samples.\relax }}{65}{figure.caption.114}
\contentsline {figure}{\numberline {6.15}{\ignorespaces Activations/ spikes in the layers of the first RBM during training. As the training progresses, the activations become more sparse. The hidden layer learns new representations for the data. The model distribution approximates the data distribution, resulting in a nearly perfect reconstruction of the input data.\relax }}{66}{figure.caption.115}
\contentsline {figure}{\numberline {6.16}{\ignorespaces Activations/ spikes in the layers of the second RBM during training. As the training progresses, the activations become more sparse. The hidden layer learns new 20 dimensional representations for the data. The model distribution approximates the data distribution, resulting in a nearly perfect reconstruction of the input data, especially of the label.\relax }}{67}{figure.caption.116}
\contentsline {figure}{\numberline {6.17}{\ignorespaces Abstract architecture of the DBN for the poker dataset.\relax }}{68}{figure.caption.118}
\contentsline {figure}{\numberline {6.18}{\ignorespaces Visualization of the convolution filters of the first layer RBM.\relax }}{68}{figure.caption.119}
\contentsline {figure}{\numberline {6.19}{\ignorespaces The accuracy and reconstruction error of the DBN. The accuracy increases to a maximal value of $90 \%$ and the reconstruction error decreases, indicating discriminative features. \relax }}{69}{figure.caption.120}
\contentsline {figure}{\numberline {6.20}{\ignorespaces Positive phase and negative phase in the first layer of the DBN of after training on the Poker-DVS dataset. The reconstruction of each class is nearly perfect indicating, the DBN has learned the basic structure of the data.\relax }}{69}{figure.caption.121}
\contentsline {figure}{\numberline {6.21}{\ignorespaces Convolutional filters of the first layers of the analog and spiking convolutional DBNs after 100 training samples. The weights of the spiking DBN appear to be more discriminative.\relax }}{69}{figure.caption.123}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
