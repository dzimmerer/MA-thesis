\select@language {english}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces A sample Bayesian network with 5 binary nodes. (a) In the network RV $c$ is directly dependent on $a$, $b$ and thus $c$ is the child of its parents $a$, $b$. The RVs $d$, $e$ are the children of $c$. Since there is a path from $a$ to $e$, $a$ is an ancestor of its descended $e$. (b) The conditional probability of $p(c |a , b)$ in a tabular form. Another variant of the conditional probability $p(c |a , b)$ is given in (c). The probability is defined by the parameters $w_{ac}$, $w_{bc}$, and due to the sigmoid activation function $\sigma $ a network with such probability functions is called sigmoid belief network.\relax }}{4}{figure.caption.3}
\contentsline {figure}{\numberline {2.2}{\ignorespaces (a) A Markov network with 5 nodes. The white node is dependent on all connected nodes (blue nodes). Given the blue nodes the white node is independent on any other node in the network. Given the white node all the blue nodes are independent on each other. (b) Two cliques in a Markov network. The blue clique is maximal, since no vertex can be added, which is fully connected to all others in the blue clique. The green one is not maximal, since the node $x_3$ could be added \cite {bishop2013pattern}.\relax }}{5}{figure.caption.4}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Sampling a Gaussian distribution. The number of samples in an interval approximates the true density function. As more samples are drawn, the approximation of the density function will be more exact \cite {sampleFD}.\relax }}{6}{figure.caption.5}
\contentsline {figure}{\numberline {2.4}{\ignorespaces A small section in the Brain. The neurons $a$ - $g$ are connected to other neurons in a complex network \cite {gerstner2014neuronal}.\relax }}{8}{figure.caption.10}
\contentsline {figure}{\numberline {2.5}{\ignorespaces A schematic view of a natural neuron. Other neurons are pre-synaptic connected via the dendrites. The signals are then forwarded and accumulated in the soma and from there on via the in myelin sheath cover axon to the axon terminal and the out going synapses \cite {neuronImg}.\relax }}{9}{figure.caption.12}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Structure of a perceptron. The input $in(t)$ is set at the input variables $x_i$ and the multiplied with the corresponding synaptic weight $w_i$ and accumulated. In addition a threshold offset $\theta $ is added. On the sum the step-function is applied i.e. the output $out(t)$ is $1$ if the sum is greater $0$ and $0$ if the sum is smaller $0$ \cite {perceptronImg}.\relax }}{11}{figure.caption.18}
\contentsline {figure}{\numberline {2.7}{\ignorespaces The discrimination function of a perceptron. The discrimination function has the shape of a linear hyper plane in data space and it defined by the synaptic weight-vector $\textbf {w}$. It divides the data space and thus the data samples into two subspaces, the positive space $\textbf {x}^{\intercal }\textbf {w} > 0$ and the negative space $\textbf {x}^{\intercal }\textbf {w} < 0$. In (a) the discrimination function is given in a two-dimensional data space and in (b) in a three-dimensional data space.\relax }}{12}{figure.caption.20}
\contentsline {figure}{\numberline {2.8}{\ignorespaces A schematic multi layer perceptron with three layers \cite {mlpImg}.\relax }}{13}{figure.caption.24}
\contentsline {figure}{\numberline {2.9}{\ignorespaces The output of different activation functions plotted given the input.\relax }}{14}{figure.caption.26}
\contentsline {figure}{\numberline {2.10}{\ignorespaces A cross correlation of a $3\times 3$ image matrix with a $2\times 2$ kernel without stride and padding. The result is a $2\times 2$ feature map.\relax }}{17}{figure.caption.31}
\contentsline {figure}{\numberline {2.11}{\ignorespaces Typical architecture of a convolutional neural network with two convolution-pooling stages \cite {cnnarchImg}.\relax }}{18}{figure.caption.34}
\contentsline {figure}{\numberline {2.12}{\ignorespaces A blueprint of a Hopfield nets with 8 binary units. The units are connected with symmetric undirected connections \cite {hopfImg}.\relax }}{19}{figure.caption.38}
\contentsline {figure}{\numberline {2.13}{\ignorespaces A Boltzmann machine with 7 units. In contrast to a Hopfield nets, units are divided into visible and hidden/ unobserved units with stochastic activations \cite {boltzImg}.\relax }}{21}{figure.caption.41}
\contentsline {figure}{\numberline {2.14}{\ignorespaces A restricted Boltzmann machine is special kind of Boltzmann machine with no lateral connections in the hidden and visible layer. This eases sampling, since the visible are only dependent on the hidden units and the hidden units only on the visible units \cite {rbmImg}.\relax }}{23}{figure.caption.44}
\contentsline {figure}{\numberline {2.15}{\ignorespaces A temporal unrolling of the contrastive divergence algorithm with $k$ sampling step. The hidden units and the visible units are alternatingly sampled conditioned on the current state of the other \cite {cdImg}.\relax }}{23}{figure.caption.46}
\contentsline {figure}{\numberline {2.16}{\ignorespaces Building up a deep belief network, by training RBMs greedily and stacking them up on top of each other. At first only one RBM is trained. On top of the first RBM the next RBM is trained. This can procedure can be performed for an arbitrary number of repetitions. In the top layer "association" RBM the label information $y$ can be in-cooperated as well \cite {cdImg}.\relax }}{24}{figure.caption.50}
\contentsline {figure}{\numberline {2.17}{\ignorespaces A LIF neuron as an electrical circuit. The capacitor $C_m$ corresponds to the potential at the membrane, $g_l$ the leakage potential and $E_l$ the resting potential. If the membrane potential is greater than $U_t$ a spike $\delta $ is emitted \cite {heikoMA}.\relax }}{26}{figure.caption.54}
\contentsline {figure}{\numberline {2.18}{\ignorespaces Different firing behaviour observed in the Brain. In (a) a neuron shows constant tonic firing, while in (b) the neuron shows frequency adaptation and in (c) the neurons shows delayed regular bursting \cite {gerstner2014neuronal}.\relax }}{27}{figure.caption.55}
\contentsline {figure}{\numberline {2.19}{\ignorespaces A Hodgkin-Huxley as an electrical circuit. The membrane potential corresponds to $C_m$, and the ion channels to $g_{Na}$, $g_{k}$, $g_{L}$ with their reversal potentials $E_{Na}$, $E_{}$, $E_{L}$ \cite {heikoMA}.\relax }}{28}{figure.caption.57}
\contentsline {figure}{\numberline {2.20}{\ignorespaces Three samples of Ornstein\IeC {\textendash }Uhlenbeck processes. This can be seen as the membrane potential of neurons in a high conductance state (in comparison to the black doted line can be seen as a neuron without noisy input) \cite {Petrovici2016}.\relax }}{30}{figure.caption.63}
\contentsline {figure}{\numberline {2.21}{\ignorespaces A membrane potential trajectory of a neuron in a high conductance state. (a) The spike train and the membrane potential. (b) The blue curve represents the actual membrane potential with refractory periods after each spike, while the red curve represents a membrane potential without a firing threshold. It is apparent, that the blue curve returns the the hypothetical red potential without a "return from rest" time \cite {Petrovici2016}. \relax }}{30}{figure.caption.64}
\contentsline {figure}{\numberline {2.22}{\ignorespaces Three different PSP kernels. The green one has an alpha-shape, the blue one is exponential shaped and the red one is rectangular \cite {Petrovici2016}. \relax }}{31}{figure.caption.66}
\contentsline {figure}{\numberline {2.23}{\ignorespaces Different STDP curves observed in the Brain. The first curve shows only long term potentiation. The middle one shows a variant of the classic STDP Curve and the last one shows purely long term depression \cite {Buchanan2010}.\relax }}{33}{figure.caption.72}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces A Gibbs sampling step in a convolutional RBM. At first the visible data is convolved with the kernel to get the hidden activations. Afterwards the kernel matrix is flipped 180\textdegree . In the downward pass the hidden activations with padding are convolved with the flipped kernel to get the new visible activations.\relax }}{36}{figure.caption.73}
\contentsline {figure}{\numberline {3.2}{\ignorespaces The RBM layer architecture with probabilistic max pooling \cite {lee2009convolutional}.\relax }}{36}{figure.caption.74}
\contentsline {figure}{\numberline {3.3}{\ignorespaces A spiking neural network as probabilistic model. The state $z_i$ of a neuron is set to $1$ for a time period $t_{on}$ after a spike. The complete state of the network $\textbf {z}$ is given by the individual state $(z_1, z_2, z_3, z_4)$ \cite {Petrovici2016}.\relax }}{37}{figure.caption.75}
\contentsline {figure}{\numberline {3.4}{\ignorespaces The artificial counter state $\xi $ of a neuron in a discrete time. In (a) the red state represent an active state $z_i = 1$. After a spike $\xi $ is set to the refractory period $\tau $. In each time step the refractory period counter $\xi $ is reduced by 1 until the neuron is inactive and can spike again. In (b) a exemplary membrane potential with the corresponding states of a neuron is given \cite {Buesing2011}.\relax }}{38}{figure.caption.76}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Comparison of the state probabilities with of neural sampling with Gibbs sampling in a four state Boltzmann machine. In (a) a probabilistic neuron model with a rectangular PSP is used and no discrepancies can be detected given a long enough sampling period. In (b) a more biological plausible alpha shaped kernel is used, which leads to some differences \cite {Buesing2011}.\relax }}{38}{figure.caption.77}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Input output transfer function of a neuron in a high conductance state. The output rate is normalized by $\frac {1}{\tau _{ref}}$ to get a output probability. The transfer function is identical to shifted sigmoid function $\sigma $ \cite {Petrovici2016}.\relax }}{39}{figure.caption.78}
\contentsline {figure}{\numberline {3.7}{\ignorespaces The proposed architectures to conversion between CNNs and SNNs \cite {Cao2014}. \relax }}{40}{figure.caption.79}
\contentsline {figure}{\numberline {3.8}{\ignorespaces An unrolled RBM with tied weights, which can be used to simulate the positive and negative phase and learn the weighs online \cite {Neil2013}.\relax }}{40}{figure.caption.80}
\contentsline {figure}{\numberline {3.9}{\ignorespaces Comparison between classical CD and event-based CD. In the classical k-CD (a) the weight update is determined by positive phase and negative phase, while in event-base CD (b) the weight update is determined by STDP and the sampling phase, determined by $g$ \cite {Neftci2013}.\relax }}{41}{figure.caption.81}
\contentsline {figure}{\numberline {3.10}{\ignorespaces Visualization of the phases of the event based contrastive divergence.\relax }}{41}{figure.caption.82}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Receptive fields over 4 neurons. In (a) the top layer neuron (red), is only connected to a some neurons in close vicinity to each other. In (b) four receptive fields are next to each other with a stride of two and shared weights. The top layer neurons have a similar topology as their receptive fields. Such a structure can give similar results to conventional more dimensional convolution. \relax }}{44}{figure.caption.83}
\contentsline {figure}{\numberline {4.2}{\ignorespaces A common structure of a deep belief network, built up of RBMs used for classification. The first layers transform the input data/ pixel image and can extract features and the top layer RBM is used for association the data with the correct label.\relax }}{44}{figure.caption.84}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Different converted structures of a CNN and a DBN. The CNN is converted to a spiking network, by simply using simple LIF neurons (white) (a). In the DBN (b) the LIF neurons are put in a high conductance state (black), by inserting high frequency Poisson noise. The weights are scaled accordingly to fit the activations of the neurons.\relax }}{45}{figure.caption.85}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Properties of a conductance based LIF neuron, put into a high conductance state with high frequency input noise. The neuron fires with a probability of $0.5$ given no additional input (a). With input the input output transfer function approximates a sigmoid function (b). A network with such neurons is able to sample in a Boltzmann distribution similar to original distribution (c). \relax }}{46}{figure.caption.88}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Properties of a current based LIF neuron, put into a simulated high conductance state by high frequency input noise and a artificial high membrane conductance. The neuron also fires with a probability of $0.5$ given no additional input (a) and with input the input output transfer function approximates a sigmoid function (b). A network with such neurons can also to sample in a Boltzmann distribution similar to original distribution (c).\relax }}{47}{figure.caption.90}
\contentsline {figure}{\numberline {4.6}{\ignorespaces The five phases for a data sample in the adapted eCD algorithm. \relax }}{49}{figure.caption.91}
\contentsline {figure}{\numberline {4.7}{\ignorespaces A (restricted) Boltzmann machine with lateral inhibition in the top layer.\relax }}{50}{figure.caption.93}
\contentsline {figure}{\numberline {4.8}{\ignorespaces The structure of a deep belief network trained with eCD. The spiking DBN consists of single Boltzmann machines stacked on top of each others. In contrast to artificial DBNs, the Boltzmann machines are still bidirection connected, and two Boltzmann machines are stacked up by forwarding the activations of the hidden layer in the bottom RBM to the visible layer in the top RBM.\relax }}{51}{figure.caption.94}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Samples from the $10 \times 10$ pixel stripe dataset. The stripes in (a) and (c) have the same position in the image, while the stripes in (b) and (d) can appear anywhere in the image. In (a) and (b) the images are binary , i.e a pixel value $p \in \{0,1\}$, while in (c) and (d) the values are continuous i.e $p \in [0,1 ]$. \relax }}{58}{figure.caption.95}
\contentsline {figure}{\numberline {6.2}{\ignorespaces Samples from the $28 \times 28$ pixel MNIST dataset \cite {lecun-mnisthandwrittendigit-2010}. The pixel values $p$ are scales to be in the interval $p \in [0,1 ]$. \relax }}{58}{figure.caption.96}
\contentsline {figure}{\numberline {6.3}{\ignorespaces Visualization of samples from the Poker-DVS dataset \cite {serrano2013128}. The images are generated by integrating all events over $8$ ms. The actual training is performed on the events.\relax }}{59}{figure.caption.97}
\contentsline {figure}{\numberline {6.4}{\ignorespaces Visualized filters $16 \times 16$ filters of the first layer convolutional RBM of a DBN trained on the $28 \times 28$ pixel MNIST dataset.\relax }}{60}{figure.caption.98}
\contentsline {figure}{\numberline {6.5}{\ignorespaces Activations in the features maps in a artificial convolutional DBN and the converted spiking network architectures. \relax }}{62}{figure.caption.102}
\contentsline {figure}{\numberline {6.6}{\ignorespaces Misclassifications of the spiking DBNs. Often $9$ and $4$ are mixed up. The first three are classified as a $9$, the fourth one is classified as a $4$. The images all share a lot of features with the suggested class and the correct class was always the second most probable.\relax }}{63}{figure.caption.103}
\contentsline {figure}{\numberline {6.7}{\ignorespaces Positive phase and negative phase of a diagonal stripe with 4 filters. In the first 4 images sets a stripe is learned, while during the last 4 images the model distribution is unlearned.\relax }}{63}{figure.caption.106}
\contentsline {figure}{\numberline {6.8}{\ignorespaces Accumulated weight change for each training sample. As more samples are presented and the weights specialize, the total weight updates decrease.\relax }}{64}{figure.caption.107}
\contentsline {figure}{\numberline {6.9}{\ignorespaces Visualization of the spikes in the visible layer during the positive phase and negative phase of a diagonal stripe. In the first column, at the beginning of the training, in the negative phase the network is not able to reconstruct the data distribution. As the training progresses (in the second and third column), the reconstruction approximates the original data sample more closely and is in the last column nearly perfect.\relax }}{64}{figure.caption.109}
\contentsline {figure}{\numberline {6.10}{\ignorespaces Spikes in the hidden layer of a spiking convolutional RBM. At the start of the training (a) is appears mostly random, while after training the activations become more sparse ((b) and (c)) and certain neurons become specialized on certain input patterns (b).\relax }}{65}{figure.caption.110}
\contentsline {figure}{\numberline {6.11}{\ignorespaces $5 \times 5$ convolution filter matrices with and without lateral inhibitory connections in the top RBM layer on the stripe dataset. In (a) the weight are trained without lateral inhibitory connections and in (b) they are trained with lateral inhibitory connections. Whereas the filters without lateral connections (a) look more similar the filters with lateral connections (b) are more different and discriminative. \relax }}{65}{figure.caption.112}
\contentsline {figure}{\numberline {6.12}{\ignorespaces Weight of the first layers of the DBNs with and without convolutions with the same number of free parameters. In (a) are weights of the DBN with convolutions visualized and in (b) the weights of the DBN without convolutions.\relax }}{66}{figure.caption.114}
\contentsline {figure}{\numberline {6.13}{\ignorespaces The runtime of a learning step in dependence on the size of the network. While the simulated time of $168$ms stays constant, the runtime increases linearly with the number of hidden units.\relax }}{67}{figure.caption.116}
\contentsline {figure}{\numberline {6.14}{\ignorespaces Abstract architecture of the DBN for the stripe dataset.\relax }}{67}{figure.caption.118}
\contentsline {figure}{\numberline {6.15}{\ignorespaces The development of the $5 \times 5$ convolution filter matrices in the first layer of a DBN during training with the convolutional eCD algorithm on $1000$ samples of the stripe dataset.\relax }}{68}{figure.caption.119}
\contentsline {figure}{\numberline {6.16}{\ignorespaces Activations/ spikes in the layers of the first RBM during training. As the training progresses, the activations become more sparse. The hidden layer learns new representations for the data. The model distribution approximates the data distribution, resulting in a nearly perfect reconstruction of the input data.\relax }}{69}{figure.caption.120}
\contentsline {figure}{\numberline {6.17}{\ignorespaces Activations/ spikes in the layers of the second RBM during training. As the training progresses, the activations become more sparse. The hidden layer learns new 20 dimensional representations for the data. The model distribution approximates the data distribution, resulting in a nearly perfect reconstruction of the input data, especially of the label.\relax }}{70}{figure.caption.121}
\contentsline {figure}{\numberline {6.18}{\ignorespaces Abstract architecture of the DBN for the poker dataset.\relax }}{71}{figure.caption.123}
\contentsline {figure}{\numberline {6.19}{\ignorespaces Visualization of the convolution filters of the first layer RBM trained with the convolutional eCD algorithm on the Poker-DVS dataset.\relax }}{71}{figure.caption.124}
\contentsline {figure}{\numberline {6.20}{\ignorespaces The accuracy and reconstruction error of the DBN. The accuracy increases to a maximal value of $90 \%$ and the reconstruction error decreases, indicating discriminative features. \relax }}{72}{figure.caption.125}
\contentsline {figure}{\numberline {6.21}{\ignorespaces Positive phase and negative phase in the first layer of the DBN of after training on the Poker-DVS dataset. The reconstruction of each class is nearly perfect indicating, the DBN has learned the basic structure of the data.\relax }}{72}{figure.caption.126}
\contentsline {figure}{\numberline {6.22}{\ignorespaces Completion of partially fed input data. In the first two columns the bottom $4$ rows and in the last two columns the $7$ bottom rows in the image data were omitted. After training, the DBN was able to reconstruct some of the missing input data an complete the input data.\relax }}{73}{figure.caption.127}
\contentsline {figure}{\numberline {6.23}{\ignorespaces Convolutional filters of the first layers of the artificial and spiking convolutional DBNs after 100 training samples. The weights of the spiking DBN appear to be more discriminative.\relax }}{73}{figure.caption.129}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {A.1}{\ignorespaces Spikes in a hidden layer of a spiking RBM with lateral inhibition. As the lateral inhibition increases, the activity becomes more sparse. In the case with big inhibitory weights, there are a few neurons dominating the activity allowing only a few different states of the network and thus poor mode switching in a training step. In a network with small inhibitory weights, there is sparse activity, but the the network visits many different states in a simulation step and shows sufficient mode mixing. \relax }}{81}{figure.caption.136}
\contentsline {figure}{\numberline {A.2}{\ignorespaces Activity in the visible layer of the top RBM in a DBNs with different architectures. The first two architectures model separate RBM layers which are stacked by one-on-one forward connections from the hidden layer of the bottom RBM to the visible layer of the top RBM, while in the last architecture, the RBMs are directly stacked, i.e. the hidden layer of the bottom RBM is the visible layer of the top RBM. While in the top two architectures, there are hardly any differences and the input is modelled correctly, due to top down influences the output of the bottom RBM gets distorted by activity in the hidden layer of the top RBM.\relax }}{82}{figure.caption.137}
\contentsline {figure}{\numberline {A.3}{\ignorespaces Weights development for eCD with synchronous weight updates. The first column shows the development. After the positive the weights increased a lot, which leads to a high decrement during the negative which leads to a "dying out" of the spikes, which is visualized in the second column. Using an increased positive phase with a higher learning rate, does not solve the problem and leads to a division of the weights into a few very positive weights and many negative weights. In contrast to this a "normal" weight distribution without any weight synchronization in presented in the last column. \relax }}{83}{figure.caption.138}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
