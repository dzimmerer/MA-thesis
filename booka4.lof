\select@language {english}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces A sample Bayesian network with 5 nodes. (a) In the network RV $c$ is directly depended on $a$, $b$ and thus $c$ is the child of its parents $a$, $b$. The RVs $d$, $e$ are the children of $c$. Since there is a path from $a$ to $e$, $a$ is an ancestor of its descended $e$. (b) The conditional probability of $p(c |a , b)$ in a tabular form. Another variant of the conditional probability $p(c |a , b)$ is given in (c). The probability is defined by the parameters $w_{ac}$, $w_{bc}$, and due to the sigmoid activation function $\sigma $ a network with such probability functions is called sigmoid belief network.\relax }}{4}{figure.caption.3}
\contentsline {figure}{\numberline {2.2}{\ignorespaces (a) A Markov network with 5 nodes. The white node is depended on all connected nodes (blue nodes). Given its the blue nodes the white node is independent on any other node in the network. (b) Two cliques in a Markov network. The blue clique is maximal, since no vertex can be added, which is fully connected to all others in the blue clique. The green one is not maximal, since the node $x_3$ could be added.\relax }}{5}{figure.caption.4}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Sampling at discrete points $S_i$ in an simple distribution. The samples $S_i$ approximate the true density function. As more samples are drawn, the approximation will represent the function more exact.\relax }}{6}{figure.caption.5}
\contentsline {figure}{\numberline {2.4}{\ignorespaces A small section in the Brain. The neurons $a$ - $g$ are connected to other neurons in a complex network.\relax }}{8}{figure.caption.10}
\contentsline {figure}{\numberline {2.5}{\ignorespaces A schematic view of a natural neuron. Other neurons are pre-synaptic connected via the dendrites. The signals are then forwarded and accumulated in the soma and from there on via the in myelin sheath cover axon to the axon terminal and the out going synapses.\relax }}{9}{figure.caption.12}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Structure of a perceptron. The input $in(t)$ is set at the input variables $x_i$ and the multiplied with the corresponding synaptic weight $w_i$ and accumulated. In addition a threshold offset $\theta $ is added. On the sum the step-function is applied i.e. the output $out(t)$ is $1$ if the sum is greater $0$ and $0$ if the sum is smaller $0$.\relax }}{10}{figure.caption.18}
\contentsline {figure}{\numberline {2.7}{\ignorespaces The discrimination function of a perceptron. The discrimination function has the shape of a linear hyper plane in data space and it defined by the synaptic weight-vector $\textbf {w}$. It divides the data space and thus the data samples into two subspaces, the positive space $\textbf {x}^{\intercal }\textbf {w} > 0$ and the negative space $\textbf {x}^{\intercal }\textbf {w} < 0$.\relax }}{11}{figure.caption.20}
\contentsline {figure}{\numberline {2.8}{\ignorespaces A schematic multi layer perceptron with four layers.\relax }}{13}{figure.caption.24}
\contentsline {figure}{\numberline {2.9}{\ignorespaces The output of different activation functions plotted given the input.\relax }}{14}{figure.caption.26}
\contentsline {figure}{\numberline {2.10}{\ignorespaces Convolving or to be more exact a cross correlation of a $3\times 3$ image matrix with a $2\times 2$ kernel without stride and padding. The result is a $2\times 2$ feature map.\relax }}{16}{figure.caption.31}
\contentsline {figure}{\numberline {2.11}{\ignorespaces Typical architecture of a convolutional neural network with two convolution-pooling stages.\relax }}{17}{figure.caption.34}
\contentsline {figure}{\numberline {2.12}{\ignorespaces A blueprint of a Hopfield nets with 8 binary units. The units are connected with symmetric undirected connections.\relax }}{19}{figure.caption.38}
\contentsline {figure}{\numberline {2.13}{\ignorespaces A Boltzmann machine with 7 units. In contrast to a Hopfield nets, units are divided into visible and hidden/ unobserved units with stochastic activations.\relax }}{20}{figure.caption.42}
\contentsline {figure}{\numberline {2.14}{\ignorespaces A restricted Boltzmann machine is special kind of Boltzmann machine with no lateral connections in the hidden and visible layer. This eases sampling, since the visible are only dependent on the hidden units and the hidden units only on the visible units.\relax }}{22}{figure.caption.45}
\contentsline {figure}{\numberline {2.15}{\ignorespaces A temporal unrolling of the contrastive divergence algorithm with $k$ sampling step. The hidden units and the visible units are alternatingly sampled conditioned on the current state of the other.\relax }}{23}{figure.caption.47}
\contentsline {figure}{\numberline {2.16}{\ignorespaces Building up a deep belief network, by training RBMs greedily and stacking them up on top of each other. In the top layer "association" RBM the label information $y$ can be incooperated as well.\relax }}{23}{figure.caption.51}
\contentsline {figure}{\numberline {2.17}{\ignorespaces A figure.\relax }}{24}{figure.caption.55}
\contentsline {figure}{\numberline {2.18}{\ignorespaces A figure.\relax }}{26}{figure.caption.57}
\contentsline {figure}{\numberline {2.19}{\ignorespaces A figure.\relax }}{28}{figure.caption.63}
\contentsline {figure}{\numberline {2.20}{\ignorespaces A figure.\relax }}{28}{figure.caption.64}
\contentsline {figure}{\numberline {2.21}{\ignorespaces A figure.\relax }}{29}{figure.caption.66}
\contentsline {figure}{\numberline {2.22}{\ignorespaces A figure.\relax }}{30}{figure.caption.72}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.2}{\ignorespaces A figure.\relax }}{32}{figure.caption.74}
\contentsline {figure}{\numberline {3.3}{\ignorespaces A figure.\relax }}{33}{figure.caption.75}
\contentsline {figure}{\numberline {3.4}{\ignorespaces A figure.\relax }}{33}{figure.caption.76}
\contentsline {figure}{\numberline {3.6}{\ignorespaces A figure.\relax }}{34}{figure.caption.78}
\contentsline {figure}{\numberline {3.8}{\ignorespaces A figure.\relax }}{36}{figure.caption.80}
\contentsline {figure}{\numberline {3.9}{\ignorespaces A figure.\relax }}{37}{figure.caption.81}
\contentsline {figure}{\numberline {3.10}{\ignorespaces A figure.\relax }}{37}{figure.caption.82}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.2}{\ignorespaces A figure.\relax }}{40}{figure.caption.84}
\contentsline {figure}{\numberline {4.6}{\ignorespaces A figure.\relax }}{44}{figure.caption.91}
\contentsline {figure}{\numberline {4.7}{\ignorespaces A figure.\relax }}{45}{figure.caption.93}
\contentsline {figure}{\numberline {4.8}{\ignorespaces A figure.\relax }}{46}{figure.caption.94}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces A figure.\relax }}{51}{figure.caption.95}
\contentsline {figure}{\numberline {6.2}{\ignorespaces A figure.\relax }}{52}{figure.caption.96}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
