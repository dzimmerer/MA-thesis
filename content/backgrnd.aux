\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{3}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Probabilistic Graphical Models}{3}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Bayesian network}{3}{subsection.2.1.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sub1}{{2.1a}{4}{A Bayesian with 5 RVs.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:sub1}{{a}{4}{A Bayesian with 5 RVs.\relax }{figure.caption.3}{}}
\newlabel{fig:sub2}{{2.1b}{4}{Tabular conditional probability $p(c |a , b)$.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:sub2}{{b}{4}{Tabular conditional probability $p(c |a , b)$.\relax }{figure.caption.3}{}}
\newlabel{fig:bayesnet}{{2.1c}{4}{Conditional probability $p(c |a , b)$ given by a function over $a,b$.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:bayesnet}{{c}{4}{Conditional probability $p(c |a , b)$ given by a function over $a,b$.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A sample Bayesian network with 5 binary nodes. (a) In the network RV $c$ is directly depended on $a$, $b$ and thus $c$ is the child of its parents $a$, $b$. The RVs $d$, $e$ are the children of $c$. Since there is a path from $a$ to $e$, $a$ is an ancestor of its descended $e$. (b) The conditional probability of $p(c |a , b)$ in a tabular form. Another variant of the conditional probability $p(c |a , b)$ is given in (c). The probability is defined by the parameters $w_{ac}$, $w_{bc}$, and due to the sigmoid activation function $\sigma $ a network with such probability functions is called sigmoid belief network.\relax }}{4}{figure.caption.3}}
\newlabel{fig:test}{{2.1}{4}{A sample Bayesian network with 5 binary nodes. (a) In the network RV $c$ is directly depended on $a$, $b$ and thus $c$ is the child of its parents $a$, $b$. The RVs $d$, $e$ are the children of $c$. Since there is a path from $a$ to $e$, $a$ is an ancestor of its descended $e$. (b) The conditional probability of $p(c |a , b)$ in a tabular form. Another variant of the conditional probability $p(c |a , b)$ is given in (c). The probability is defined by the parameters $w_{ac}$, $w_{bc}$, and due to the sigmoid activation function $\sigma $ a network with such probability functions is called sigmoid belief network.\relax }{figure.caption.3}{}}
\newlabel{fig:sub1}{{2.2a}{5}{A Markov network with with 5 nodes.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:sub1}{{a}{5}{A Markov network with with 5 nodes.\relax }{figure.caption.4}{}}
\newlabel{fig:markovnet}{{2.2b}{5}{Cliques in a Markov network\relax }{figure.caption.4}{}}
\newlabel{sub@fig:markovnet}{{b}{5}{Cliques in a Markov network\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces (a) A Markov network with 5 nodes. The white node is depended on all connected nodes (blue nodes). Given the blue nodes the white node is independent on any other node in the network. Given the white node all the blue nodes are independent on each other. (b) Two cliques in a Markov network. The blue clique is maximal, since no vertex can be added, which is fully connected to all others in the blue clique. The green one is not maximal, since the node $x_3$ could be added.\relax }}{5}{figure.caption.4}}
\newlabel{fig:test}{{2.2}{5}{(a) A Markov network with 5 nodes. The white node is depended on all connected nodes (blue nodes). Given the blue nodes the white node is independent on any other node in the network. Given the white node all the blue nodes are independent on each other. (b) Two cliques in a Markov network. The blue clique is maximal, since no vertex can be added, which is fully connected to all others in the blue clique. The green one is not maximal, since the node $x_3$ could be added.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Markov Random Field}{5}{subsection.2.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Energy-Based Models}{5}{subsection.2.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Sampling at discrete points $S_i$ in an simple distribution. The samples $S_i$ approximate the true density function. As more samples are drawn, the approximation will represent the function more exact.\relax }}{6}{figure.caption.5}}
\newlabel{fig:Sampling}{{2.3}{6}{Sampling at discrete points $S_i$ in an simple distribution. The samples $S_i$ approximate the true density function. As more samples are drawn, the approximation will represent the function more exact.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Sampling}{6}{subsection.2.1.4}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Ancestral Sampling}{6}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Markov Chain Monte Carlo}{7}{section*.7}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Gibbs sampling}{7}{section*.8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Neural Networks}{7}{section.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A small section in the Brain. The neurons $a$ - $g$ are connected to other neurons in a complex network.\relax }}{8}{figure.caption.10}}
\newlabel{fig:brain}{{2.4}{8}{A small section in the Brain. The neurons $a$ - $g$ are connected to other neurons in a complex network.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Natural}{8}{subsection.2.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Brain}{8}{section*.9}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Neuron}{8}{section*.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces A schematic view of a natural neuron. Other neurons are pre-synaptic connected via the dendrites. The signals are then forwarded and accumulated in the soma and from there on via the in myelin sheath cover axon to the axon terminal and the out going synapses.\relax }}{9}{figure.caption.12}}
\newlabel{fig:neurpn}{{2.5}{9}{A schematic view of a natural neuron. Other neurons are pre-synaptic connected via the dendrites. The signals are then forwarded and accumulated in the soma and from there on via the in myelin sheath cover axon to the axon terminal and the out going synapses.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Learning}{9}{section*.13}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Synaptic Plasticity}{10}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline "What fires together, wires together"}{10}{section*.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Artificial neural networks}{10}{subsection.2.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Perceptron}{10}{section*.16}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Structure of a perceptron. The input $in(t)$ is set at the input variables $x_i$ and the multiplied with the corresponding synaptic weight $w_i$ and accumulated. In addition a threshold offset $\theta $ is added. On the sum the step-function is applied i.e. the output $out(t)$ is $1$ if the sum is greater $0$ and $0$ if the sum is smaller $0$.\relax }}{11}{figure.caption.18}}
\newlabel{fig:perceptron}{{2.6}{11}{Structure of a perceptron. The input $in(t)$ is set at the input variables $x_i$ and the multiplied with the corresponding synaptic weight $w_i$ and accumulated. In addition a threshold offset $\theta $ is added. On the sum the step-function is applied i.e. the output $out(t)$ is $1$ if the sum is greater $0$ and $0$ if the sum is smaller $0$.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Model}{11}{section*.17}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Decision Function}{11}{section*.19}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The discrimination function of a perceptron. The discrimination function has the shape of a linear hyper plane in data space and it defined by the synaptic weight-vector $\textbf  {w}$. It divides the data space and thus the data samples into two subspaces, the positive space $\textbf  {x}^{\intercal }\textbf  {w} > 0$ and the negative space $\textbf  {x}^{\intercal }\textbf  {w} < 0$.\relax }}{12}{figure.caption.20}}
\newlabel{fig:discrimation}{{2.7}{12}{The discrimination function of a perceptron. The discrimination function has the shape of a linear hyper plane in data space and it defined by the synaptic weight-vector $\textbf {w}$. It divides the data space and thus the data samples into two subspaces, the positive space $\textbf {x}^{\intercal }\textbf {w} > 0$ and the negative space $\textbf {x}^{\intercal }\textbf {w} < 0$.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Perceptron Learning}{12}{section*.21}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Mutlilayer-Perceptron}{12}{section*.22}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Model architecture}{12}{section*.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces A schematic multi layer perceptron with three layers.\relax }}{13}{figure.caption.24}}
\newlabel{fig:mlp}{{2.8}{13}{A schematic multi layer perceptron with three layers.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Activation functions}{13}{section*.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The output of different activation functions plotted given the input.\relax }}{14}{figure.caption.26}}
\newlabel{fig:activations}{{2.9}{14}{The output of different activation functions plotted given the input.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Error functions}{14}{section*.27}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Backpropagation}{14}{section*.28}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Convolutinal Neural Networks}{16}{section*.29}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Convolution}{16}{section*.30}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces A cross correlation of a $3\times 3$ image matrix with a $2\times 2$ kernel without stride and padding. The result is a $2\times 2$ feature map.\relax }}{17}{figure.caption.31}}
\newlabel{fig:conv}{{2.10}{17}{A cross correlation of a $3\times 3$ image matrix with a $2\times 2$ kernel without stride and padding. The result is a $2\times 2$ feature map.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Convolution Layers}{17}{section*.32}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Architecture}{17}{section*.33}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Training}{17}{section*.35}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Typical architecture of a convolutional neural network with two convolution-pooling stages.\relax }}{18}{figure.caption.34}}
\newlabel{fig:convarcitecuture}{{2.11}{18}{Typical architecture of a convolutional neural network with two convolution-pooling stages.\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Hopfield Nets}{18}{section*.36}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Model}{18}{section*.37}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces A blueprint of a Hopfield nets with 8 binary units. The units are connected with symmetric undirected connections.\relax }}{19}{figure.caption.38}}
\newlabel{fig:hopfiled}{{2.12}{19}{A blueprint of a Hopfield nets with 8 binary units. The units are connected with symmetric undirected connections.\relax }{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Boltzmann Machines}{20}{section*.39}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Model}{20}{section*.40}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Learning Rule}{20}{section*.42}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces A Boltzmann machine with 7 units. In contrast to a Hopfield nets, units are divided into visible and hidden/ unobserved units with stochastic activations.\relax }}{21}{figure.caption.41}}
\newlabel{fig:bm}{{2.13}{21}{A Boltzmann machine with 7 units. In contrast to a Hopfield nets, units are divided into visible and hidden/ unobserved units with stochastic activations.\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline RBMs}{22}{section*.43}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline CD-k Training}{22}{section*.45}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces A restricted Boltzmann machine is special kind of Boltzmann machine with no lateral connections in the hidden and visible layer. This eases sampling, since the visible are only dependent on the hidden units and the hidden units only on the visible units.\relax }}{23}{figure.caption.44}}
\newlabel{fig:rbm}{{2.14}{23}{A restricted Boltzmann machine is special kind of Boltzmann machine with no lateral connections in the hidden and visible layer. This eases sampling, since the visible are only dependent on the hidden units and the hidden units only on the visible units.\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces A temporal unrolling of the contrastive divergence algorithm with $k$ sampling step. The hidden units and the visible units are alternatingly sampled conditioned on the current state of the other.\relax }}{23}{figure.caption.46}}
\newlabel{fig:cd}{{2.15}{23}{A temporal unrolling of the contrastive divergence algorithm with $k$ sampling step. The hidden units and the visible units are alternatingly sampled conditioned on the current state of the other.\relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Persistent CD}{23}{section*.47}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Deep Belief Networks}{23}{section*.48}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Building up a deep belief network, by training RBMs greedily and stacking them up on top of each other. In the top layer "association" RBM the label information $y$ can be in-cooperated as well.\relax }}{24}{figure.caption.50}}
\newlabel{fig:dbn}{{2.16}{24}{Building up a deep belief network, by training RBMs greedily and stacking them up on top of each other. In the top layer "association" RBM the label information $y$ can be in-cooperated as well.\relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Training}{24}{section*.49}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Fine-tuning}{24}{section*.51}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces A LIF neuron as an electrical circuit. The capacitor $C_m$ corresponds to the potential at the membrane, $g_l$ the leakage potential and $E_l$ the resting potential. If the membrane potential is greater than $U_t$ a spike $\delta $ is emitted.\relax }}{25}{figure.caption.54}}
\newlabel{fig:lif}{{2.17}{25}{A LIF neuron as an electrical circuit. The capacitor $C_m$ corresponds to the potential at the membrane, $g_l$ the leakage potential and $E_l$ the resting potential. If the membrane potential is greater than $U_t$ a spike $\delta $ is emitted.\relax }{figure.caption.54}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Spiking neural networks}{25}{subsection.2.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Neuron Models}{25}{section*.52}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline LIF}{25}{section*.53}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces Different firing behaviour observed in the Brain. In (a) a neuron shows constant tonic firing, while in (b) the neuron shows frequency adaptation and in (c) the neurons shows delayed regular bursting.\relax }}{26}{figure.caption.55}}
\newlabel{fig:discrimation}{{2.18}{26}{Different firing behaviour observed in the Brain. In (a) a neuron shows constant tonic firing, while in (b) the neuron shows frequency adaptation and in (c) the neurons shows delayed regular bursting.\relax }{figure.caption.55}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Hodgkin-Huxley}{26}{section*.56}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces A Hodgkin-Huxley as an electrical circuit. The membrane potential corresponds to $C_m$, and the ion channels to $g_{Na}$, $g_{k}$, $g_{L}$ with their reversal potentials $E_{Na}$, $E_{}$, $E_{L}$.\relax }}{27}{figure.caption.57}}
\newlabel{fig:hogdehux}{{2.19}{27}{A Hodgkin-Huxley as an electrical circuit. The membrane potential corresponds to $C_m$, and the ion channels to $g_{Na}$, $g_{k}$, $g_{L}$ with their reversal potentials $E_{Na}$, $E_{}$, $E_{L}$.\relax }{figure.caption.57}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Activity in a network}{27}{section*.58}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Synapses}{28}{section*.59}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Current-based synaptic interaction}{28}{section*.60}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Conductance-based synaptic interaction}{28}{section*.61}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces Three samples of Ornstein\IeC {\textendash }Uhlenbeck processes. This can be seen as the membrane potential of neurons in a high conductance state (in comparison to the black doted line can be seen as a neuron without noisy input).\relax }}{29}{figure.caption.63}}
\newlabel{fig:ornuhl}{{2.20}{29}{Three samples of Ornstein–Uhlenbeck processes. This can be seen as the membrane potential of neurons in a high conductance state (in comparison to the black doted line can be seen as a neuron without noisy input).\relax }{figure.caption.63}{}}
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline High conductance state}{29}{section*.62}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Kernel function}{29}{section*.65}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces A membrane potential trajectory of a neuron in a high conductance state. (a) The spike train and the membrane potential. (b) The blue curve represents the actual membrane potential with refractory periods after each spike, while the red curve represents a membrane potential without a firing threshold. It is apparent, that the blue curve returns the the hypothetical red potential without a "return from rest" time. \relax }}{30}{figure.caption.64}}
\newlabel{fig:hcs}{{2.21}{30}{A membrane potential trajectory of a neuron in a high conductance state. (a) The spike train and the membrane potential. (b) The blue curve represents the actual membrane potential with refractory periods after each spike, while the red curve represents a membrane potential without a firing threshold. It is apparent, that the blue curve returns the the hypothetical red potential without a "return from rest" time. \relax }{figure.caption.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.22}{\ignorespaces Three different PSP kernels. The green one has an alpha-shape, the blue one is exponential shaped and the red one is rectangular. \relax }}{30}{figure.caption.66}}
\newlabel{fig:pspkernels}{{2.22}{30}{Three different PSP kernels. The green one has an alpha-shape, the blue one is exponential shaped and the red one is rectangular. \relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Neural Coding}{30}{section*.67}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Learning}{31}{section*.68}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Spike time depended plasticity}{31}{section*.69}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Long-term potentiation}{31}{section*.70}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.23}{\ignorespaces Different STDP curves observed in the Brain. The first curve shows only long term potentiation. The second show symmetric STDP. The fourth one shows a variant of the classic STDP Curve show in the center and the last one shows purely long term depression.\relax }}{32}{figure.caption.72}}
\newlabel{fig:stdp}{{2.23}{32}{Different STDP curves observed in the Brain. The first curve shows only long term potentiation. The second show symmetric STDP. The fourth one shows a variant of the classic STDP Curve show in the center and the last one shows purely long term depression.\relax }{figure.caption.72}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Long-term depression}{32}{section*.71}}
\@setckpt{content/backgrnd}{
\setcounter{page}{33}
\setcounter{equation}{0}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{23}
\setcounter{table}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{Item}{16}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{15}
\setcounter{section@level}{4}
}
