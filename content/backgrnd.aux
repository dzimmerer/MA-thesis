\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{Goodfellow-et-al-2016-Book}
\citation{Petrovici2016}
\citation{Goodfellow-et-al-2016-Book}
\citation{Ghahramani2002}
\citation{Zhou2007}
\citation{Zhou2007}
\citation{Ghahramani2002}
\citation{Faltin2007}
\citation{Goodfellow-et-al-2016-Book}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{3}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Probabilistic Graphical Models}{3}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Bayesian network}{3}{subsection.2.1.1}}
\citation{Goodfellow-et-al-2016-Book}
\citation{murphy2012machine}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sub1}{{2.1a}{4}{A Bayesian with 5 RVs.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:sub1}{{a}{4}{A Bayesian with 5 RVs.\relax }{figure.caption.3}{}}
\newlabel{fig:sub2}{{2.1b}{4}{Tabular conditional probability $p(c |a , b)$.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:sub2}{{b}{4}{Tabular conditional probability $p(c |a , b)$.\relax }{figure.caption.3}{}}
\newlabel{fig:bayesnet}{{2.1c}{4}{Conditional probability $p(c |a , b)$ given by a function over $a,b$.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:bayesnet}{{c}{4}{Conditional probability $p(c |a , b)$ given by a function over $a,b$.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A sample Bayesian network with 5 binary nodes. (a) In the network RV $c$ is directly depended on $a$, $b$ and thus $c$ is the child of its parents $a$, $b$. The RVs $d$, $e$ are the children of $c$. Since there is a path from $a$ to $e$, $a$ is an ancestor of its descended $e$. (b) The conditional probability of $p(c |a , b)$ in a tabular form. Another variant of the conditional probability $p(c |a , b)$ is given in (c). The probability is defined by the parameters $w_{ac}$, $w_{bc}$, and due to the sigmoid activation function $\sigma $ a network with such probability functions is called sigmoid belief network.\relax }}{4}{figure.caption.3}}
\newlabel{fig:test}{{2.1}{4}{A sample Bayesian network with 5 binary nodes. (a) In the network RV $c$ is directly depended on $a$, $b$ and thus $c$ is the child of its parents $a$, $b$. The RVs $d$, $e$ are the children of $c$. Since there is a path from $a$ to $e$, $a$ is an ancestor of its descended $e$. (b) The conditional probability of $p(c |a , b)$ in a tabular form. Another variant of the conditional probability $p(c |a , b)$ is given in (c). The probability is defined by the parameters $w_{ac}$, $w_{bc}$, and due to the sigmoid activation function $\sigma $ a network with such probability functions is called sigmoid belief network.\relax }{figure.caption.3}{}}
\citation{bishop2013pattern}
\citation{bishop2013pattern}
\citation{Goodfellow-et-al-2016-Book}
\newlabel{fig:markovnet1}{{2.2a}{5}{A Markov network with with 5 nodes.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:markovnet1}{{a}{5}{A Markov network with with 5 nodes.\relax }{figure.caption.4}{}}
\newlabel{fig:markovnet2}{{2.2b}{5}{Cliques in a Markov network\relax }{figure.caption.4}{}}
\newlabel{sub@fig:markovnet2}{{b}{5}{Cliques in a Markov network\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces (a) A Markov network with 5 nodes. The white node is depended on all connected nodes (blue nodes). Given the blue nodes the white node is independent on any other node in the network. Given the white node all the blue nodes are independent on each other. (b) Two cliques in a Markov network. The blue clique is maximal, since no vertex can be added, which is fully connected to all others in the blue clique. The green one is not maximal, since the node $x_3$ could be added \cite  {bishop2013pattern}.\relax }}{5}{figure.caption.4}}
\newlabel{fig:markovnet}{{2.2}{5}{(a) A Markov network with 5 nodes. The white node is depended on all connected nodes (blue nodes). Given the blue nodes the white node is independent on any other node in the network. Given the white node all the blue nodes are independent on each other. (b) Two cliques in a Markov network. The blue clique is maximal, since no vertex can be added, which is fully connected to all others in the blue clique. The green one is not maximal, since the node $x_3$ could be added \cite {bishop2013pattern}.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Markov Random Field}{5}{subsection.2.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Energy-Based Models}{5}{subsection.2.1.3}}
\citation{Goodfellow-et-al-2016-Book}
\citation{Petrovici2016}
\citation{sampleFD}
\citation{sampleFD}
\citation{Goodfellow-et-al-2016-Book}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Sampling a Gaussian distribution. The number of samples in an interval approximates the true density function. As more samples are drawn, the approximation of the density function will be more exact \cite  {sampleFD}.\relax }}{6}{figure.caption.5}}
\newlabel{fig:Sampling}{{2.3}{6}{Sampling a Gaussian distribution. The number of samples in an interval approximates the true density function. As more samples are drawn, the approximation of the density function will be more exact \cite {sampleFD}.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Sampling}{6}{subsection.2.1.4}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Ancestral Sampling}{6}{section*.6}}
\citation{Goodfellow-et-al-2016-Book}
\citation{Goodfellow-et-al-2016-Book}
\citation{gerstner2014neuronal}
\citation{Byrne1997}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Markov Chain Monte Carlo}{7}{section*.7}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Gibbs sampling}{7}{section*.8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Neural Networks}{7}{section.2.2}}
\citation{gerstner2014neuronal}
\citation{gerstner2014neuronal}
\citation{gerstner2014neuronal}
\citation{Byrne1997}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A small section in the Brain. The neurons $a$ - $g$ are connected to other neurons in a complex network \cite  {gerstner2014neuronal}.\relax }}{8}{figure.caption.10}}
\newlabel{fig:brain}{{2.4}{8}{A small section in the Brain. The neurons $a$ - $g$ are connected to other neurons in a complex network \cite {gerstner2014neuronal}.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Natural}{8}{subsection.2.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Brain}{8}{section*.9}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Neuron}{8}{section*.11}}
\citation{neuronImg}
\citation{neuronImg}
\citation{gerstner2014neuronal}
\citation{Byrne1997}
\citation{Markram2012}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces A schematic view of a natural neuron. Other neurons are pre-synaptic connected via the dendrites. The signals are then forwarded and accumulated in the soma and from there on via the in myelin sheath cover axon to the axon terminal and the out going synapses \cite  {neuronImg}.\relax }}{9}{figure.caption.12}}
\newlabel{fig:neuron}{{2.5}{9}{A schematic view of a natural neuron. Other neurons are pre-synaptic connected via the dendrites. The signals are then forwarded and accumulated in the soma and from there on via the in myelin sheath cover axon to the axon terminal and the out going synapses \cite {neuronImg}.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Learning}{9}{section*.13}}
\citation{hebb19680}
\citation{gerstner2014neuronal}
\citation{rosenblatt1958perceptron}
\citation{perceptronImg}
\citation{perceptronImg}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Synaptic Plasticity}{10}{section*.14}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline "What fires together, wires together"}{10}{section*.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Artificial neural networks}{10}{subsection.2.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Perceptron}{10}{section*.16}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Structure of a perceptron. The input $in(t)$ is set at the input variables $x_i$ and the multiplied with the corresponding synaptic weight $w_i$ and accumulated. In addition a threshold offset $\theta $ is added. On the sum the step-function is applied i.e. the output $out(t)$ is $1$ if the sum is greater $0$ and $0$ if the sum is smaller $0$ \cite  {perceptronImg}.\relax }}{11}{figure.caption.18}}
\newlabel{fig:perceptron}{{2.6}{11}{Structure of a perceptron. The input $in(t)$ is set at the input variables $x_i$ and the multiplied with the corresponding synaptic weight $w_i$ and accumulated. In addition a threshold offset $\theta $ is added. On the sum the step-function is applied i.e. the output $out(t)$ is $1$ if the sum is greater $0$ and $0$ if the sum is smaller $0$ \cite {perceptronImg}.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Model}{11}{section*.17}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Decision Function}{11}{section*.19}}
\citation{rumelhart1985learning}
\citation{Goodfellow-et-al-2016-Book}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The discrimination function of a perceptron. The discrimination function has the shape of a linear hyper plane in data space and it defined by the synaptic weight-vector $\textbf  {w}$. It divides the data space and thus the data samples into two subspaces, the positive space $\textbf  {x}^{\intercal }\textbf  {w} > 0$ and the negative space $\textbf  {x}^{\intercal }\textbf  {w} < 0$. In (a) the discrimination function is given in a two-dimensional data space and in (b) in a three-dimensional data space.\relax }}{12}{figure.caption.20}}
\newlabel{fig:discrimation}{{2.7}{12}{The discrimination function of a perceptron. The discrimination function has the shape of a linear hyper plane in data space and it defined by the synaptic weight-vector $\textbf {w}$. It divides the data space and thus the data samples into two subspaces, the positive space $\textbf {x}^{\intercal }\textbf {w} > 0$ and the negative space $\textbf {x}^{\intercal }\textbf {w} < 0$. In (a) the discrimination function is given in a two-dimensional data space and in (b) in a three-dimensional data space.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Perceptron Learning}{12}{section*.21}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Mutlilayer-Perceptron}{12}{section*.22}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Model architecture}{12}{section*.23}}
\citation{mlpImg}
\citation{mlpImg}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces A schematic multi layer perceptron with three layers \cite  {mlpImg}.\relax }}{13}{figure.caption.24}}
\newlabel{fig:mlp}{{2.8}{13}{A schematic multi layer perceptron with three layers \cite {mlpImg}.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Activation functions}{13}{section*.25}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The output of different activation functions plotted given the input.\relax }}{14}{figure.caption.26}}
\newlabel{fig:activations}{{2.9}{14}{The output of different activation functions plotted given the input.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Error functions}{14}{section*.27}}
\citation{rumelhart1985learning}
\citation{Goodfellow-et-al-2016-Book}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Backpropagation}{15}{section*.28}}
\citation{lecun1989backpropagation}
\citation{Goodfellow-et-al-2016-Book}
\citation{Goodfellow-et-al-2016-Book}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Convolutinal Neural Networks}{16}{section*.29}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Convolution}{16}{section*.30}}
\citation{simonyan2014very}
\citation{NIPS2012_4824}
\citation{simonyan2014very}
\citation{szegedy2015going}
\citation{cnnarchImg}
\citation{cnnarchImg}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces A cross correlation of a $3\times 3$ image matrix with a $2\times 2$ kernel without stride and padding. The result is a $2\times 2$ feature map.\relax }}{17}{figure.caption.31}}
\newlabel{fig:conv}{{2.10}{17}{A cross correlation of a $3\times 3$ image matrix with a $2\times 2$ kernel without stride and padding. The result is a $2\times 2$ feature map.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Convolution Layers}{17}{section*.32}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Architecture}{17}{section*.33}}
\citation{hopfield1982neural}
\citation{Goodfellow-et-al-2016-Book}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Typical architecture of a convolutional neural network with two convolution-pooling stages \cite  {cnnarchImg}.\relax }}{18}{figure.caption.34}}
\newlabel{fig:convarcitecuture}{{2.11}{18}{Typical architecture of a convolutional neural network with two convolution-pooling stages \cite {cnnarchImg}.\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Training}{18}{section*.35}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Hopfield Nets}{18}{section*.36}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Model}{18}{section*.37}}
\citation{hopfImg}
\citation{hopfImg}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces A blueprint of a Hopfield nets with 8 binary units. The units are connected with symmetric undirected connections \cite  {hopfImg}.\relax }}{19}{figure.caption.38}}
\newlabel{fig:hopfiled}{{2.12}{19}{A blueprint of a Hopfield nets with 8 binary units. The units are connected with symmetric undirected connections \cite {hopfImg}.\relax }{figure.caption.38}{}}
\citation{ackley1985learning}
\citation{Goodfellow-et-al-2016-Book}
\citation{boltzImg}
\citation{boltzImg}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Boltzmann Machines}{20}{section*.39}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Model}{20}{section*.40}}
\citation{ackley1985learning}
\citation{hinton2002training}
\citation{Woodford2002}
\citation{Bengio2009}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces A Boltzmann machine with 7 units. In contrast to a Hopfield nets, units are divided into visible and hidden/ unobserved units with stochastic activations \cite  {boltzImg}.\relax }}{21}{figure.caption.41}}
\newlabel{fig:bm}{{2.13}{21}{A Boltzmann machine with 7 units. In contrast to a Hopfield nets, units are divided into visible and hidden/ unobserved units with stochastic activations \cite {boltzImg}.\relax }{figure.caption.41}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Learning Rule}{21}{section*.42}}
\citation{smolensky1986information}
\citation{hinton2010practical}
\citation{Fischer2014}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline RBMs}{22}{section*.43}}
\citation{rbmImg}
\citation{rbmImg}
\citation{hinton2002training}
\citation{cdImg}
\citation{cdImg}
\citation{tieleman2008training}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces A restricted Boltzmann machine is special kind of Boltzmann machine with no lateral connections in the hidden and visible layer. This eases sampling, since the visible are only dependent on the hidden units and the hidden units only on the visible units \cite  {rbmImg}.\relax }}{23}{figure.caption.44}}
\newlabel{fig:rbm}{{2.14}{23}{A restricted Boltzmann machine is special kind of Boltzmann machine with no lateral connections in the hidden and visible layer. This eases sampling, since the visible are only dependent on the hidden units and the hidden units only on the visible units \cite {rbmImg}.\relax }{figure.caption.44}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces A temporal unrolling of the contrastive divergence algorithm with $k$ sampling step. The hidden units and the visible units are alternatingly sampled conditioned on the current state of the other \cite  {cdImg}.\relax }}{23}{figure.caption.46}}
\newlabel{fig:cd}{{2.15}{23}{A temporal unrolling of the contrastive divergence algorithm with $k$ sampling step. The hidden units and the visible units are alternatingly sampled conditioned on the current state of the other \cite {cdImg}.\relax }{figure.caption.46}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline CD-k Training}{23}{section*.45}}
\citation{hinton2006fast}
\citation{hinton2009deep}
\citation{Goodfellow-et-al-2016-Book}
\citation{cdImg}
\citation{cdImg}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Building up a deep belief network, by training RBMs greedily and stacking them up on top of each other. At first only one RBM is trained. On top of the first RBM the next RBM is trained. This can procedure can be performed for an arbitrary number of repetitions. In the top layer "association" RBM the label information $y$ can be in-cooperated as well \cite  {cdImg}.\relax }}{24}{figure.caption.50}}
\newlabel{fig:dbn}{{2.16}{24}{Building up a deep belief network, by training RBMs greedily and stacking them up on top of each other. At first only one RBM is trained. On top of the first RBM the next RBM is trained. This can procedure can be performed for an arbitrary number of repetitions. In the top layer "association" RBM the label information $y$ can be in-cooperated as well \cite {cdImg}.\relax }{figure.caption.50}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Persistent CD}{24}{section*.47}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Deep Belief Networks}{24}{section*.48}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Training}{24}{section*.49}}
\citation{hinton1995wake}
\citation{maass1997networks}
\citation{abbott1999lapicque}
\citation{gerstner2014neuronal}
\citation{Petrovici2016}
\citation{heikoMA}
\citation{heikoMA}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Fine-tuning}{25}{section*.51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Spiking neural networks}{25}{subsection.2.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Neuron Models}{25}{section*.52}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline LIF}{25}{section*.53}}
\citation{gerstner2014neuronal}
\citation{gerstner2014neuronal}
\citation{Hodgkin1952}
\citation{gerstner2014neuronal}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces A LIF neuron as an electrical circuit. The capacitor $C_m$ corresponds to the potential at the membrane, $g_l$ the leakage potential and $E_l$ the resting potential. If the membrane potential is greater than $U_t$ a spike $\delta $ is emitted \cite  {heikoMA}.\relax }}{26}{figure.caption.54}}
\newlabel{fig:lif}{{2.17}{26}{A LIF neuron as an electrical circuit. The capacitor $C_m$ corresponds to the potential at the membrane, $g_l$ the leakage potential and $E_l$ the resting potential. If the membrane potential is greater than $U_t$ a spike $\delta $ is emitted \cite {heikoMA}.\relax }{figure.caption.54}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Hodgkin-Huxley}{26}{section*.56}}
\citation{heikoMA}
\citation{heikoMA}
\citation{Heeger2000}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces Different firing behaviour observed in the Brain. In (a) a neuron shows constant tonic firing, while in (b) the neuron shows frequency adaptation and in (c) the neurons shows delayed regular bursting \cite  {gerstner2014neuronal}.\relax }}{27}{figure.caption.55}}
\newlabel{fig:neuronbe}{{2.18}{27}{Different firing behaviour observed in the Brain. In (a) a neuron shows constant tonic firing, while in (b) the neuron shows frequency adaptation and in (c) the neurons shows delayed regular bursting \cite {gerstner2014neuronal}.\relax }{figure.caption.55}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Activity in a network}{27}{section*.58}}
\citation{Petrovici2016}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces A Hodgkin-Huxley as an electrical circuit. The membrane potential corresponds to $C_m$, and the ion channels to $g_{Na}$, $g_{k}$, $g_{L}$ with their reversal potentials $E_{Na}$, $E_{}$, $E_{L}$ \cite  {heikoMA}.\relax }}{28}{figure.caption.57}}
\newlabel{fig:hogdehux}{{2.19}{28}{A Hodgkin-Huxley as an electrical circuit. The membrane potential corresponds to $C_m$, and the ion channels to $g_{Na}$, $g_{k}$, $g_{L}$ with their reversal potentials $E_{Na}$, $E_{}$, $E_{L}$ \cite {heikoMA}.\relax }{figure.caption.57}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Synapses}{28}{section*.59}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Current-based synaptic interaction}{28}{section*.60}}
\citation{Petrovici2016}
\citation{Petrovici2016}
\citation{Petrovici2016}
\citation{Petrovici2016}
\citation{Petrovici2016}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Conductance-based synaptic interaction}{29}{section*.61}}
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline High conductance state}{29}{section*.62}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Kernel function}{29}{section*.65}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces Three samples of Ornstein\IeC {\textendash }Uhlenbeck processes. This can be seen as the membrane potential of neurons in a high conductance state (in comparison to the black doted line can be seen as a neuron without noisy input) \cite  {Petrovici2016}.\relax }}{30}{figure.caption.63}}
\newlabel{fig:ornuhl}{{2.20}{30}{Three samples of Ornstein–Uhlenbeck processes. This can be seen as the membrane potential of neurons in a high conductance state (in comparison to the black doted line can be seen as a neuron without noisy input) \cite {Petrovici2016}.\relax }{figure.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces A membrane potential trajectory of a neuron in a high conductance state. (a) The spike train and the membrane potential. (b) The blue curve represents the actual membrane potential with refractory periods after each spike, while the red curve represents a membrane potential without a firing threshold. It is apparent, that the blue curve returns the the hypothetical red potential without a "return from rest" time \cite  {Petrovici2016}. \relax }}{30}{figure.caption.64}}
\newlabel{fig:hcs}{{2.21}{30}{A membrane potential trajectory of a neuron in a high conductance state. (a) The spike train and the membrane potential. (b) The blue curve represents the actual membrane potential with refractory periods after each spike, while the red curve represents a membrane potential without a firing threshold. It is apparent, that the blue curve returns the the hypothetical red potential without a "return from rest" time \cite {Petrovici2016}. \relax }{figure.caption.64}{}}
\citation{Petrovici2016}
\citation{Petrovici2016}
\citation{Meftah2013}
\@writefile{lof}{\contentsline {figure}{\numberline {2.22}{\ignorespaces Three different PSP kernels. The green one has an alpha-shape, the blue one is exponential shaped and the red one is rectangular \cite  {Petrovici2016}. \relax }}{31}{figure.caption.66}}
\newlabel{fig:pspkernels}{{2.22}{31}{Three different PSP kernels. The green one has an alpha-shape, the blue one is exponential shaped and the red one is rectangular \cite {Petrovici2016}. \relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Neural Coding}{31}{section*.67}}
\citation{gerstner2014neuronal}
\citation{gerstner2014neuronal}
\citation{Sjostrom2010}
\citation{Buchanan2010}
\citation{Buchanan2010}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Learning}{32}{section*.68}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Spike time depended plasticity}{32}{section*.69}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Long-term potentiation}{32}{section*.70}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Long-term depression}{32}{section*.71}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.23}{\ignorespaces Different STDP curves observed in the Brain. The first curve shows only long term potentiation. The middle one shows a variant of the classic STDP Curve and the last one shows purely long term depression \cite  {Buchanan2010}.\relax }}{33}{figure.caption.72}}
\newlabel{fig:stdp}{{2.23}{33}{Different STDP curves observed in the Brain. The first curve shows only long term potentiation. The middle one shows a variant of the classic STDP Curve and the last one shows purely long term depression \cite {Buchanan2010}.\relax }{figure.caption.72}{}}
\@setckpt{content/backgrnd}{
\setcounter{page}{34}
\setcounter{equation}{0}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{23}
\setcounter{table}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{Item}{16}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{15}
\setcounter{section@level}{4}
}
