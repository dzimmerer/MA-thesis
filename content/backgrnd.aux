\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{3}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Probabilistic Graphical Models}{3}{section.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Bayesian network}{3}{subsection.2.1.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sub1}{{2.1a}{4}{A Bayesian with 5 RVs.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:sub1}{{a}{4}{A Bayesian with 5 RVs.\relax }{figure.caption.3}{}}
\newlabel{fig:sub2}{{2.1b}{4}{Tabular conditional probability $p(c |a , b)$.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:sub2}{{b}{4}{Tabular conditional probability $p(c |a , b)$.\relax }{figure.caption.3}{}}
\newlabel{fig:bayesnet}{{2.1c}{4}{Conditional probability $p(c |a , b)$ given by a function over $a,b$.\relax }{figure.caption.3}{}}
\newlabel{sub@fig:bayesnet}{{c}{4}{Conditional probability $p(c |a , b)$ given by a function over $a,b$.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces A sample Bayesian network with 5 nodes. (a) In the network RV $c$ is directly depended on $a$, $b$ and thus $c$ is the child of its parents $a$, $b$. The RVs $d$, $e$ are the children of $c$. Since there is a path from $a$ to $e$, $a$ is an ancestor of its descended $e$. (b) The conditional probability of $p(c |a , b)$ in a tabular form. Another variant of the conditional probability $p(c |a , b)$ is given in (c). The probability is defined by the parameters $w_{ac}$, $w_{bc}$, and due to the sigmoid activation function $\sigma $ a network with such probability functions is called sigmoid belief network.\relax }}{4}{figure.caption.3}}
\newlabel{fig:test}{{2.1}{4}{A sample Bayesian network with 5 nodes. (a) In the network RV $c$ is directly depended on $a$, $b$ and thus $c$ is the child of its parents $a$, $b$. The RVs $d$, $e$ are the children of $c$. Since there is a path from $a$ to $e$, $a$ is an ancestor of its descended $e$. (b) The conditional probability of $p(c |a , b)$ in a tabular form. Another variant of the conditional probability $p(c |a , b)$ is given in (c). The probability is defined by the parameters $w_{ac}$, $w_{bc}$, and due to the sigmoid activation function $\sigma $ a network with such probability functions is called sigmoid belief network.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Markov Random Field}{4}{subsection.2.1.2}}
\newlabel{fig:sub1}{{2.2a}{5}{A Markov network with with 5 nodes.\relax }{figure.caption.4}{}}
\newlabel{sub@fig:sub1}{{a}{5}{A Markov network with with 5 nodes.\relax }{figure.caption.4}{}}
\newlabel{fig:markovnet}{{2.2b}{5}{Cliques in a Markov network\relax }{figure.caption.4}{}}
\newlabel{sub@fig:markovnet}{{b}{5}{Cliques in a Markov network\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces (a) A Markov network with 5 nodes. The white node is depended on all connected nodes (blue nodes). Given its the blue nodes the white node is independent on any other node in the network. (b) Two cliques in a Markov network. The blue clique is maximal, since no vertex can be added, which is fully connected to all others in the blue clique. The green one is not maximal, since the node $x_3$ could be added.\relax }}{5}{figure.caption.4}}
\newlabel{fig:test}{{2.2}{5}{(a) A Markov network with 5 nodes. The white node is depended on all connected nodes (blue nodes). Given its the blue nodes the white node is independent on any other node in the network. (b) Two cliques in a Markov network. The blue clique is maximal, since no vertex can be added, which is fully connected to all others in the blue clique. The green one is not maximal, since the node $x_3$ could be added.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Energy-Based Models}{5}{subsection.2.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Sampling at discrete points $S_i$ in an simple distribution. The samples $S_i$ approximate the true density function. As more samples are drawn, the approximation will represent the function more exact.\relax }}{6}{figure.caption.5}}
\newlabel{fig:Sampling}{{2.3}{6}{Sampling at discrete points $S_i$ in an simple distribution. The samples $S_i$ approximate the true density function. As more samples are drawn, the approximation will represent the function more exact.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Sampling}{6}{subsection.2.1.4}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Ancestral Sampling}{6}{section*.6}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Markov Chain Monte Carlo}{6}{section*.7}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Gibbs sampling}{7}{section*.8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Neural Networks}{7}{section.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Natural}{7}{subsection.2.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Brain}{7}{section*.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces A small section in the Brain. The neurons $a$ - $g$ are connected to other neurons in a complex network.\relax }}{8}{figure.caption.10}}
\newlabel{fig:brain}{{2.4}{8}{A small section in the Brain. The neurons $a$ - $g$ are connected to other neurons in a complex network.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Neuron}{8}{section*.11}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces A schematic view of a natural neuron. Other neurons are pre-synaptic connected via the dendrites. The signals are then forwarded and accumulated in the soma and from there on via the in myelin sheath cover axon to the axon terminal and the out going synapses.\relax }}{9}{figure.caption.12}}
\newlabel{fig:neurpn}{{2.5}{9}{A schematic view of a natural neuron. Other neurons are pre-synaptic connected via the dendrites. The signals are then forwarded and accumulated in the soma and from there on via the in myelin sheath cover axon to the axon terminal and the out going synapses.\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Learning}{9}{section*.13}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Synaptic Plasticity}{9}{section*.14}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Structure of a perceptron. The input $in(t)$ is set at the input variables $x_i$ and the multiplied with the corresponding synaptic weight $w_i$ and accumulated. In addition a threshold offset $\theta $ is added. On the sum the step-function is applied i.e. the output $out(t)$ is $1$ if the sum is greater $0$ and $0$ if the sum is smaller $0$.\relax }}{10}{figure.caption.18}}
\newlabel{fig:perceptron}{{2.6}{10}{Structure of a perceptron. The input $in(t)$ is set at the input variables $x_i$ and the multiplied with the corresponding synaptic weight $w_i$ and accumulated. In addition a threshold offset $\theta $ is added. On the sum the step-function is applied i.e. the output $out(t)$ is $1$ if the sum is greater $0$ and $0$ if the sum is smaller $0$.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline "What fires together, wires together"}{10}{section*.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Artificial neural networks}{10}{subsection.2.2.2}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Perceptron}{10}{section*.16}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Model}{10}{section*.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces The discrimination function of a perceptron. The discrimination function has the shape of a linear hyper plane in data space and it defined by the synaptic weight-vector $\textbf  {w}$. It divides the data space and thus the data samples into two subspaces, the positive space $\textbf  {x}^{\intercal }\textbf  {w} > 0$ and the negative space $\textbf  {x}^{\intercal }\textbf  {w} < 0$.\relax }}{11}{figure.caption.20}}
\newlabel{fig:discrimation}{{2.7}{11}{The discrimination function of a perceptron. The discrimination function has the shape of a linear hyper plane in data space and it defined by the synaptic weight-vector $\textbf {w}$. It divides the data space and thus the data samples into two subspaces, the positive space $\textbf {x}^{\intercal }\textbf {w} > 0$ and the negative space $\textbf {x}^{\intercal }\textbf {w} < 0$.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Decision Function}{11}{section*.19}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Perceptron Learning}{11}{section*.21}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Mutlilayer-Perceptron}{12}{section*.22}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Model architecture}{12}{section*.23}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces A schematic multi layer perceptron with four layers.\relax }}{13}{figure.caption.24}}
\newlabel{fig:mlp}{{2.8}{13}{A schematic multi layer perceptron with four layers.\relax }{figure.caption.24}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Activation functions}{13}{section*.25}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Error functions}{13}{section*.27}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces The output of different activation functions plotted given the input.\relax }}{14}{figure.caption.26}}
\newlabel{fig:activations}{{2.9}{14}{The output of different activation functions plotted given the input.\relax }{figure.caption.26}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Backpropagation}{14}{section*.28}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Convolutinal Neural Networks}{15}{section*.29}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Convolution}{15}{section*.30}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Convolving or to be more exact a cross correlation of a $3\times 3$ image matrix with a $2\times 2$ kernel without stride and padding. The result is a $2\times 2$ feature map.\relax }}{16}{figure.caption.31}}
\newlabel{fig:conv}{{2.10}{16}{Convolving or to be more exact a cross correlation of a $3\times 3$ image matrix with a $2\times 2$ kernel without stride and padding. The result is a $2\times 2$ feature map.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Convolution Layers}{16}{section*.32}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Typical architecture of a convolutional neural network with two convolution-pooling stages.\relax }}{17}{figure.caption.34}}
\newlabel{fig:convarcitecuture}{{2.11}{17}{Typical architecture of a convolutional neural network with two convolution-pooling stages.\relax }{figure.caption.34}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Architecture}{17}{section*.33}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Training}{17}{section*.35}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Hopfield Nets}{18}{section*.36}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Model}{18}{section*.37}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Properties}{18}{section*.39}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces A blueprint of a Hopfield nets with 8 binary units. The units are connected with symmetric undirected connections.\relax }}{19}{figure.caption.38}}
\newlabel{fig:hopfiled}{{2.12}{19}{A blueprint of a Hopfield nets with 8 binary units. The units are connected with symmetric undirected connections.\relax }{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Boltzmann Machines}{19}{section*.40}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Model}{19}{section*.41}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces A Boltzmann machine with 7 units. In contrast to a Hopfield nets, units are divided into visible and hidden/ unobserved units with stochastic activations.\relax }}{20}{figure.caption.42}}
\newlabel{fig:bm}{{2.13}{20}{A Boltzmann machine with 7 units. In contrast to a Hopfield nets, units are divided into visible and hidden/ unobserved units with stochastic activations.\relax }{figure.caption.42}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Learning Rule}{20}{section*.43}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline RBMs}{21}{section*.44}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces A restricted Boltzmann machine is special kind of Boltzmann machine with no lateral connections in the hidden and visible layer. This eases sampling, since the visible are only dependent on the hidden units and the hidden units only on the visible units.\relax }}{22}{figure.caption.45}}
\newlabel{fig:rbm}{{2.14}{22}{A restricted Boltzmann machine is special kind of Boltzmann machine with no lateral connections in the hidden and visible layer. This eases sampling, since the visible are only dependent on the hidden units and the hidden units only on the visible units.\relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline CD-l Training}{22}{section*.46}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Persistent CD}{22}{section*.48}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Deep Belief Networks}{22}{section*.49}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces A temporal unrolling of the contrastive divergence algorithm with $k$ sampling step. The hidden units and the visible units are alternatingly sampled conditioned on the current state of the other.\relax }}{23}{figure.caption.47}}
\newlabel{fig:cd}{{2.15}{23}{A temporal unrolling of the contrastive divergence algorithm with $k$ sampling step. The hidden units and the visible units are alternatingly sampled conditioned on the current state of the other.\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Building up a deep belief network, by training RBMs greedily and stacking them up on top of each other. In the top layer "association" RBM the label information $y$ can be incooperated as well.\relax }}{23}{figure.caption.51}}
\newlabel{fig:dbn}{{2.16}{23}{Building up a deep belief network, by training RBMs greedily and stacking them up on top of each other. In the top layer "association" RBM the label information $y$ can be incooperated as well.\relax }{figure.caption.51}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Training}{23}{section*.50}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Fine-tuning}{23}{section*.52}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces A figure.\relax }}{24}{figure.caption.55}}
\newlabel{fig:test}{{2.17}{24}{A figure.\relax }{figure.caption.55}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Spiking neural networks}{24}{subsection.2.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Neuron Models}{24}{section*.53}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline LIF}{24}{section*.54}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Hodgkin-Huxley}{25}{section*.56}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.18}{\ignorespaces A figure.\relax }}{26}{figure.caption.57}}
\newlabel{fig:test}{{2.18}{26}{A figure.\relax }{figure.caption.57}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Poisson}{26}{section*.58}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Synapses}{27}{section*.59}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Current-based synaptic interaction}{27}{section*.60}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Conductance-based synaptic interaction}{27}{section*.61}}
\@writefile{toc}{\contentsline {subparagraph}{\nonumberline High conductance state}{27}{section*.62}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.19}{\ignorespaces A figure.\relax }}{28}{figure.caption.63}}
\newlabel{fig:test}{{2.19}{28}{A figure.\relax }{figure.caption.63}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.20}{\ignorespaces A figure.\relax }}{28}{figure.caption.64}}
\newlabel{fig:test}{{2.20}{28}{A figure.\relax }{figure.caption.64}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Kernel function}{28}{section*.65}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.21}{\ignorespaces A figure.\relax }}{29}{figure.caption.66}}
\newlabel{fig:test}{{2.21}{29}{A figure.\relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Neural Coding}{29}{section*.67}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Learning}{29}{section*.68}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Spike time depended plasticity}{29}{section*.69}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.22}{\ignorespaces A figure.\relax }}{30}{figure.caption.72}}
\newlabel{fig:test}{{2.22}{30}{A figure.\relax }{figure.caption.72}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Long-term potentiation}{30}{section*.70}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Long-term depression}{30}{section*.71}}
\@setckpt{content/backgrnd}{
\setcounter{page}{31}
\setcounter{equation}{0}
\setcounter{enumi}{3}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{2}
\setcounter{subsection}{3}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{22}
\setcounter{table}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{Item}{15}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{15}
\setcounter{section@level}{4}
}
