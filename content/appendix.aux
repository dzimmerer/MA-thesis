\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix}{75}{appendix.A}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{lot}{\contentsline {table}{\numberline {A.1}{\ignorespaces Current-based LIF neuron parameters used for the converted CNN\relax }}{75}{table.caption.131}}
\newlabel{my-label}{{A.1}{75}{Current-based LIF neuron parameters used for the converted CNN\relax }{table.caption.131}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.2}{\ignorespaces Current-based LIF neuron parameters used for the converted DBN\relax }}{76}{table.caption.132}}
\newlabel{my-label}{{A.2}{76}{Current-based LIF neuron parameters used for the converted DBN\relax }{table.caption.132}{}}
\@writefile{lot}{\contentsline {table}{\numberline {A.3}{\ignorespaces Conductance-based LIF neuron parameters used for the converted DBN\relax }}{76}{table.caption.133}}
\newlabel{my-label}{{A.3}{76}{Conductance-based LIF neuron parameters used for the converted DBN\relax }{table.caption.133}{}}
\newlabel{fig:sub1}{{A.1a}{77}{Spike without lateral inhibition.\relax }{figure.caption.134}{}}
\newlabel{sub@fig:sub1}{{a}{77}{Spike without lateral inhibition.\relax }{figure.caption.134}{}}
\newlabel{fig:sub2}{{A.1b}{77}{Spike with small lateral inhibition.\relax }{figure.caption.134}{}}
\newlabel{sub@fig:sub2}{{b}{77}{Spike with small lateral inhibition.\relax }{figure.caption.134}{}}
\newlabel{fig:sub2}{{A.1c}{77}{Spike with big lateral inhibition.\relax }{figure.caption.134}{}}
\newlabel{sub@fig:sub2}{{c}{77}{Spike with big lateral inhibition.\relax }{figure.caption.134}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Spikes in a hidden layer of a spiking RBM with lateral inhibition. As the lateral inhibition increases, the activity becomes more sparse. In the case with big inhibitory weights, there are a few neurons dominating the activity allowing only a few different states of the network and thus poor mode switching in a training step. In a network with small inhibitory weights, there is sparse activity, but the the network visits many different states in a simulation step and shows sufficient mode mixing. \relax }}{77}{figure.caption.134}}
\newlabel{fig:stripes}{{A.1}{77}{Spikes in a hidden layer of a spiking RBM with lateral inhibition. As the lateral inhibition increases, the activity becomes more sparse. In the case with big inhibitory weights, there are a few neurons dominating the activity allowing only a few different states of the network and thus poor mode switching in a training step. In a network with small inhibitory weights, there is sparse activity, but the the network visits many different states in a simulation step and shows sufficient mode mixing. \relax }{figure.caption.134}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 88}}{78}{Appendix}{figure.caption.135}{}}
\newlabel{sub@fig:sub1}{{}{78}{Appendix}{figure.caption.135}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 94}}{78}{Appendix}{figure.caption.135}{}}
\newlabel{sub@fig:sub1}{{a}{78}{Appendix}{figure.caption.135}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 101}}{78}{Appendix}{figure.caption.135}{}}
\newlabel{sub@fig:sub1}{{a}{78}{Appendix}{figure.caption.135}{}}
\newlabel{fig:sub2}{{\caption@xref {fig:sub2}{ on input line 107}}{78}{Appendix}{figure.caption.135}{}}
\newlabel{sub@fig:sub2}{{b}{78}{Appendix}{figure.caption.135}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 114}}{78}{Appendix}{figure.caption.135}{}}
\newlabel{sub@fig:sub1}{{b}{78}{Appendix}{figure.caption.135}{}}
\newlabel{fig:sub2}{{\caption@xref {fig:sub2}{ on input line 120}}{78}{Appendix}{figure.caption.135}{}}
\newlabel{sub@fig:sub2}{{c}{78}{Appendix}{figure.caption.135}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Activity in the visible layer of the top RBM in a DBNs with different architectures. The first two architectures model separate RBM layers which are stacked by one-on-one forward connections from the hidden layer of the bottom RBM to the visible layer of the top RBM, while in the last architecture, the RBMs are directly stacked, i.e. the hidden layer of the bottom RBM is the visible layer of the top RBM. While in the top two architectures, there are hardly any differences and the input is modelled correctly, due to top down influences the output of the bottom RBM gets distorted by activity in the hidden layer of the top RBM.\relax }}{78}{figure.caption.135}}
\newlabel{fig:stripes}{{A.2}{78}{Activity in the visible layer of the top RBM in a DBNs with different architectures. The first two architectures model separate RBM layers which are stacked by one-on-one forward connections from the hidden layer of the bottom RBM to the visible layer of the top RBM, while in the last architecture, the RBMs are directly stacked, i.e. the hidden layer of the bottom RBM is the visible layer of the top RBM. While in the top two architectures, there are hardly any differences and the input is modelled correctly, due to top down influences the output of the bottom RBM gets distorted by activity in the hidden layer of the top RBM.\relax }{figure.caption.135}{}}
\newlabel{fig:sub1}{{A.3a}{79}{Weights at the beginning.\relax }{figure.caption.136}{}}
\newlabel{sub@fig:sub1}{{a}{79}{Weights at the beginning.\relax }{figure.caption.136}{}}
\newlabel{fig:sub2}{{A.3b}{79}{Weights after first positive phase.\relax }{figure.caption.136}{}}
\newlabel{sub@fig:sub2}{{b}{79}{Weights after first positive phase.\relax }{figure.caption.136}{}}
\newlabel{fig:sub2}{{A.3c}{79}{Weights after first negative phase.\relax }{figure.caption.136}{}}
\newlabel{sub@fig:sub2}{{c}{79}{Weights after first negative phase.\relax }{figure.caption.136}{}}
\newlabel{fig:sub2}{{A.3d}{79}{Weights after a few training steps.\relax }{figure.caption.136}{}}
\newlabel{sub@fig:sub2}{{d}{79}{Weights after a few training steps.\relax }{figure.caption.136}{}}
\newlabel{fig:sub2}{{A.3e}{79}{Spikes activity development.\relax }{figure.caption.136}{}}
\newlabel{sub@fig:sub2}{{e}{79}{Spikes activity development.\relax }{figure.caption.136}{}}
\newlabel{fig:sub2}{{A.3f}{79}{Weights after training with different positive and negative learning rates.\relax }{figure.caption.136}{}}
\newlabel{sub@fig:sub2}{{f}{79}{Weights after training with different positive and negative learning rates.\relax }{figure.caption.136}{}}
\newlabel{fig:sub2}{{A.3g}{79}{Weights without any synchronization.\relax }{figure.caption.136}{}}
\newlabel{sub@fig:sub2}{{g}{79}{Weights without any synchronization.\relax }{figure.caption.136}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Weights development for eCD with synchronous weight updates. The first column shows the development. After the positive the weights increased a lot, which leads to a high decrement during the negative which leads to a "dying out" of the spikes, which is visualized in the second column. Using an increased positive phase with a higher learning rate, does not solve the problem and leads to a division of the weights into a few very positive weights and many negative weights. In contrast to this a "normal" weight distribution without any weight synchronization in presented in the last column. \relax }}{79}{figure.caption.136}}
\newlabel{fig:stripes}{{A.3}{79}{Weights development for eCD with synchronous weight updates. The first column shows the development. After the positive the weights increased a lot, which leads to a high decrement during the negative which leads to a "dying out" of the spikes, which is visualized in the second column. Using an increased positive phase with a higher learning rate, does not solve the problem and leads to a division of the weights into a few very positive weights and many negative weights. In contrast to this a "normal" weight distribution without any weight synchronization in presented in the last column. \relax }{figure.caption.136}{}}
\@setckpt{content/appendix}{
\setcounter{page}{80}
\setcounter{equation}{0}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{1}
\setcounter{section}{0}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{3}
\setcounter{table}{3}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{7}
\setcounter{subtable}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{4}
\setcounter{ALG@line}{4}
\setcounter{ALG@rem}{4}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{Item}{27}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{47}
\setcounter{section@level}{2}
}
