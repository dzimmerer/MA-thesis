\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{lecun-mnisthandwrittendigit-2010}
\citation{lecun-mnisthandwrittendigit-2010}
\citation{lecun-mnisthandwrittendigit-2010}
\citation{serrano2013128}
\citation{serrano2013128}
\citation{serrano2013128}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Experiments \& Results}{55}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{c:expres}{{6}{55}{Experiments \& Results}{chapter.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Datasets}{55}{section.6.1}}
\newlabel{c:datasets}{{6.1}{55}{Datasets}{section.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Stripe Dataset}{55}{subsection.6.1.1}}
\newlabel{c:stripes}{{6.1.1}{55}{Stripe Dataset}{subsection.6.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.2}MNIST}{55}{subsection.6.1.2}}
\newlabel{c:mnist}{{6.1.2}{55}{MNIST}{subsection.6.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.3}Poker-DVS}{55}{subsection.6.1.3}}
\newlabel{c:pokerdvs}{{6.1.3}{55}{Poker-DVS}{subsection.6.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.4}Ball-Can-Pen-DVS}{55}{subsection.6.1.4}}
\newlabel{c:bcpdvs}{{6.1.4}{55}{Ball-Can-Pen-DVS}{subsection.6.1.4}{}}
\newlabel{fig:stripes1}{{6.1a}{56}{\relax }{figure.caption.95}{}}
\newlabel{sub@fig:stripes1}{{a}{56}{\relax }{figure.caption.95}{}}
\newlabel{fig:stripes2}{{6.1b}{56}{\relax }{figure.caption.95}{}}
\newlabel{sub@fig:stripes2}{{b}{56}{\relax }{figure.caption.95}{}}
\newlabel{fig:stripes3}{{6.1c}{56}{\relax }{figure.caption.95}{}}
\newlabel{sub@fig:stripes3}{{c}{56}{\relax }{figure.caption.95}{}}
\newlabel{fig:stripes4}{{6.1d}{56}{\relax }{figure.caption.95}{}}
\newlabel{sub@fig:stripes4}{{d}{56}{\relax }{figure.caption.95}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Samples from the $10 \times 10$ pixel stripe dataset. The stripes in (a) and (c) have the same position in the image, while the stripes in (b) and (d) can appear anywhere in the image. In (a) and (b) the images are binary , i.e a pixel value $p \in \{0,1\}$, while in (c) and (d) the values are continuous i.e $p \in [0,1 ]$. \relax }}{56}{figure.caption.95}}
\newlabel{fig:stripes}{{6.1}{56}{Samples from the $10 \times 10$ pixel stripe dataset. The stripes in (a) and (c) have the same position in the image, while the stripes in (b) and (d) can appear anywhere in the image. In (a) and (b) the images are binary , i.e a pixel value $p \in \{0,1\}$, while in (c) and (d) the values are continuous i.e $p \in \lbrack 0,1 \rbrack $. \relax }{figure.caption.95}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces Samples from the $28 \times 28$ pixel MNIST dataset \cite  {lecun-mnisthandwrittendigit-2010}. The pixel values $p$ are scales to be in the interval $p \in [0,1 ]$. \relax }}{56}{figure.caption.96}}
\newlabel{fig:mnist}{{6.2}{56}{Samples from the $28 \times 28$ pixel MNIST dataset \cite {lecun-mnisthandwrittendigit-2010}. The pixel values $p$ are scales to be in the interval $p \in \lbrack 0,1 \rbrack $. \relax }{figure.caption.96}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Visualization of samples from the Poker-DVS dataset \cite  {serrano2013128}. The images are generated by integrating all events over $8$ ms. The actual training is performed on the events.\relax }}{57}{figure.caption.97}}
\newlabel{fig:pokerdvs}{{6.3}{57}{Visualization of samples from the Poker-DVS dataset \cite {serrano2013128}. The images are generated by integrating all events over $8$ ms. The actual training is performed on the events.\relax }{figure.caption.97}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces  \relax }}{57}{figure.caption.98}}
\newlabel{fig:bcpdvsb}{{6.4}{57}{\relax }{figure.caption.98}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces  \relax }}{57}{figure.caption.99}}
\newlabel{fig:bcpdvsc}{{6.5}{57}{\relax }{figure.caption.99}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces  Recorded and down sampled samples from the Ball-Can-Pen-DVS dataset visualized by integrating all events of a sample and then normalizing them by the maximal number of events per pixel. In the first row are the original size DVS samples visualized and in the second row the scaled samples. In Figure \ref  {fig:bcpdvsb} are samples from different Balls, in Figure \ref  {fig:bcpdvsc} are samples of Cans and in Figure \ref  {fig:bcpdvsp} are samples of Pens presented. \relax }}{58}{figure.caption.100}}
\newlabel{fig:bcpdvsp}{{6.6}{58}{Recorded and down sampled samples from the Ball-Can-Pen-DVS dataset visualized by integrating all events of a sample and then normalizing them by the maximal number of events per pixel. In the first row are the original size DVS samples visualized and in the second row the scaled samples. In Figure \ref {fig:bcpdvsb} are samples from different Balls, in Figure \ref {fig:bcpdvsc} are samples of Cans and in Figure \ref {fig:bcpdvsp} are samples of Pens presented. \relax }{figure.caption.100}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Experiments}{58}{section.6.2}}
\newlabel{c:exps}{{6.2}{58}{Experiments}{section.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Computational Constrains}{58}{subsection.6.2.1}}
\newlabel{c:compconstr}{{6.2.1}{58}{Computational Constrains}{subsection.6.2.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Conversion}{59}{subsection.6.2.2}}
\newlabel{c:conversionexp}{{6.2.2}{59}{Conversion}{subsection.6.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Conversion comparison}{59}{section*.102}}
\newlabel{c:conversioncomp}{{6.2.2}{59}{Conversion comparison}{section*.102}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.7}{\ignorespaces Visualized filters $16 \times 16$ filters of the first layer convolutional RBM of a DBN trained on the $28 \times 28$ pixel MNIST dataset.\relax }}{60}{figure.caption.101}}
\newlabel{fig:rbmw}{{6.7}{60}{Visualized filters $16 \times 16$ filters of the first layer convolutional RBM of a DBN trained on the $28 \times 28$ pixel MNIST dataset.\relax }{figure.caption.101}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.1}{\ignorespaces Kullback-Leibler divergence between the activations in the feature maps.\relax }}{60}{table.caption.103}}
\newlabel{tab:kldiv}{{6.1}{60}{Kullback-Leibler divergence between the activations in the feature maps.\relax }{table.caption.103}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.2}{\ignorespaces Classification performances of the converted spiking DBNs to a the artificial DBN on a subset of 100 samples.\relax }}{60}{table.caption.104}}
\newlabel{tab:convperf}{{6.2}{60}{Classification performances of the converted spiking DBNs to a the artificial DBN on a subset of 100 samples.\relax }{table.caption.104}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.8}{\ignorespaces Activations in the features maps in a artificial convolutional DBN and the converted spiking network architectures. \relax }}{61}{figure.caption.105}}
\newlabel{fig:convacts}{{6.8}{61}{Activations in the features maps in a artificial convolutional DBN and the converted spiking network architectures. \relax }{figure.caption.105}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 437}}{62}{Conversion comparison}{figure.caption.106}{}}
\newlabel{sub@fig:sub1}{{}{62}{Conversion comparison}{figure.caption.106}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 442}}{62}{Conversion comparison}{figure.caption.106}{}}
\newlabel{sub@fig:sub1}{{}{62}{Conversion comparison}{figure.caption.106}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 447}}{62}{Conversion comparison}{figure.caption.106}{}}
\newlabel{sub@fig:sub1}{{}{62}{Conversion comparison}{figure.caption.106}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 452}}{62}{Conversion comparison}{figure.caption.106}{}}
\newlabel{sub@fig:sub1}{{}{62}{Conversion comparison}{figure.caption.106}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.9}{\ignorespaces Misclassifications of the spiking DBNs. Often $9$ and $4$ are mixed up. The first three are classified as a $9$, the fourth one is classified as a $4$. The images all share a lot of features with the suggested class and the correct class was always the second most probable.\relax }}{62}{figure.caption.106}}
\newlabel{fig:mismnisthum}{{6.9}{62}{Misclassifications of the spiking DBNs. Often $9$ and $4$ are mixed up. The first three are classified as a $9$, the fourth one is classified as a $4$. The images all share a lot of features with the suggested class and the correct class was always the second most probable.\relax }{figure.caption.106}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.3}{\ignorespaces Classification performances of the converted spiking DBNs with different simulated runtimes.\relax }}{62}{table.caption.107}}
\newlabel{tab:pervovert}{{6.3}{62}{Classification performances of the converted spiking DBNs with different simulated runtimes.\relax }{table.caption.107}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}eCD}{62}{subsection.6.2.3}}
\newlabel{c:ecdexp}{{6.2.3}{62}{eCD}{subsection.6.2.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6.4}{\ignorespaces eCD-parameters for the experiments\relax }}{62}{table.caption.108}}
\newlabel{tab:ecdrunparam}{{6.4}{62}{eCD-parameters for the experiments\relax }{table.caption.108}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.10}{\ignorespaces Positive phase and negative phase of a diagonal stripe with 4 filters. In the first 4 images sets a stripe is learned, while during the last 4 images the model distribution is unlearned.\relax }}{63}{figure.caption.109}}
\newlabel{fig:ecdstrlearn}{{6.10}{63}{Positive phase and negative phase of a diagonal stripe with 4 filters. In the first 4 images sets a stripe is learned, while during the last 4 images the model distribution is unlearned.\relax }{figure.caption.109}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Learning the data distribution}{63}{section*.111}}
\newlabel{c:datadistexp}{{6.2.3}{63}{Learning the data distribution}{section*.111}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Lateral connections}{63}{section*.114}}
\newlabel{c:latinhibexp}{{6.2.3}{63}{Lateral connections}{section*.114}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.11}{\ignorespaces Accumulated weight change for each training sample. As more samples are presented and the weights specialize, the total weight updates decrease.\relax }}{64}{figure.caption.110}}
\newlabel{fig:ecdwdiff}{{6.11}{64}{Accumulated weight change for each training sample. As more samples are presented and the weights specialize, the total weight updates decrease.\relax }{figure.caption.110}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 731}}{64}{Learning the data distribution}{figure.caption.112}{}}
\newlabel{sub@fig:sub1}{{}{64}{Learning the data distribution}{figure.caption.112}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 736}}{64}{Learning the data distribution}{figure.caption.112}{}}
\newlabel{sub@fig:sub1}{{}{64}{Learning the data distribution}{figure.caption.112}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 741}}{64}{Learning the data distribution}{figure.caption.112}{}}
\newlabel{sub@fig:sub1}{{}{64}{Learning the data distribution}{figure.caption.112}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 746}}{64}{Learning the data distribution}{figure.caption.112}{}}
\newlabel{sub@fig:sub1}{{}{64}{Learning the data distribution}{figure.caption.112}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 753}}{64}{Learning the data distribution}{figure.caption.112}{}}
\newlabel{sub@fig:sub1}{{}{64}{Learning the data distribution}{figure.caption.112}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 758}}{64}{Learning the data distribution}{figure.caption.112}{}}
\newlabel{sub@fig:sub1}{{}{64}{Learning the data distribution}{figure.caption.112}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 763}}{64}{Learning the data distribution}{figure.caption.112}{}}
\newlabel{sub@fig:sub1}{{}{64}{Learning the data distribution}{figure.caption.112}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 768}}{64}{Learning the data distribution}{figure.caption.112}{}}
\newlabel{sub@fig:sub1}{{}{64}{Learning the data distribution}{figure.caption.112}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.12}{\ignorespaces Visualization of the spikes in the visible layer during the positive phase and negative phase of a diagonal stripe. In the first column, at the beginning of the training, in the negative phase the network is not able to reconstruct the data distribution. As the training progresses (in the second and third column), the reconstruction approximates the original data sample more closely and is in the last column nearly perfect.\relax }}{64}{figure.caption.112}}
\newlabel{fig:posnegstrec}{{6.12}{64}{Visualization of the spikes in the visible layer during the positive phase and negative phase of a diagonal stripe. In the first column, at the beginning of the training, in the negative phase the network is not able to reconstruct the data distribution. As the training progresses (in the second and third column), the reconstruction approximates the original data sample more closely and is in the last column nearly perfect.\relax }{figure.caption.112}{}}
\newlabel{fig:sub1}{{6.13a}{64}{Spikes at the beginning.\relax }{figure.caption.113}{}}
\newlabel{sub@fig:sub1}{{a}{64}{Spikes at the beginning.\relax }{figure.caption.113}{}}
\newlabel{fig:sub2}{{6.13b}{64}{Spikes after some training.\relax }{figure.caption.113}{}}
\newlabel{sub@fig:sub2}{{b}{64}{Spikes after some training.\relax }{figure.caption.113}{}}
\newlabel{fig:sub2}{{6.13c}{64}{Total number of spikes per sample over time.\relax }{figure.caption.113}{}}
\newlabel{sub@fig:sub2}{{c}{64}{Total number of spikes per sample over time.\relax }{figure.caption.113}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.13}{\ignorespaces Spikes in the hidden layer of a spiking convolutional RBM. At the start of the training (a) is appears mostly random, while after training the activations become more sparse ((b) and (c)) and certain neurons become specialized on certain input patterns (b).\relax }}{64}{figure.caption.113}}
\newlabel{fig:actdevstr}{{6.13}{64}{Spikes in the hidden layer of a spiking convolutional RBM. At the start of the training (a) is appears mostly random, while after training the activations become more sparse ((b) and (c)) and certain neurons become specialized on certain input patterns (b).\relax }{figure.caption.113}{}}
\newlabel{fig:sub1}{{6.14a}{65}{\relax }{figure.caption.115}{}}
\newlabel{sub@fig:sub1}{{a}{65}{\relax }{figure.caption.115}{}}
\newlabel{fig:sub2}{{6.14b}{65}{\relax }{figure.caption.115}{}}
\newlabel{sub@fig:sub2}{{b}{65}{\relax }{figure.caption.115}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.14}{\ignorespaces $5 \times 5$ convolution filter matrices with and without lateral inhibitory connections in the top RBM layer on the stripe dataset. In (a) the weight are trained without lateral inhibitory connections and in (b) they are trained with lateral inhibitory connections. Whereas the filters without lateral connections (a) look more similar the filters with lateral connections (b) are more different and discriminative. \relax }}{65}{figure.caption.115}}
\newlabel{fig:latconstr}{{6.14}{65}{$5 \times 5$ convolution filter matrices with and without lateral inhibitory connections in the top RBM layer on the stripe dataset. In (a) the weight are trained without lateral inhibitory connections and in (b) they are trained with lateral inhibitory connections. Whereas the filters without lateral connections (a) look more similar the filters with lateral connections (b) are more different and discriminative. \relax }{figure.caption.115}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Convolution vs no Convolution}{65}{section*.116}}
\newlabel{c:convvsnoconvexp}{{6.2.3}{65}{Convolution vs no Convolution}{section*.116}{}}
\newlabel{fig:sub1}{{6.15a}{66}{Weights of the DBN with convolutions.\relax }{figure.caption.117}{}}
\newlabel{sub@fig:sub1}{{a}{66}{Weights of the DBN with convolutions.\relax }{figure.caption.117}{}}
\newlabel{fig:sub2}{{6.15b}{66}{Weights of the DBN without convolutions.\relax }{figure.caption.117}{}}
\newlabel{sub@fig:sub2}{{b}{66}{Weights of the DBN without convolutions.\relax }{figure.caption.117}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.15}{\ignorespaces Weight of the first layers of the DBNs with and without convolutions with the same number of free parameters. In (a) are weights of the DBN with convolutions visualized and in (b) the weights of the DBN without convolutions.\relax }}{66}{figure.caption.117}}
\newlabel{fig:wwoconwconv}{{6.15}{66}{Weight of the first layers of the DBNs with and without convolutions with the same number of free parameters. In (a) are weights of the DBN with convolutions visualized and in (b) the weights of the DBN without convolutions.\relax }{figure.caption.117}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Simulation time}{66}{section*.118}}
\newlabel{c:simtimeexp}{{6.2.3}{66}{Simulation time}{section*.118}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.16}{\ignorespaces The runtime of a learning step in dependence on the size of the network. While the simulated time of $168$ms stays constant, the runtime increases linearly with the number of hidden units. If the blue line matched the red line, the simulation could be performed in realtime.\relax }}{66}{figure.caption.119}}
\newlabel{fig:simtime}{{6.16}{66}{The runtime of a learning step in dependence on the size of the network. While the simulated time of $168$ms stays constant, the runtime increases linearly with the number of hidden units. If the blue line matched the red line, the simulation could be performed in realtime.\relax }{figure.caption.119}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Performance on the stripe dataset}{66}{section*.120}}
\newlabel{c:stripeexp}{{6.2.3}{66}{Performance on the stripe dataset}{section*.120}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.17}{\ignorespaces Abstract architecture of the DBN for the stripe dataset.\relax }}{67}{figure.caption.121}}
\newlabel{fig:dbnstrarch}{{6.17}{67}{Abstract architecture of the DBN for the stripe dataset.\relax }{figure.caption.121}{}}
\newlabel{fig:sub1}{{6.18a}{68}{Weights a the beginning of training.\relax }{figure.caption.122}{}}
\newlabel{sub@fig:sub1}{{a}{68}{Weights a the beginning of training.\relax }{figure.caption.122}{}}
\newlabel{fig:sub2}{{6.18b}{68}{Weights after 250 samples.\relax }{figure.caption.122}{}}
\newlabel{sub@fig:sub2}{{b}{68}{Weights after 250 samples.\relax }{figure.caption.122}{}}
\newlabel{fig:sub2}{{6.18c}{68}{Weights after 500 samples.\relax }{figure.caption.122}{}}
\newlabel{sub@fig:sub2}{{c}{68}{Weights after 500 samples.\relax }{figure.caption.122}{}}
\newlabel{fig:sub2}{{6.18d}{68}{Weights after 750 samples.\relax }{figure.caption.122}{}}
\newlabel{sub@fig:sub2}{{d}{68}{Weights after 750 samples.\relax }{figure.caption.122}{}}
\newlabel{fig:sub2}{{6.18e}{68}{Weights after 1000 samples.\relax }{figure.caption.122}{}}
\newlabel{sub@fig:sub2}{{e}{68}{Weights after 1000 samples.\relax }{figure.caption.122}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.18}{\ignorespaces The development of the $5 \times 5$ convolution filter matrices in the first layer of a DBN during training with the convolutional eCD algorithm on $1000$ samples of the stripe dataset.\relax }}{68}{figure.caption.122}}
\newlabel{fig:stripesdbnw}{{6.18}{68}{The development of the $5 \times 5$ convolution filter matrices in the first layer of a DBN during training with the convolutional eCD algorithm on $1000$ samples of the stripe dataset.\relax }{figure.caption.122}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Performance on the Poker-DVS dataset}{68}{section*.125}}
\newlabel{c:pokerexp}{{6.2.3}{68}{Performance on the Poker-DVS dataset}{section*.125}{}}
\newlabel{fig:sub1}{{6.19a}{69}{Visible layer activations in the beginning.\relax }{figure.caption.123}{}}
\newlabel{sub@fig:sub1}{{a}{69}{Visible layer activations in the beginning.\relax }{figure.caption.123}{}}
\newlabel{fig:sub2}{{6.19b}{69}{Visible layer activations after 1000 samples.\relax }{figure.caption.123}{}}
\newlabel{sub@fig:sub2}{{b}{69}{Visible layer activations after 1000 samples.\relax }{figure.caption.123}{}}
\newlabel{fig:sub2}{{6.19c}{69}{Hidden layer activations in the beginning.\relax }{figure.caption.123}{}}
\newlabel{sub@fig:sub2}{{c}{69}{Hidden layer activations in the beginning.\relax }{figure.caption.123}{}}
\newlabel{fig:sub2}{{6.19d}{69}{Hidden layer activations after 1000 samples.\relax }{figure.caption.123}{}}
\newlabel{sub@fig:sub2}{{d}{69}{Hidden layer activations after 1000 samples.\relax }{figure.caption.123}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.19}{\ignorespaces Activations/ spikes in the layers of the first RBM during training. As the training progresses, the activations become more sparse. The hidden layer learns new representations for the data. The model distribution approximates the data distribution, resulting in a nearly perfect reconstruction of the input data.\relax }}{69}{figure.caption.123}}
\newlabel{fig:stripesspl1}{{6.19}{69}{Activations/ spikes in the layers of the first RBM during training. As the training progresses, the activations become more sparse. The hidden layer learns new representations for the data. The model distribution approximates the data distribution, resulting in a nearly perfect reconstruction of the input data.\relax }{figure.caption.123}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Performance on the Ball-Can-Pen-DVS dataset}{69}{section*.131}}
\newlabel{c:pokerexp}{{6.2.3}{69}{Performance on the Ball-Can-Pen-DVS dataset}{section*.131}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}Conversion vs online}{69}{subsection.6.2.4}}
\newlabel{c:comparisonexp}{{6.2.4}{69}{Conversion vs online}{subsection.6.2.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\nonumberline Direct comparison}{69}{section*.133}}
\newlabel{fig:sub1}{{6.20a}{70}{Visible layer activations in the beginning.\relax }{figure.caption.124}{}}
\newlabel{sub@fig:sub1}{{a}{70}{Visible layer activations in the beginning.\relax }{figure.caption.124}{}}
\newlabel{fig:sub2}{{6.20b}{70}{Visible layer activations after 500 samples.\relax }{figure.caption.124}{}}
\newlabel{sub@fig:sub2}{{b}{70}{Visible layer activations after 500 samples.\relax }{figure.caption.124}{}}
\newlabel{fig:sub2}{{6.20c}{70}{Label layer activations in the beginning.\relax }{figure.caption.124}{}}
\newlabel{sub@fig:sub2}{{c}{70}{Label layer activations in the beginning.\relax }{figure.caption.124}{}}
\newlabel{fig:sub2}{{6.20d}{70}{Label layer activations after 500 samples.\relax }{figure.caption.124}{}}
\newlabel{sub@fig:sub2}{{d}{70}{Label layer activations after 500 samples.\relax }{figure.caption.124}{}}
\newlabel{fig:sub2}{{6.20e}{70}{Hidden layer activations in the beginning.\relax }{figure.caption.124}{}}
\newlabel{sub@fig:sub2}{{e}{70}{Hidden layer activations in the beginning.\relax }{figure.caption.124}{}}
\newlabel{fig:sub2}{{6.20f}{70}{Hidden layer activations after 500 samples.\relax }{figure.caption.124}{}}
\newlabel{sub@fig:sub2}{{f}{70}{Hidden layer activations after 500 samples.\relax }{figure.caption.124}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.20}{\ignorespaces Activations/ spikes in the layers of the second RBM during training. As the training progresses, the activations become more sparse. The hidden layer learns new 20 dimensional representations for the data. The model distribution approximates the data distribution, resulting in a nearly perfect reconstruction of the input data, especially of the label.\relax }}{70}{figure.caption.124}}
\newlabel{fig:stripesspl2}{{6.20}{70}{Activations/ spikes in the layers of the second RBM during training. As the training progresses, the activations become more sparse. The hidden layer learns new 20 dimensional representations for the data. The model distribution approximates the data distribution, resulting in a nearly perfect reconstruction of the input data, especially of the label.\relax }{figure.caption.124}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.21}{\ignorespaces Abstract architecture of the DBN for the poker dataset.\relax }}{71}{figure.caption.126}}
\newlabel{fig:pokerdbnarch}{{6.21}{71}{Abstract architecture of the DBN for the poker dataset.\relax }{figure.caption.126}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.22}{\ignorespaces Visualization of the convolution filters of the first layer RBM trained with the convolutional eCD algorithm on the Poker-DVS dataset.\relax }}{71}{figure.caption.127}}
\newlabel{fig:pokerw}{{6.22}{71}{Visualization of the convolution filters of the first layer RBM trained with the convolutional eCD algorithm on the Poker-DVS dataset.\relax }{figure.caption.127}{}}
\newlabel{fig:sub1}{{6.23a}{72}{Accuracy of the DBN during training with 200 samples.\relax }{figure.caption.128}{}}
\newlabel{sub@fig:sub1}{{a}{72}{Accuracy of the DBN during training with 200 samples.\relax }{figure.caption.128}{}}
\newlabel{fig:sub2}{{6.23b}{72}{Reconstruction error of the first layer of the DBN during training with 200 samples.\relax }{figure.caption.128}{}}
\newlabel{sub@fig:sub2}{{b}{72}{Reconstruction error of the first layer of the DBN during training with 200 samples.\relax }{figure.caption.128}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.23}{\ignorespaces The accuracy and reconstruction error of the DBN. The accuracy increases to a maximal value of $94 \%$ and the reconstruction error decreases, indicating discriminative features. \relax }}{72}{figure.caption.128}}
\newlabel{fig:poker_err}{{6.23}{72}{The accuracy and reconstruction error of the DBN. The accuracy increases to a maximal value of $94 \%$ and the reconstruction error decreases, indicating discriminative features. \relax }{figure.caption.128}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 1064}}{72}{Performance on the Poker-DVS dataset}{figure.caption.129}{}}
\newlabel{sub@fig:sub1}{{}{72}{Performance on the Poker-DVS dataset}{figure.caption.129}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 1069}}{72}{Performance on the Poker-DVS dataset}{figure.caption.129}{}}
\newlabel{sub@fig:sub1}{{}{72}{Performance on the Poker-DVS dataset}{figure.caption.129}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 1074}}{72}{Performance on the Poker-DVS dataset}{figure.caption.129}{}}
\newlabel{sub@fig:sub1}{{}{72}{Performance on the Poker-DVS dataset}{figure.caption.129}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 1079}}{72}{Performance on the Poker-DVS dataset}{figure.caption.129}{}}
\newlabel{sub@fig:sub1}{{}{72}{Performance on the Poker-DVS dataset}{figure.caption.129}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 1086}}{72}{Performance on the Poker-DVS dataset}{figure.caption.129}{}}
\newlabel{sub@fig:sub1}{{}{72}{Performance on the Poker-DVS dataset}{figure.caption.129}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 1091}}{72}{Performance on the Poker-DVS dataset}{figure.caption.129}{}}
\newlabel{sub@fig:sub1}{{}{72}{Performance on the Poker-DVS dataset}{figure.caption.129}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 1096}}{72}{Performance on the Poker-DVS dataset}{figure.caption.129}{}}
\newlabel{sub@fig:sub1}{{}{72}{Performance on the Poker-DVS dataset}{figure.caption.129}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 1101}}{72}{Performance on the Poker-DVS dataset}{figure.caption.129}{}}
\newlabel{sub@fig:sub1}{{}{72}{Performance on the Poker-DVS dataset}{figure.caption.129}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.24}{\ignorespaces Positive phase and negative phase in the first layer of the DBN of after training on the Poker-DVS dataset. The reconstruction of each class is nearly perfect indicating, the DBN has learned the basic structure of the data.\relax }}{72}{figure.caption.129}}
\newlabel{fig:pokerrecon}{{6.24}{72}{Positive phase and negative phase in the first layer of the DBN of after training on the Poker-DVS dataset. The reconstruction of each class is nearly perfect indicating, the DBN has learned the basic structure of the data.\relax }{figure.caption.129}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 1114}}{73}{Performance on the Poker-DVS dataset}{figure.caption.130}{}}
\newlabel{sub@fig:sub1}{{}{73}{Performance on the Poker-DVS dataset}{figure.caption.130}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 1119}}{73}{Performance on the Poker-DVS dataset}{figure.caption.130}{}}
\newlabel{sub@fig:sub1}{{}{73}{Performance on the Poker-DVS dataset}{figure.caption.130}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 1124}}{73}{Performance on the Poker-DVS dataset}{figure.caption.130}{}}
\newlabel{sub@fig:sub1}{{}{73}{Performance on the Poker-DVS dataset}{figure.caption.130}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 1129}}{73}{Performance on the Poker-DVS dataset}{figure.caption.130}{}}
\newlabel{sub@fig:sub1}{{}{73}{Performance on the Poker-DVS dataset}{figure.caption.130}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 1136}}{73}{Performance on the Poker-DVS dataset}{figure.caption.130}{}}
\newlabel{sub@fig:sub1}{{}{73}{Performance on the Poker-DVS dataset}{figure.caption.130}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 1141}}{73}{Performance on the Poker-DVS dataset}{figure.caption.130}{}}
\newlabel{sub@fig:sub1}{{}{73}{Performance on the Poker-DVS dataset}{figure.caption.130}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 1146}}{73}{Performance on the Poker-DVS dataset}{figure.caption.130}{}}
\newlabel{sub@fig:sub1}{{}{73}{Performance on the Poker-DVS dataset}{figure.caption.130}{}}
\newlabel{fig:sub1}{{\caption@xref {fig:sub1}{ on input line 1151}}{73}{Performance on the Poker-DVS dataset}{figure.caption.130}{}}
\newlabel{sub@fig:sub1}{{}{73}{Performance on the Poker-DVS dataset}{figure.caption.130}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.25}{\ignorespaces Completion of partially fed input data. As input data, the bottom half of the image data was omitted. After training, the DBN was able to reconstruct some of the missing input data and complete the input data.\relax }}{73}{figure.caption.130}}
\newlabel{fig:pokercompl}{{6.25}{73}{Completion of partially fed input data. As input data, the bottom half of the image data was omitted. After training, the DBN was able to reconstruct some of the missing input data and complete the input data.\relax }{figure.caption.130}{}}
\newlabel{fig:bcpdbnarch}{{6.26a}{73}{\relax }{figure.caption.132}{}}
\newlabel{sub@fig:bcpdbnarch}{{a}{73}{\relax }{figure.caption.132}{}}
\newlabel{fig:bcpw}{{6.26b}{73}{\relax }{figure.caption.132}{}}
\newlabel{sub@fig:bcpw}{{b}{73}{\relax }{figure.caption.132}{}}
\newlabel{fig:bcpacc}{{6.26c}{73}{\relax }{figure.caption.132}{}}
\newlabel{sub@fig:bcpacc}{{c}{73}{\relax }{figure.caption.132}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.26}{\ignorespaces Performance on the Ball-Can-Pen-DVS dataset. (a) Abstract architecture of the DBN for the Ball-Can-Pen-DVS dataset. (b) Visualization of the convolution filters of the first layer RBM trained with the convolutional eCD algorithm on the Ball-Can-Pen-DVS dataset. (c) Accuracy of the DBN during training with over the whole Ball-Can-Pen-DVS dataset.\relax }}{73}{figure.caption.132}}
\newlabel{fig:sub1}{{6.27a}{73}{Weights of the artificial DBN.\relax }{figure.caption.134}{}}
\newlabel{sub@fig:sub1}{{a}{73}{Weights of the artificial DBN.\relax }{figure.caption.134}{}}
\newlabel{fig:sub2}{{6.27b}{73}{Weights of the spinking DBN.\relax }{figure.caption.134}{}}
\newlabel{sub@fig:sub2}{{b}{73}{Weights of the spinking DBN.\relax }{figure.caption.134}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.27}{\ignorespaces Convolutional filters of the first layers of the artificial and spiking convolutional DBNs after 100 training samples. The weights of the spiking DBN appear to be more discriminative.\relax }}{73}{figure.caption.134}}
\newlabel{fig:convvsnoconv}{{6.27}{73}{Convolutional filters of the first layers of the artificial and spiking convolutional DBNs after 100 training samples. The weights of the spiking DBN appear to be more discriminative.\relax }{figure.caption.134}{}}
\@setckpt{content/exp}{
\setcounter{page}{74}
\setcounter{equation}{0}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{6}
\setcounter{section}{2}
\setcounter{subsection}{4}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{27}
\setcounter{table}{4}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{2}
\setcounter{subtable}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{4}
\setcounter{ALG@line}{4}
\setcounter{ALG@rem}{4}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{Item}{27}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{43}
\setcounter{section@level}{3}
}
