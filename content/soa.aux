\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{desjardins2008empirical}
\citation{lee2009convolutional}
\citation{norouzi2009stacks}
\citation{convImg}
\citation{convImg}
\citation{NorouziM2009}
\citation{lee2009convolutional}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related Work}{35}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{c:relwork}{{3}{35}{Related Work}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Convolutional RBM}{35}{section.3.1}}
\newlabel{c:convrbm}{{3.1}{35}{Convolutional RBM}{section.3.1}{}}
\citation{lee2009convolutional}
\citation{lee2009convolutional}
\citation{lee2003hierarchical}
\citation{yang2007probabilistic}
\citation{griffiths2008bayesian}
\citation{Buesing2011}
\citation{Buesing2011}
\newlabel{fig:convrbmsub1}{{3.1a}{36}{The upward pass in the convolutional RBM\relax }{figure.caption.73}{}}
\newlabel{sub@fig:convrbmsub1}{{a}{36}{The upward pass in the convolutional RBM\relax }{figure.caption.73}{}}
\newlabel{fig:convrbmsub2}{{3.1b}{36}{The 180\textdegree flipped of the kernel matrix\relax }{figure.caption.73}{}}
\newlabel{sub@fig:convrbmsub2}{{b}{36}{The 180\textdegree flipped of the kernel matrix\relax }{figure.caption.73}{}}
\newlabel{fig:convrbmsub3}{{3.1c}{36}{The downward pass in the convolutional RBM\relax }{figure.caption.73}{}}
\newlabel{sub@fig:convrbmsub3}{{c}{36}{The downward pass in the convolutional RBM\relax }{figure.caption.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A Gibbs sampling step in a convolutional RBM. At first the visible data is convolved with the kernel to get the hidden activations. Afterwards the kernel matrix is flipped 180\textdegree . In the downward pass the hidden activations with padding are convolved with the flipped kernel to get the new visible activations \cite  {convImg}.\relax }}{36}{figure.caption.73}}
\newlabel{fig:convrbm}{{3.1}{36}{A Gibbs sampling step in a convolutional RBM. At first the visible data is convolved with the kernel to get the hidden activations. Afterwards the kernel matrix is flipped 180\textdegree . In the downward pass the hidden activations with padding are convolved with the flipped kernel to get the new visible activations \cite {convImg}.\relax }{figure.caption.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The RBM layer architecture with probabilistic max pooling \cite  {lee2009convolutional}.\relax }}{36}{figure.caption.74}}
\newlabel{fig:probmaxpool}{{3.2}{36}{The RBM layer architecture with probabilistic max pooling \cite {lee2009convolutional}.\relax }{figure.caption.74}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Sampling in SNNs}{36}{section.3.2}}
\newlabel{c:snnsampling}{{3.2}{36}{Sampling in SNNs}{section.3.2}{}}
\citation{Petrovici2016}
\citation{Petrovici2016}
\citation{Buesing2011}
\citation{Buesing2011}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A spiking neural network as probabilistic model. The state $z_i$ of a neuron is set to $1$ for a time period $t_{on}$ after a spike. The complete state of the network $\textbf  {z}$ is given by the individual state $(z_1, z_2, z_3, z_4)$ \cite  {Petrovici2016}.\relax }}{37}{figure.caption.75}}
\newlabel{fig:snnsamp1}{{3.3}{37}{A spiking neural network as probabilistic model. The state $z_i$ of a neuron is set to $1$ for a time period $t_{on}$ after a spike. The complete state of the network $\textbf {z}$ is given by the individual state $(z_1, z_2, z_3, z_4)$ \cite {Petrovici2016}.\relax }{figure.caption.75}{}}
\citation{Buesing2011}
\citation{Buesing2011}
\citation{Petrovici2016}
\citation{Petrovici2016}
\citation{Petrovici2016}
\citation{Petrovici2016}
\citation{OConnor2013}
\citation{Cao2014}
\citation{Diehl2015}
\newlabel{fig:snnsamp2sub1}{{3.4a}{38}{\relax }{figure.caption.76}{}}
\newlabel{sub@fig:snnsamp2sub1}{{a}{38}{\relax }{figure.caption.76}{}}
\newlabel{fig:snnsamp2sub1}{{3.4b}{38}{\relax }{figure.caption.76}{}}
\newlabel{sub@fig:snnsamp2sub1}{{b}{38}{\relax }{figure.caption.76}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The artificial counter state $\xi $ of a neuron in a discrete time. In (a) the red state represent an active state $z_i = 1$. After a spike $\xi $ is set to the refractory period $\tau $. In each time step the refractory period counter $\xi $ is reduced by 1 until the neuron is inactive and can spike again. In (b) a exemplary membrane potential with the corresponding states of a neuron is given \cite  {Buesing2011}.\relax }}{38}{figure.caption.76}}
\newlabel{fig:snnsamp2}{{3.4}{38}{The artificial counter state $\xi $ of a neuron in a discrete time. In (a) the red state represent an active state $z_i = 1$. After a spike $\xi $ is set to the refractory period $\tau $. In each time step the refractory period counter $\xi $ is reduced by 1 until the neuron is inactive and can spike again. In (b) a exemplary membrane potential with the corresponding states of a neuron is given \cite {Buesing2011}.\relax }{figure.caption.76}{}}
\newlabel{fig:sub1}{{3.5a}{38}{\relax }{figure.caption.77}{}}
\newlabel{sub@fig:sub1}{{a}{38}{\relax }{figure.caption.77}{}}
\newlabel{fig:sub2}{{3.5b}{38}{\relax }{figure.caption.77}{}}
\newlabel{sub@fig:sub2}{{b}{38}{\relax }{figure.caption.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Comparison of the state probabilities with of neural sampling with Gibbs sampling in a five state Boltzmann machine. In (a) a probabilistic neuron model with a rectangular PSP is used and no discrepancies can be detected given a long enough sampling period. In (b) a more biological plausible alpha shaped kernel is used, which leads to some differences \cite  {Buesing2011}.\relax }}{38}{figure.caption.77}}
\newlabel{fig:snnsamp3}{{3.5}{38}{Comparison of the state probabilities with of neural sampling with Gibbs sampling in a five state Boltzmann machine. In (a) a probabilistic neuron model with a rectangular PSP is used and no discrepancies can be detected given a long enough sampling period. In (b) a more biological plausible alpha shaped kernel is used, which leads to some differences \cite {Buesing2011}.\relax }{figure.caption.77}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Artificial to spiking neural network conversion}{38}{section.3.3}}
\newlabel{c:nnconversion}{{3.3}{38}{Artificial to spiking neural network conversion}{section.3.3}{}}
\citation{Diehl2015}
\citation{Cao2014}
\citation{Cao2014}
\citation{Teh2005}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Input output transfer function of a neuron in a high conductance state. The output rate is normalized by $\frac  {1}{\tau _{ref}}$ to get a output probability. The transfer function is identical to shifted sigmoid function $\sigma $ \cite  {Petrovici2016}.\relax }}{39}{figure.caption.78}}
\newlabel{fig:snnsamp4}{{3.6}{39}{Input output transfer function of a neuron in a high conductance state. The output rate is normalized by $\frac {1}{\tau _{ref}}$ to get a output probability. The transfer function is identical to shifted sigmoid function $\sigma $ \cite {Petrovici2016}.\relax }{figure.caption.78}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}eCD and Sampling Machines}{39}{section.3.4}}
\newlabel{c:ecd}{{3.4}{39}{eCD and Sampling Machines}{section.3.4}{}}
\citation{Diehl2015}
\citation{Neil2013}
\citation{Neil2013}
\citation{Neftci2013}
\citation{Neftci2013}
\citation{Neftci2013}
\newlabel{fig:sub1}{{3.7a}{40}{A for conversion adapted CNN architecture.\relax }{figure.caption.79}{}}
\newlabel{sub@fig:sub1}{{a}{40}{A for conversion adapted CNN architecture.\relax }{figure.caption.79}{}}
\newlabel{fig:sub2}{{3.7b}{40}{A to an SNN converted convolutional network.\relax }{figure.caption.79}{}}
\newlabel{sub@fig:sub2}{{b}{40}{A to an SNN converted convolutional network.\relax }{figure.caption.79}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The proposed architectures to conversion between CNNs and SNNs \cite  {Cao2014}. \relax }}{40}{figure.caption.79}}
\newlabel{fig:csnnconv}{{3.7}{40}{The proposed architectures to conversion between CNNs and SNNs \cite {Cao2014}. \relax }{figure.caption.79}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces An unrolled RBM with tied weights, which can be used to simulate the positive and negative phase and learn the weighs online \cite  {Neil2013}.\relax }}{40}{figure.caption.80}}
\newlabel{fig:evtCD}{{3.8}{40}{An unrolled RBM with tied weights, which can be used to simulate the positive and negative phase and learn the weighs online \cite {Neil2013}.\relax }{figure.caption.80}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Comparison between classical CD and event-based CD. In the classical k-CD (a) the weight update is determined by positive phase and negative phase, while in event-base CD (b) the weight update is determined by STDP and the sampling phase, determined by $g$ \cite  {Neftci2013}.\relax }}{41}{figure.caption.81}}
\newlabel{fig:ecdcomp}{{3.9}{41}{Comparison between classical CD and event-based CD. In the classical k-CD (a) the weight update is determined by positive phase and negative phase, while in event-base CD (b) the weight update is determined by STDP and the sampling phase, determined by $g$ \cite {Neftci2013}.\relax }{figure.caption.81}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Visualization of the phases of the event based contrastive divergence.\relax }}{41}{figure.caption.82}}
\newlabel{fig:ecdph}{{3.10}{41}{Visualization of the phases of the event based contrastive divergence.\relax }{figure.caption.82}{}}
\@setckpt{content/soa}{
\setcounter{page}{42}
\setcounter{equation}{0}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{10}
\setcounter{table}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{Item}{22}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{20}
\setcounter{section@level}{1}
}
