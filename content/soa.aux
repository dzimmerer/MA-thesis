\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Related Work}{33}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Convolutional RBM}{33}{section.3.1}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Sampling in SNNs}{33}{section.3.2}}
\newlabel{fig:sub1}{{3.1a}{34}{The upward pass in the convolutional RBM\relax }{figure.caption.73}{}}
\newlabel{sub@fig:sub1}{{a}{34}{The upward pass in the convolutional RBM\relax }{figure.caption.73}{}}
\newlabel{fig:sub2}{{3.1b}{34}{The 180\textdegree flipped of the kernel matrix\relax }{figure.caption.73}{}}
\newlabel{sub@fig:sub2}{{b}{34}{The 180\textdegree flipped of the kernel matrix\relax }{figure.caption.73}{}}
\newlabel{fig:sub3}{{3.1c}{34}{The downward pass in the convolutional RBM\relax }{figure.caption.73}{}}
\newlabel{sub@fig:sub3}{{c}{34}{The downward pass in the convolutional RBM\relax }{figure.caption.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A Gibbs sampling step in a convolutional RBM. At first the visible data is convolved with the kernel to get the hidden activations. Afterwards the kernel matrix is flipped 180\textdegree . In the downward pass the hidden activations with padding are convolved with the flipped kernel to get the new visible activations.\relax }}{34}{figure.caption.73}}
\newlabel{fig:convrbm}{{3.1}{34}{A Gibbs sampling step in a convolutional RBM. At first the visible data is convolved with the kernel to get the hidden activations. Afterwards the kernel matrix is flipped 180\textdegree . In the downward pass the hidden activations with padding are convolved with the flipped kernel to get the new visible activations.\relax }{figure.caption.73}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces The RBM layer architecture with probabilistic max pooling.\relax }}{34}{figure.caption.74}}
\newlabel{fig:probmaxpool}{{3.2}{34}{The RBM layer architecture with probabilistic max pooling.\relax }{figure.caption.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces A spiking neural network as probabilistic model. The state $z_i$ of a neuron is set to $1$ for a time period $t_{on}$ after a spike. The complete state of the network $\textbf  {z}$ is given by the individual state $(z_1, z_2, z_3, z_4)$ .\relax }}{35}{figure.caption.75}}
\newlabel{fig:snnsamp1}{{3.3}{35}{A spiking neural network as probabilistic model. The state $z_i$ of a neuron is set to $1$ for a time period $t_{on}$ after a spike. The complete state of the network $\textbf {z}$ is given by the individual state $(z_1, z_2, z_3, z_4)$ .\relax }{figure.caption.75}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces The artificial counter state $\xi $ of a neuron in a discrete time. The red state represent an active state $z_i = 1$. After a spike $\xi $ is set to the refactory period $\tau $. In each time step the refractory period counter $\xi $ is reduced by 1 until the neuron is inactive and can spike again.\relax }}{36}{figure.caption.76}}
\newlabel{fig:snnsamp2}{{3.4}{36}{The artificial counter state $\xi $ of a neuron in a discrete time. The red state represent an active state $z_i = 1$. After a spike $\xi $ is set to the refactory period $\tau $. In each time step the refractory period counter $\xi $ is reduced by 1 until the neuron is inactive and can spike again.\relax }{figure.caption.76}{}}
\newlabel{fig:sub1}{{3.5a}{36}{Sampling in a Boltzmann machine with a rectangular PSP kernel.\relax }{figure.caption.77}{}}
\newlabel{sub@fig:sub1}{{a}{36}{Sampling in a Boltzmann machine with a rectangular PSP kernel.\relax }{figure.caption.77}{}}
\newlabel{fig:sub2}{{3.5b}{36}{Sampling in a Boltzmann machine with a alpha-shaped PSP kernel. \relax }{figure.caption.77}{}}
\newlabel{sub@fig:sub2}{{b}{36}{Sampling in a Boltzmann machine with a alpha-shaped PSP kernel. \relax }{figure.caption.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Comparison of the state probabilities with of neural sampling with Gibbs sampling in a four state Boltzmann machine. In (a) a probabilistic neuron model with a rectangular PSP is used and no discrepancies can be detected given a long enough sampling period. In (b) a more biological plausible alpha shaped kernel is used, which leads to some differences.\relax }}{36}{figure.caption.77}}
\newlabel{fig:snnsamp3}{{3.5}{36}{Comparison of the state probabilities with of neural sampling with Gibbs sampling in a four state Boltzmann machine. In (a) a probabilistic neuron model with a rectangular PSP is used and no discrepancies can be detected given a long enough sampling period. In (b) a more biological plausible alpha shaped kernel is used, which leads to some differences.\relax }{figure.caption.77}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Input output transfer function of a neuron in a high conductance state. The output rate is normalized by $\frac  {1}{\tau _{ref}}$ to get a output probability. The transfer function is identical to shifted sigmoid function $\sigma $.\relax }}{37}{figure.caption.78}}
\newlabel{fig:snnsmap4}{{3.6}{37}{Input output transfer function of a neuron in a high conductance state. The output rate is normalized by $\frac {1}{\tau _{ref}}$ to get a output probability. The transfer function is identical to shifted sigmoid function $\sigma $.\relax }{figure.caption.78}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Artificial to spiking neural network conversion}{37}{section.3.3}}
\newlabel{fig:sub1}{{3.7a}{38}{A for conversion adapted CNN architecture.\relax }{figure.caption.79}{}}
\newlabel{sub@fig:sub1}{{a}{38}{A for conversion adapted CNN architecture.\relax }{figure.caption.79}{}}
\newlabel{fig:sub2}{{3.7b}{38}{A to an SNN converted convolutional network.\relax }{figure.caption.79}{}}
\newlabel{sub@fig:sub2}{{b}{38}{A to an SNN converted convolutional network.\relax }{figure.caption.79}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces The proposed architectures to conversion between CNNs and SNNs. \relax }}{38}{figure.caption.79}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}eCD and Sampling Machines}{38}{section.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces An unrolled RBM with tied weights, which can be used to simulate the positive and negative phase and learn the weighs online.\relax }}{39}{figure.caption.80}}
\newlabel{fig:evtCD}{{3.8}{39}{An unrolled RBM with tied weights, which can be used to simulate the positive and negative phase and learn the weighs online.\relax }{figure.caption.80}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Comparison between classical CD and event-based CD. In the classical k-CD (a) the weight update is determined by positive phase and negative phase, while in event-base CD (b) the weight update is determined by STDP and the sampling phase, determined by $g$.\relax }}{39}{figure.caption.81}}
\newlabel{fig:test}{{3.9}{39}{Comparison between classical CD and event-based CD. In the classical k-CD (a) the weight update is determined by positive phase and negative phase, while in event-base CD (b) the weight update is determined by STDP and the sampling phase, determined by $g$.\relax }{figure.caption.81}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Visualization of the phases of the event based contrastive divergence.\relax }}{40}{figure.caption.82}}
\newlabel{fig:test}{{3.10}{40}{Visualization of the phases of the event based contrastive divergence.\relax }{figure.caption.82}{}}
\@setckpt{content/soa}{
\setcounter{page}{41}
\setcounter{equation}{0}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{4}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{10}
\setcounter{table}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{Item}{19}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{20}
\setcounter{section@level}{1}
}
