\chapter{Approach} \label{c:approach}

In this section after describing the general convolutional architecture in spiking neural networks in order to find the best performing spiking convolutional DBN, we discuss two different approaches to build these.

The first one is the more classical one, in which an in discrete time trained DBN is transferred to a spiking neural network.
The second approach trains the RBMs event based with STDP directly as spiking neural networks. 

In Chapter \ref{c:expres}, we then evaluate and compare the performance of both approaches.

\section{Convolutional Architecture in Spiking Neural Networks} \label{c:spikingconvarch}

We implement a convolutional layer with receptive fields and shared weights between two neuron populations. 
Each population has a similar three dimensional topology as in artificial CNNs with the number of neurons given by $n_{ \text{neurons}} = \text{channels} \times \text{height} \times \text{width}$.

Groups of neurons in the bottom layer in close vicinity, a so-called receptive fields, are each synaptically connected to the same top layer neuron (see Fig. \ref{fig:receptfields1}).
Different receptive fields have the same shape and size $n$ and share the same synaptic weights (given the top layer neurons belong to the same feature map).
Adjacent receptive fields can overlap by several neurons and the output neurons have the same topology as their input regions. 

A receptive field covers a partial region of the input data and the synaptic weights from the bottom layer neurons to the top layer neuron can the be seen as a convolution over the partial input data.
Since the weights of the receptive field can be shared, the neural activity in the top layer can be seen as a convolution over the input data with a filter of the size of a receptive field.
In this case the top layer activations of receptive fields with shared weights can also be called a feature map  (see Fig. \ref{fig:receptfields2}).
By using more neurons with different sets of shared weights over the same receptive fields, more feature maps can be implemented.     

\begin{figure}[h!]
	\centering
	\begin{subfigure}[t]{.4\textwidth}
  		\centering
  		\includegraphics[width=.5\linewidth]{imgs/recpt_field1.png}
  		\caption{}
  		\label{fig:receptfields1}
	\end{subfigure}%
	\begin{subfigure}[t]{.4\textwidth}
  		\centering
  		\includegraphics[width=.5\linewidth]{imgs/recpt_field2.png}
  		\caption{}
  		\label{fig:receptfields2}
	\end{subfigure}
	\caption[Receptive fields over 4 neurons.]{Receptive fields over 4 neurons. In (a) the top layer neuron (red) is only connected to a some neurons in close vicinity to each other. In (b) four receptive fields are next to each other with a stride of two and shared weights. }
	\label{fig:receptfields}
\end{figure}




\section{Conversion} \label{c:convappro}

The conversion approach can be roughly divided into two steps:
\begin{enumerate}
\item Train artificial RBMs to build up a DBN.
\item Convert the DBN to a spiking neural network.
\end{enumerate}


\subsection{Convolutional DBNs} \label{c:convdbns}

To train a convolutional DBN we proceed similar to Hinton et al. and Lee et al. \cite{hinton2006fast, lee2009convolutional}.

At first the convolutional RBMs are trained greedily with CD, as described in Chapter \ref{c:convrbm}, for a certain number of iterations on data batches of batch-size $b$.
After an RBM is trained, we convert the dataset $X$ into a new dataset $X'$ by sampling of the hidden layer of the RBM (one forward-pass step):
\[
p(x') = \sigma(W * x) .
\]

On this converted dataset the next convolutional RBM is trained greedily.

Evaluating feature qualities is still an active research topic.
To get a measurement of our feature quality, we use labelled data and train a classifier on top of the extracted features.
To stay in a biologically plausible domain, we train a fully connected RBM on the top level features combined with the label of the data samples in order to associate the features with the correct labels.
This architecture is similar to the architecture proposed by Hinton \cite{hinton2006fast} (see Fig. \ref{fig:dbnmnist}).

\begin{figure}
	\centering
    	\includegraphics[width=0.4\textwidth]{imgs/dbn_mnist.png} 
    \caption[A common structure of a deep belief network.]{A common structure of a deep belief network, built up of RBMs, used for classification. The first layers transform the input data/ pixel image and can extract features and the top layer RBM is used for association the data with the correct label \cite{Zorzi2013Modeling}.}
	\label{fig:dbnmnist}
\end{figure}

To evaluate the final performance, we input a data sample without a label into the DBN and after a forward-pass let the top layer sample a label prediction by performing Gibbs sampling steps, which can be seen as a reconstruction of partially presented data (as mentioned in Chapter \ref{c:pgms}).


\subsection{Conversion} \label{c:conversionappr}

We present three different variations of the conversion.

\begin{figure}
	\centering
	\begin{subfigure}[t]{.5\textwidth}
  		\centering
  		\includegraphics[width=.54\linewidth]{imgs/convert_cnn.png}
  		\caption{Structure of a spiking CNN with LIF neurons.}
  		\label{fig:converted1}
	\end{subfigure}%
	\begin{subfigure}[t]{.5\textwidth}
  		\centering
  		\includegraphics[width=.8\linewidth]{imgs/convert_dbn.png}
  		\caption{Structure of a spiking DBN with LIF neurons.}
  		\label{fig:converted2}
  	\end{subfigure}
	\caption[Different converted structures of an CNN and an DBN.]{Different converted structures of a CNN and a DBN. The CNN is converted to a spiking network by simply using simple LIF neurons (white) (a). In the DBN (b) the LIF neurons are put in a high conductance state (black) by inserting high frequency Poisson noise. The weights are scaled accordingly to fit the activations of the neurons.}
	\label{fig:converted}
\end{figure}


\paragraph{Conversion as CNN}  \label{c:convascnn}

One way to convert the DBN to the spiking domain, is by interpreting it as a pre-trained CNN with purely forward connections (we do not perform any commonly used gradient descent fine tuning to get comparable results with only CD trained models, but fine tuning could further improve the performance).
While this allows classification, it removes the generative capabilities of the network.

For the conversion, we proceed similar to Cao et al. and Diehl et al. \cite{Cao2014, Diehl2015}.
They use average pooling and ReLU functions to get a similar architecture as SNNs.
In contrast, we don't use any pooling, since for RBMs there is no simple way to integrate average pooling (used by Cao et al. and Diehl et al. in their trained CNNs), and for spiking CNNs there is no simple way to integrate probabilistic max pooling, which is to our knowledge currently the only into the training integrated pooling thought of for RBMs.
We also use the sigmoid function, since RBMs are commonly trained with sigmoid activations (but some approaches propose ReLU for RBMs as well \cite{Nair2010}).
Furthermore the input-rate output-rate transfer function of rate based LIF neurons with a refractory period matches the sigmoid function more closely.

The DBN layers are each replaced by a LIF neuron population with an identical architecture (see Fig. \ref{fig:converted1}). 
The connections are replaced by (directed) synapses and the weights of the synapses scaled with a constant factor to get similar activations.
 

\paragraph{Conversion with conductance-based LIF neurons} \label{c:convascoba}

Another way to convert the DBN to the spiking domain is by interpreting it as a directed graphical model, a sigmoid belief network, and performing ancestral sampling.
This approach is heavily based on the synaptic sampling theory, i.e. it uses spiking neurons to perform sampling.
The sampling can be either performed with current-based or conductance-base LIF neurons as described in Chapter \ref{c:snnsampling}.

For the COBA neurons, we choose a biological plausible neuron model (see parameters in Table \ref{cobalifparam}). 
The high membrane conductance, an increased Gaussian distributed mean membrane potential and thus a firing probability of $0.5$, is achieved by using high frequency Poisson generated inhibitory and excitatory spikes to get the neuron to a high conductance state (HCS) (see Fig. \ref{fig:cobahcs1}). 
This neuron model has an input-rate output-rate transfer function which approximates a sigmoid function (see Fig. \ref{fig:cobahcs2}).

The PSPs are chosen to have an alpha shape instead of a rectangular shape, which as described in Chapter \ref{c:snnsampling} may introduce some discrepancies when performing sampling compared to Gibbs sampling, but is more biologically plausible (see Fig. \ref{fig:cobahcs3}).

\begin{figure}
	\centering
	\begin{subfigure}[t]{.5\textwidth}
  		\centering
  		\includegraphics[width=.8\linewidth]{imgs/coba_lif_act.png}
  		\caption{Firing of an HCS COBA LIF neuron.}
  		\label{fig:cobahcs1}
	\end{subfigure}%
	\begin{subfigure}[t]{.5\textwidth}
  		\centering
  		\includegraphics[width=.8\linewidth]{imgs/coba_lif_sigmoid.png}
  		\caption{Input output transfer function of an HCS COBA LIF neuron.}
  		\label{fig:cobahcs2}
	\end{subfigure}
	\begin{subfigure}[t]{.5\textwidth}
  		\centering
  		\includegraphics[width=.8\linewidth]{imgs/coba_lif_bm2.png}
  		\caption{omparison between an HCS COBA LIF neuron sampling and the true distribution in an RBM.}
  		\label{fig:cobahcs3}
	\end{subfigure}
	\caption[Properties of a conductance based LIF neuron in a high conductance state.]{Properties of a conductance based LIF neuron put into a high conductance state with high frequency input noise. The neuron fires with a probability of $0.5$ given no additional input (a). With input the input output transfer function approximates a sigmoid function (b). A network with such neurons is able to sample in a Boltzmann distribution similar to original distribution (c). }
	\label{fig:cobahcs}

\end{figure}
  
The DBN is converted by replacing each layer with a layer of conductance-based LIF neurons with Poisson noise, and the connections are transformed to synapses with the weights scaled to achieve an action function similar to the sigmoid function (see Fig. \ref{fig:converted2}).

Consequently, the DBN simply performs ancestral sampling with the data sample as evidences and the label as inferred state.   


\paragraph{Conversion with current-based LIF neurons} \label{c:convascuba}

To reduce the computational expenses of the COBA models, they can be replaced by less computational complex CUBA models.
The model parameters as chose to simulate an HCS state.
Therefore, the membrane time constant is reduced and the membrane conductance is increased and a static input current is inserted.  
Adding high frequency Poisson noise results in a sigmoid shaped input-rate output-rate transfer function (see Fig. \ref{fig:cubahcs}).

\begin{figure}
	\centering
	\begin{subfigure}[t]{.5\textwidth}
  		\centering
  		\includegraphics[width=.8\linewidth]{imgs/cuba_lif_act.png}
  		\caption{Firing of an HCS CUBA LIF neuron.}
  		\label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}[t]{.5\textwidth}
  		\centering
  		\includegraphics[width=.8\linewidth]{imgs/cuba_lif_sigmoid.png}
  		\caption{Input output transfer function of an HCS CUBA LIF neuron.}
  		\label{fig:sub2}
	\end{subfigure}
	\begin{subfigure}[t]{.5\textwidth}
  		\centering
  		\includegraphics[width=.8\linewidth]{imgs/cuba_lif_bm.png}
  		\caption{Comparison between an HCS CUBA LIF neuron sampling and the true distribution in an RBM.}
  		\label{fig:sub2}
	\end{subfigure}
	\caption[Properties of a current based LIF neuron in a high conductance state.]{Properties of a current based LIF neuron put into a simulated high conductance state by high frequency input noise and an artificial high membrane conductance. The neuron also fires with a probability of $0.5$ given no additional input (a) and with input the input output transfer function approximates a sigmoid function (b). A network with such neurons can also to sample in a Boltzmann distribution similar to original distribution (c).}
	\label{fig:cubahcs}
\end{figure}
The DBN conversion is similar to the COBA case, but instead of COBA LIF neurons the adapted CUBA LIF neurons are used (see Fig. \ref{fig:converted2}).

\section{eCD} \label{c:ecdappr}

Another approach implements the training of the convolutional spiking DBN with an STDP based learning rule. 

The main idea of the learning rule is adapted from Neftcis eCD and is applied to a network of LIF neurons with symmetric bidirectional synapses \cite{Neftci2013}.
We modify the STDP learning rule described in Chapter \ref{c:ecd} by extending the model with a learning rate. 
This rule can be reformulated as an iterative rule as follows:
\begin{itemize}
\item A visible unit $v$ spikes: 
\[
\begin{split}
A_v &= A_v \exp(\frac{-\Delta t}{\tau}) + a_{\delta} ,\\
A_h &= A_h \exp(\frac{-\Delta t}{\tau}) ,\\
\delta w &= \mu g(t)  A_v  ,\\
\end{split}
\]
\item A hidden unit $h$ spikes: 
\[
\begin{split}
A_h &= A_h \exp(\frac{-\Delta t}{\tau}) + a_{\delta} ,\\
A_v &= A_v \exp(\frac{-\Delta t}{\tau}) ,\\
\delta w &= \mu  g(t) A_h  ,\\
\end{split}
\]
\end{itemize}
where $g(t)$ is the STDP status flag, $\Delta t$ is the time difference to the last previous spike, and $a_{delta}$ represents the input of the incoming spike.


The original division into four training phases poses similarities to pCD since the activity of the hidden layer of the previous step is used as starting state for the next step.
We extent the model by a 5th phase between two data samples, where the network is "flushed" thus enabling normal CD  (see Fig. \ref{fig:ecd5}):

\begin{enumerate}
\item The data signal is applied and the system is allowed to model the data distribution ($g(t)=0$).
\item Positive STDP is used to get $v_i h_j$-data and is added to the weights (postive phase $g(t)=1$).
\item The data signal is removed and the system is allowed to model the model distribution ($g(t)=0$).
\item Negative STDP is used to get $v_i h_j$-model and is subtracted from the synaptic weights (negative phase $g(t)=-1$).
\item The neural activity is "flushed" by inserting a strong negative current into the visible and hidden layer, no learning is performed ($g(t)=0$).
\end{enumerate}

\begin{figure}
	\centering
   	\includegraphics[width=0.4\textwidth]{imgs/eCD_5phases.png} 
    \caption{The five phases for a data sample in the adapted eCD algorithm. }
	\label{fig:ecd5}
\end{figure}

 
\subsection{Convolution in eCD} \label{c:ecdconv}

We implement convolutions with local receptive fields and shared weights between synapses through weight synchronization.
Since each synaptic weight has its local STDP based update rule (eCD), we have to find a way to synchronize weights between synapses of the same feature map.
To keep the shared weights the same, we perform an STDP weight synchronization step at discrete time steps, since updating all weights after a single update did not show any promising results, due to some self-reinforcement and resulting weight "explosion" (see \ref{fig:ecdnestconv} for further insight).
Thus the weight synchronization at a time step for shared weights $w_i$ , $w_j$ can be illustrated by the following rule:  
\[
\begin{split}
w_i(t) = w_{shared}(t-1) + \delta w_i, \\ 
w_j(t) = w_{shared}(t-1) + \delta w_j, 
\end{split}
\]
which gives the new shared weight
\[
\begin{split}
w_{shared}(t) = \frac{1}{2} (w_i(t) + w_j(t) ) = \frac{1}{2} (w_{shared}(t-1) + \delta w_i + w_{shared}(t-1) + \delta w_j) = \\ w_{shared}(t-1) + \frac{1}{2} (\delta w_i + \delta w_j).
\end{split}
\]

This results in an update rule similar to the convolutional RBM update rule as presented in Chapter \ref{c:convrbm}.
Thus we can simply take the mean of the weight changes and apply it to all the weights, which is equivalent to just taking the average of the new individual weights. 

\paragraph{Lateral inhibition} \label{c:latinhib}
In addition, we introduce fixed negative connections between neurons in the hidden layers (which is more biologically plausible \cite{King2013}, see a simplified sample in Fig. \ref{fig:bminhib}).
This removes one advantage of RBMs since the hidden units are no longer independent, which makes it harder to sample from the true distributions, but since the network continuously performs sampling steps, the approximation has shown to be sufficient, if the weights are not too strong and prevent changes of different modes (see Chapter \ref{c:latinhibexp} and Fig. \ref{fig:dbnmixing} ).

\begin{figure}
	\centering
    	\includegraphics[width=0.4\textwidth]{imgs/lateral_inhib.png} 
    \caption{A (restricted) Boltzmann machine with lateral inhibition in the top layer.}
	\label{fig:bminhib}
\end{figure}


Connecting neurons to neurons on a similar position in other feature maps appears to make the features more discriminative and less correlated.
This also poses some similarities to adding a negative structured bias to the hidden units, which has shown to result in better features \cite{DBLP:journals/corr/KheradpishehGTM16, LeCun, NorouziM2009}.

An intuitive interpretation is that if one feature reacts to a certain input it will be highly active and prevent the others from being active as well and thus prevent them from learning the same features.  

\subsection{Spiking DBNs} \label{c:spikingdbn}

To build up a spiking DBN, we train the convolutional BMs layer-wise and forward the input of the previous layer to the next layer.

Either replacing the symmetric bidirectional synapses with forward only synapses or keeping them bidirected did not show any significant differences (see Fig. \ref{fig:spikingdbnarch} ). 
To be exact the spiking DBN with bidirectional synapses in the bottom RBMs is a mixture between a deep belief network and a deep Boltzmann machine, since the single Boltzmann machines are still bidirectional connected and only the hidden layer activations are forwarded in a directed manner to the next Boltzmann machines (see Fig. \ref{fig:spikedbn}).

Due to some top-down influences, when stacking a new BM on the trained BM directly, the hidden distribution gets distorted and unfit input for the new BM to be trained on. 
To solve this in our case, we use forward connections from the hidden layer of the bottom BM to the visible layer of a new two layered top BM. 
An approach to predict the influences of the top RBM in advance like Salakhutdinovs DBMs by doubling the weights or joint training may sound promising and could be an area for further research \cite{salakhutdinov2009deep}.

\begin{figure}[h]
	\centering
    	\includegraphics[width=0.4\textwidth]{imgs/spike_dbm.png} 
    \caption[Structure of a deep belief network trained with eCD.]{The structure of a deep belief network trained with eCD. The spiking DBN consists of single Boltzmann machines stacked on top of each others. In contrast to artificial DBNs, the Boltzmann machines are still bidirection connected, and two Boltzmann machines are stacked up by forwarding the activations of the hidden layer in the bottom RBM to the visible layer in the top RBM.}
	\label{fig:spikedbn}
\end{figure}

