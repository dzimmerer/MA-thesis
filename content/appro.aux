\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Approach}{41}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Convolutional architecture in spiking neural network}{41}{section.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Conversion}{41}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Conv DBNs}{41}{subsection.4.2.1}}
\newlabel{fig:sub1}{{4.1a}{42}{A singe receptive field over 4 neurons\relax }{figure.caption.83}{}}
\newlabel{sub@fig:sub1}{{a}{42}{A singe receptive field over 4 neurons\relax }{figure.caption.83}{}}
\newlabel{fig:sub2}{{4.1b}{42}{Four receptive fields with shared weights.\relax }{figure.caption.83}{}}
\newlabel{sub@fig:sub2}{{b}{42}{Four receptive fields with shared weights.\relax }{figure.caption.83}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Receptive fields over 4 neurons. In (a) the top layer neuron (red), is only connected to a some neurons in close vicinity to each other. In (b) four receptive fields are next to each other with a stride of two and shared weights. The top layer neurons have a similar topology as their receptive fields. Such a structure can give similar results to conventional more dimensional convolution. \relax }}{42}{figure.caption.83}}
\newlabel{fig:receptfields}{{4.1}{42}{Receptive fields over 4 neurons. In (a) the top layer neuron (red), is only connected to a some neurons in close vicinity to each other. In (b) four receptive fields are next to each other with a stride of two and shared weights. The top layer neurons have a similar topology as their receptive fields. Such a structure can give similar results to conventional more dimensional convolution. \relax }{figure.caption.83}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces A common structure of a deep belief network, build up of RBMs used for classification. The first layers transform the input data/ pixel image and can extract features and the top layer RBM is used for association the data with the correct label.\relax }}{42}{figure.caption.84}}
\newlabel{fig:dbnmnist}{{4.2}{42}{A common structure of a deep belief network, build up of RBMs used for classification. The first layers transform the input data/ pixel image and can extract features and the top layer RBM is used for association the data with the correct label.\relax }{figure.caption.84}{}}
\newlabel{fig:sub1}{{4.3a}{43}{Structure of a spiking CNN with LIF neurons.\relax }{figure.caption.85}{}}
\newlabel{sub@fig:sub1}{{a}{43}{Structure of a spiking CNN with LIF neurons.\relax }{figure.caption.85}{}}
\newlabel{fig:sub2}{{4.3b}{43}{Structure of a spiking DBN with LIF neurons.\relax }{figure.caption.85}{}}
\newlabel{sub@fig:sub2}{{b}{43}{Structure of a spiking DBN with LIF neurons.\relax }{figure.caption.85}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Different converted structures of a CNN and a DBN. The CNN is converted to a spiking network, by simply using simple LIF neurons (white) (a). In the DBN (b) the LIF neurons are put in a high conductance state (black), by inserting high frequency Poisson noise. The weights are scaled accordingly to fit the activations of the neurons.\relax }}{43}{figure.caption.85}}
\newlabel{fig:converted}{{4.3}{43}{Different converted structures of a CNN and a DBN. The CNN is converted to a spiking network, by simply using simple LIF neurons (white) (a). In the DBN (b) the LIF neurons are put in a high conductance state (black), by inserting high frequency Poisson noise. The weights are scaled accordingly to fit the activations of the neurons.\relax }{figure.caption.85}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Conversion}{43}{subsection.4.2.2}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Conversion as CNN}{43}{section*.86}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Conversion with conductance-based LIF}{43}{section*.87}}
\newlabel{fig:sub1}{{4.4a}{44}{Firing of a HCS COBA LIF neuron.\relax }{figure.caption.88}{}}
\newlabel{sub@fig:sub1}{{a}{44}{Firing of a HCS COBA LIF neuron.\relax }{figure.caption.88}{}}
\newlabel{fig:sub2}{{4.4b}{44}{Input output transfer function of a HCS COBA LIF neuron.\relax }{figure.caption.88}{}}
\newlabel{sub@fig:sub2}{{b}{44}{Input output transfer function of a HCS COBA LIF neuron.\relax }{figure.caption.88}{}}
\newlabel{fig:sub2}{{4.4c}{44}{omparison between a HCS COBA LIF neuron sampling and the true distribution in a RBM.\relax }{figure.caption.88}{}}
\newlabel{sub@fig:sub2}{{c}{44}{omparison between a HCS COBA LIF neuron sampling and the true distribution in a RBM.\relax }{figure.caption.88}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Properties of a conductance based LIF neuron, put into a high conductance state with high frequency input noise. The neuron fires with a probability of $0.5$ given no additional input (a). With input the input output transfer function approximates a sigmoid function (b). A network with such neurons is able to sample in a Boltzmann distribution similar to original distribution (c). \relax }}{44}{figure.caption.88}}
\newlabel{fig:cobahcs}{{4.4}{44}{Properties of a conductance based LIF neuron, put into a high conductance state with high frequency input noise. The neuron fires with a probability of $0.5$ given no additional input (a). With input the input output transfer function approximates a sigmoid function (b). A network with such neurons is able to sample in a Boltzmann distribution similar to original distribution (c). \relax }{figure.caption.88}{}}
\newlabel{fig:sub1}{{4.5a}{45}{Firing of a HCS CUBA LIF neuron.\relax }{figure.caption.90}{}}
\newlabel{sub@fig:sub1}{{a}{45}{Firing of a HCS CUBA LIF neuron.\relax }{figure.caption.90}{}}
\newlabel{fig:sub2}{{4.5b}{45}{Input output transfer function of a HCS CUBA LIF neuron.\relax }{figure.caption.90}{}}
\newlabel{sub@fig:sub2}{{b}{45}{Input output transfer function of a HCS CUBA LIF neuron.\relax }{figure.caption.90}{}}
\newlabel{fig:sub2}{{4.5c}{45}{Comparison between a HCS CUBA LIF neuron sampling and the true distribution in a RBM.\relax }{figure.caption.90}{}}
\newlabel{sub@fig:sub2}{{c}{45}{Comparison between a HCS CUBA LIF neuron sampling and the true distribution in a RBM.\relax }{figure.caption.90}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Properties of a current based LIF neuron, put into a simulated high conductance state by high frequency input noise and a artificial high membrane conductance. The neuron also fires with a probability of $0.5$ given no additional input (a) and with input the input output transfer function approximates a sigmoid function (b). A network with such neurons can also to sample in a Boltzmann distribution similar to original distribution (c).\relax }}{45}{figure.caption.90}}
\newlabel{fig:cubahcs}{{4.5}{45}{Properties of a current based LIF neuron, put into a simulated high conductance state by high frequency input noise and a artificial high membrane conductance. The neuron also fires with a probability of $0.5$ given no additional input (a) and with input the input output transfer function approximates a sigmoid function (b). A network with such neurons can also to sample in a Boltzmann distribution similar to original distribution (c).\relax }{figure.caption.90}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Conversion with current-based LIF}{45}{section*.89}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}eCD}{45}{section.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The five phases for a data sample in the adapted eCD algorithm. \relax }}{46}{figure.caption.91}}
\newlabel{fig:ecd5}{{4.6}{46}{The five phases for a data sample in the adapted eCD algorithm. \relax }{figure.caption.91}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces A (restricted) Boltzmann machine with lateral inhibition in the top layer.\relax }}{47}{figure.caption.93}}
\newlabel{fig:bminhib}{{4.7}{47}{A (restricted) Boltzmann machine with lateral inhibition in the top layer.\relax }{figure.caption.93}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Convolution in eCD}{47}{subsection.4.3.1}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Lateral inhibition}{47}{section*.92}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Spiking DBNs}{48}{subsection.4.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces The structure of a deep belief network trained with eCD. The DBN consists of single Boltzmann machines stacked on top of each others. In contrast to analog DBNs, the Boltzmann machines are still bidirection connected, and two Boltzmann machines are stacked up by forwarding the activations of the hidden layer in the bottom RBM to the visible layer in the top RBM.\relax }}{49}{figure.caption.94}}
\newlabel{fig:spikedbn}{{4.8}{49}{The structure of a deep belief network trained with eCD. The DBN consists of single Boltzmann machines stacked on top of each others. In contrast to analog DBNs, the Boltzmann machines are still bidirection connected, and two Boltzmann machines are stacked up by forwarding the activations of the hidden layer in the bottom RBM to the visible layer in the top RBM.\relax }{figure.caption.94}{}}
\@setckpt{content/appro}{
\setcounter{page}{50}
\setcounter{equation}{0}
\setcounter{enumi}{5}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{8}
\setcounter{table}{0}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{NAT@ctr}{0}
\setcounter{parentequation}{0}
\setcounter{@todonotes@numberoftodonotes}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{8}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{Item}{27}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{28}
\setcounter{section@level}{2}
}
