
@misc{sampleFD,
  title = {MCMC sampling for dummies},
  howpublished = {\url{http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/}},
  note = {Accessed: 2016-11-22}
}

@misc{neuronImg,
  title = {File:Neuron Hand-tuned.svg - Wikipedia},
  howpublished = {\url{http://en.wikipedia.org/wiki/File:Neuron_Hand-tuned.svg}},
  note = {Accessed: 2016-11-22}
}

@misc{perceptronImg,
  title = {Fgoml/perceptron at master github.com},
  howpublished = {\url{https://github.com/cdipaolo/goml/tree/master/perceptron}},
  note = {Accessed: 2016-11-22}
}

@misc{mlpImg,
  title = {CS231n Convolutional Neural Networks for Visual Recognition},
  howpublished = {\url{http://cs231n.github.io/neural-networks-1/}},
  note = {Accessed: 2016-11-22}
}

@misc{DBNImg,
  title = {Deep Belief Networks - LISA Wiki -- PascalLamblin },
  howpublished = {\url{http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/DeepBeliefNetworks}},
  note = {Accessed: 2016-11-22}
}

@misc{boltzImg,
  title = {Boltzmann Machines},
  howpublished = {\url{http://gorayni.blogspot.de/2014/06/boltzmann-machines.html}},
  note = {Accessed: 2016-11-22}
}

@misc{rbmImg,
  title = {Restricted Boltzmann Machines},
  howpublished = {\url{http://www.todaysoftmag.com/article/747/restricted-boltzmann-machines}},
  note = {Accessed: 2016-11-22}
}

@misc{cdImg,
  title = {Neural networks: Restricted Boltzmann machine - contrastive divergence -- Hugo Larochelle},
  howpublished = {\url{http://info.usherbrooke.ca/hlarochelle/neural_networks/content.html}},
  note = {Accessed: 2016-11-22}
}

@misc{convImg,
  title = {Convolution arithmetic},
  howpublished = {\url{https://github.com/vdumoulin/conv_arithmetic}},
  note = {Accessed: 2016-11-22}
}

@misc{theanoRBM,
  title = {Restricted Boltzmann Machines (RBM) - DeepLearning 0.1 documentation},
  howpublished = {\url{http://deeplearning.net/tutorial/rbm.html}},
  note = {Accessed: 2016-11-22}
}





@inproceedings{cnnarchImg,
  title={Convolutional networks and applications in vision.},
  author={LeCun, Yann and Kavukcuoglu, Koray and Farabet, Cl{\'e}ment and others}
}

@inproceedings{heikoMA,
author = {Heiko Donat},
title = {{Towards Grasping with Spiking Neural Networks for an Anthropomorphic Robot Hand.}}
}

@article{DBLP:journals/corr/KheradpishehGTM16,
  author    = {Saeed Reza Kheradpisheh and
               Mohammad Ganjtabesh and
               Simon J. Thorpe and
               Timoth{\'{e}}e Masquelier},
  title     = {STDP-based spiking deep neural networks for object recognition},
  journal   = {CoRR},
  volume    = {abs/1611.01421},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.01421},
  timestamp = {Thu, 01 Dec 2016 19:32:08 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/KheradpishehGTM16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}


@article{sillito2006always,
  title={Always returning: feedback and sensory processing in visual cortex and thalamus},
  author={Sillito, Adam M and Cudeiro, Javier and Jones, Helen E},
  journal={Trends in neurosciences},
  volume={29},
  number={6},
  pages={307--316},
  year={2006},
  publisher={Elsevier}
}

@ARTICLE{Zorzi2013Modeling,
author={Zorzi, Marco and Testolin, Alberto and Stoianov, Ivilin},   
title={Modeling language and cognition with deep unsupervised learning: a tutorial overview},      
journal={Frontiers in Psychology},      
volume={4},     
pages={515},     
year={2013},        
url={http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00515},       	
doi={10.3389/fpsyg.2013.00515},      	
issn={1664-1078},   
abstract={Deep unsupervised learning in stochastic recurrent neural networks with many layers of hidden units is a recent breakthrough in neural computation research. These networks build a hierarchy of progressively more complex distributed representations of the sensory data by fitting a hierarchical generative model. In this article we discuss the theoretical foundations of this approach and we review key issues related to training, testing and analysis of deep networks for modeling language and cognitive processing. The classic letter and word perception problem of McClelland and Rumelhart (1981) is used as a tutorial example to illustrate how structured and abstract representations may emerge from deep generative learning. We argue that the focus on deep architectures and generative (rather than discriminative) learning represents a crucial step forward for the connectionist modeling enterprise, because it offers a more plausible model of cortical learning as well as way to bridge the gap between emergentist connectionist models and structured Bayesian models of cognition.}
}

@book{bishop2013pattern,
author = {Bishop, C M},
isbn = {9788132209065},
publisher = {Springer},
series = {Information science and statistics},
title = {{Pattern Recognition and Machine Learning}},
url = {https://books.google.de/books?id=HL4HrgEACAAJ},
year = {2013}
}
@article{Scellier2016a,
abstract = {This work follows Bengio and Fischer (2015) in which theoretical foundations were laid to show how iterative in-ference can backpropagate error signals. Neurons move their activations towards configurations corresponding to lower energy and smaller prediction error: a new observation creates a perturbation at visible neurons that propagates into hid-den layers, with these propagated perturbations corresponding to the back-propagated gradient. This avoids the need for a lengthy relaxation in the positive phase of training (when both inputs and targets are observed), as was believed with previous work on fixed-point recurrent networks. We show experimentally that energy-based neural networks with sev-eral hidden layers can be trained at discriminative tasks by using iterative inference and an STDP-like learning rule. The main result of this paper is that we can train neural networks with 1, 2 and 3 hidden layers on the permutation-invariant MNIST task and get the training error down to 0.00{\%}. The results presented here make it more biologically plausible that a mechanism similar to back-propagation may take place in brains in order to achieve credit assignment in deep networks. The paper also discusses some of the remaining open problems to achieve a biologically plausible implementation of backprop in brains.},
archivePrefix = {arXiv},
arxivId = {1602.05179},
author = {Scellier, Benjamin and Bengio, Yoshua},
eprint = {1602.05179},
file = {:home/david/Documents/papers/1602.05179v4.pdf:pdf},
title = {{Equilibrium Propagation: Bridging the Gap Between Energy-Based Models and Backpropagation Benjamin}},
year = {2016}
}
@inproceedings{salakhutdinov2009deep,
author = {Salakhutdinov, Ruslan and Hinton, Geoffrey E},
title = {{Deep Boltzmann Machines.}}
}
@article{hinton2010practical,
author = {Hinton, Geoffrey},
journal = {Momentum},
number = {1},
pages = {926},
title = {{A practical guide to training restricted Boltzmann machines}},
volume = {9},
year = {2010}
}
@misc{hebb19680,
author = {Hebb, Donald},
publisher = {Wiley, New York},
title = {{The organization of behavior}},
year = {1968}
}
@book{murphy2012machine,
author = {Murphy, K P},
isbn = {9780262304320},
publisher = {MIT Press},
series = {Adaptive computation and machine learning},
title = {{Machine Learning: A Probabilistic Perspective}},
url = {https://books.google.de/books?id=RC43AgAAQBAJ},
year = {2012}
}
@article{vincent2010stacked,
author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
journal = {Journal of Machine Learning Research},
number = {Dec},
pages = {3371--3408},
title = {{Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion}},
volume = {11},
year = {2010}
}
@article{DBLP:journals/corr/ScellierB16,
author = {Scellier, Benjamin and Bengio, Yoshua},
journal = {CoRR},
title = {{Towards a Biologically Plausible Backprop}},
url = {http://arxiv.org/abs/1602.05179},
volume = {abs/1602.05179},
year = {2016}
}
@inproceedings{jin2008efficient,
author = {Jin, Xin and Furber, Steve B and Woods, John V},
booktitle = {2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)},
organization = {IEEE},
pages = {2812--2819},
title = {{Efficient modelling of spiking neural networks on a scalable chip multiprocessor}},
year = {2008}
}
@article{Pfeil1311,
author = {Pfeil, Thomas and Gr{\"{u}}bl, Andreas and Jeltsch, Sebastian and M{\"{u}}ller, Eric and M{\"{u}}ller, Paul and Petrovici, Mihai A and Schmuker, Michael and Br{\"{u}}derle, Daniel and Schemmel, Johannes and Meier, Karlheinz},
journal = {Frontiers in Neuroscience},
keywords = {spikey},
mendeley-tags = {spikey},
pages = {11},
title = {{Six networks on a universal neuromorphic computing substrate}},
volume = {7},
year = {2013}
}
@article{serrano2013128,
author = {Serrano-Gotarredona, Teresa and Linares-Barranco, Bernab{\'{e}}},
journal = {IEEE Journal of Solid-State Circuits},
keywords = {dvs},
mendeley-tags = {dvs},
number = {3},
pages = {827--838},
publisher = {IEEE},
title = {{A 128 128 1.5 contrast sensitivity 0.9 FPN 3 mirco s latency 4 mW asynchronous frame-free dynamic vision sensor using transimpedance preamplifiers}},
volume = {48},
year = {2013}
}
@article{lecun-mnisthandwrittendigit-2010,
author = {LeCun, Yann and Cortes, Corinna},
howpublished = {http://yann.lecun.com/exdb/mnist/},
keywords = {MSc {\_}checked character{\_}recognition mnist network n},
title = {{MNIST handwritten digit database}},
url = {http://yann.lecun.com/exdb/mnist/},
year = {2010}
}
@article{10.3389/neuro.11.005.2008,
abstract = {"Brian" is a new simulator for spiking neural networks, written in Python (http://brian.di.ens.fr). It is an intuitive and highly flexible tool for rapidly developing new models, especially networks of single-compartment neurons. In addition to using standard types of neuron models, users can define models by writing arbitrary differential equations in ordinary mathematical notation. Python scientific libraries can also be used for defining models and analysing data. Vectorisation techniques allow efficient simulations despite the overheads of an interpreted language. Brian will be especially valuable for working on non-standard neuron models not easily covered by existing software, and as an alternative to using Matlab or C for simulations. With its easy and intuitive syntax, Brian is also very well suited for teaching computational neuroscience.},
author = {Goodman, Dan and Brette, Romain},
doi = {10.3389/neuro.11.005.2008},
issn = {1662-5196},
journal = {Frontiers in Neuroinformatics},
pages = {5},
title = {{Brian: a simulator for spiking neural networks in Python}},
url = {http://journal.frontiersin.org/article/10.3389/neuro.11.005.2008},
volume = {2},
year = {2008}
}
@article{10.3389/neuro.11.011.2008,
abstract = {Computational neuroscience has produced a diversity of software for simulations of networks of spiking neurons, with both negative and positive consequences. On the one hand, each simulator uses its own programming or configuration language, leading to considerable difficulty in porting models from one simulator to another. This impedes communication between investigators and makes it harder to reproduce and build on the work of others. On the other hand, simulation results can be cross-checked between different simulators, giving greater confidence in their correctness, and each simulator has different optimizations, so the most appropriate simulator can be chosen for a given modelling task. A common programming interface to multiple simulators would reduce or eliminate the problems of simulator diversity while retaining the benefits. PyNN is such an interface, making it possible to write a simulation script once, using the Python programming language, and run it without modification on any supported simulator (currently NEURON, NEST, PCSIM, Brian and the Heidelberg VLSI neuromorphic hardware). PyNN increases the productivity of neuronal network modelling by providing high-level abstraction, by promoting code sharing and reuse, and by providing a foundation for simulator-agnostic analysis, visualization, and data-management tools. PyNN increases the reliability of modelling studies by making it much easier to check results on multiple simulators. PyNN is open-source software and is available from http://neuralensemble.org/PyNN.},
author = {Davison, Andrew and Br{\"{u}}derle, Daniel and Eppler, Jochen and Kremkow, Jens and Muller, Eilif and Pecevski, Dejan and Perrinet, Laurent and Yger, Pierre},
doi = {10.3389/neuro.11.011.2008},
issn = {1662-5196},
journal = {Frontiers in Neuroinformatics},
pages = {11},
title = {{PyNN: a common interface for neuronal network simulators}},
url = {http://journal.frontiersin.org/article/10.3389/neuro.11.011.2008},
volume = {2},
year = {2009}
}
@article{Gewaltig:NEST,
author = {Gewaltig, Marc-Oliver and Diesmann, Markus},
journal = {Scholarpedia},
number = {4},
pages = {1430},
title = {{NEST (NEural Simulation Tool)}},
volume = {2},
year = {2007}
}
@article{2016arXiv160502688full,
author = {Al-Rfou, Rami and Alain, Guillaume and Almahairi, Amjad and Angermueller, Christof and Bahdanau, Dzmitry and Ballas, Nicolas and Bastien, Fr{\'{e}}d{\'{e}}ric and Bayer, Justin and Belikov, Anatoly and Belopolsky, Alexander and Bengio, Yoshua and Bergeron, Arnaud and Bergstra, James and Bisson, Valentin and {Bleecher Snyder}, Josh and Bouchard, Nicolas and Boulanger-Lewandowski, Nicolas and Bouthillier, Xavier and de Br{\'{e}}bisson, Alexandre and Breuleux, Olivier and Carrier, Pierre-Luc and Cho, Kyunghyun and Chorowski, Jan and Christiano, Paul and Cooijmans, Tim and C{\^{o}}t{\'{e}}, Marc-Alexandre and C{\^{o}}t{\'{e}}, Myriam and Courville, Aaron and Dauphin, Yann N and Delalleau, Olivier and Demouth, Julien and Desjardins, Guillaume and Dieleman, Sander and Dinh, Laurent and Ducoffe, M{\'{e}}lanie and Dumoulin, Vincent and {Ebrahimi Kahou}, Samira and Erhan, Dumitru and Fan, Ziye and Firat, Orhan and Germain, Mathieu and Glorot, Xavier and Goodfellow, Ian and Graham, Matt and Gulcehre, Caglar and Hamel, Philippe and Harlouchet, Iban and Heng, Jean-Philippe and Hidasi, Bal{\'{a}}zs and Honari, Sina and Jain, Arjun and Jean, S{\'{e}}bastien and Jia, Kai and Korobov, Mikhail and Kulkarni, Vivek and Lamb, Alex and Lamblin, Pascal and Larsen, Eric and Laurent, C{\'{e}}sar and Lee, Sean and Lefrancois, Simon and Lemieux, Simon and L{\'{e}}onard, Nicholas and Lin, Zhouhan and Livezey, Jesse A and Lorenz, Cory and Lowin, Jeremiah and Ma, Qianli and Manzagol, Pierre-Antoine and Mastropietro, Olivier and McGibbon, Robert T and Memisevic, Roland and van Merri{\"{e}}nboer, Bart and Michalski, Vincent and Mirza, Mehdi and Orlandi, Alberto and Pal, Christopher and Pascanu, Razvan and Pezeshki, Mohammad and Raffel, Colin and Renshaw, Daniel and Rocklin, Matthew and Romero, Adriana and Roth, Markus and Sadowski, Peter and Salvatier, John and Savard, Fran{\c{c}}ois and Schl{\"{u}}ter, Jan and Schulman, John and Schwartz, Gabriel and Serban, Iulian Vlad and Serdyuk, Dmitriy and Shabanian, Samira and Simon, {\'{E}}tienne and Spieckermann, Sigurd and Subramanyam, S Ramana and Sygnowski, Jakub and Tanguay, J{\'{e}}r{\'{e}}mie and van Tulder, Gijs and Turian, Joseph and Urban, Sebastian and Vincent, Pascal and Visin, Francesco and de Vries, Harm and Warde-Farley, David and Webb, Dustin J and Willson, Matthew and Xu, Kelvin and Xue, Lijun and Yao, Li and Zhang, Saizheng and Zhang, Ying},
journal = {arXiv e-prints},
keywords = {Computer Science - Learning,Computer Science - Mathematical Software,Computer Science - Symbolic Computation},
title = {{Theano: A Python framework for fast computation of mathematical expressions}},
url = {http://arxiv.org/abs/1605.02688},
volume = {abs/1605.0},
year = {2016}
}
@misc{Hodgkin1952,
author = {Hodgkin, A L and Huxley, A F},
booktitle = {The Journal of Physiology},
issn = {0022-3751 (Print)},
language = {eng},
month = {aug},
number = {4},
pages = {500--544},
pmid = {12991237},
title = {{A quantitative description of membrane current and its application to conduction and excitation in nerve}},
volume = {117},
year = {1952}
}
@article{griffiths2008bayesian,
author = {Griffiths, Thomas L and Kemp, Charles and Tenenbaum, Joshua B},
publisher = {Cambridge University Press},
title = {{Bayesian models of cognition}},
year = {2008}
}
@article{yang2007probabilistic,
author = {Yang, Tianming and Shadlen, Michael N},
journal = {Nature},
number = {7148},
pages = {1075--1080},
publisher = {Nature Publishing Group},
title = {{Probabilistic reasoning by neurons}},
volume = {447},
year = {2007}
}
@article{lee2003hierarchical,
author = {Lee, Tai Sing and Mumford, David},
journal = {JOSA A},
number = {7},
pages = {1434--1448},
publisher = {Optical Society of America},
title = {{Hierarchical Bayesian inference in the visual cortex}},
volume = {20},
year = {2003}
}
@inproceedings{norouzi2009stacks,
author = {Norouzi, Mohammad and Ranjbar, Mani and Mori, Greg},
booktitle = {Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on},
keywords = {CRBM},
mendeley-tags = {CRBM},
organization = {IEEE},
pages = {2735--2742},
title = {{Stacks of convolutional restricted boltzmann machines for shift-invariant feature learning}},
year = {2009}
}
@article{desjardins2008empirical,
author = {Desjardins, Guillaume and Bengio, Yoshua},
journal = {DIRO, Universit{\{}{\'{e}}{\}} de Montr{\{}{\'{e}}{\}}al},
keywords = {CRBM},
mendeley-tags = {CRBM},
pages = {1--13},
title = {{Empirical evaluation of convolutional RBMs for vision}},
year = {2008}
}
@article{abbott1999lapicque,
author = {Abbott, Larry F},
journal = {Brain research bulletin},
number = {5},
pages = {303--304},
publisher = {Elsevier},
title = {{Lapicque's introduction of the integrate-and-fire model neuron (1907)}},
volume = {50},
year = {1999}
}
@article{hinton1995wake,
author = {Hinton, Geoffrey E and Dayan, Peter and Frey, Brendan J and Neal, Radford M},
journal = {Science},
number = {5214},
pages = {1158},
publisher = {The American Association for the Advancement of Science},
title = {{The" wake-sleep" algorithm for unsupervised neural networks}},
volume = {268},
year = {1995}
}
@article{hinton2009deep,
author = {Hinton, Geoffrey E},
journal = {Scholarpedia},
number = {5},
pages = {5947},
title = {{Deep belief networks}},
volume = {4},
year = {2009}
}
@techreport{smolensky1986information,
author = {Smolensky, Paul},
institution = {DTIC Document},
keywords = {RBM},
mendeley-tags = {RBM},
title = {{Information processing in dynamical systems: Foundations of harmony theory}},
year = {1986}
}
@inproceedings{tieleman2008training,
author = {Tieleman, Tijmen},
booktitle = {Proceedings of the 25th international conference on Machine learning},
organization = {ACM},
pages = {1064--1071},
title = {{Training restricted Boltzmann machines using approximations to the likelihood gradient}},
year = {2008}
}
@article{hinton2002training,
author = {Hinton, Geoffrey E},
journal = {Neural computation},
number = {8},
pages = {1771--1800},
publisher = {MIT Press},
title = {{Training products of experts by minimizing contrastive divergence}},
volume = {14},
year = {2002}
}
@article{ackley1985learning,
author = {Ackley, David H and Hinton, Geoffrey E and Sejnowski, Terrence J},
journal = {Cognitive science},
number = {1},
pages = {147--169},
publisher = {Elsevier},
title = {{A learning algorithm for Boltzmann machines}},
volume = {9},
year = {1985}
}
@article{hopfield1982neural,
author = {Hopfield, John J},
journal = {Proceedings of the national academy of sciences},
number = {8},
pages = {2554--2558},
publisher = {National Acad Sciences},
title = {{Neural networks and physical systems with emergent collective computational abilities}},
volume = {79},
year = {1982}
}
@article{simonyan2014very,
author = {Simonyan, Karen and Zisserman, Andrew},
journal = {arXiv preprint arXiv:1409.1556},
keywords = {stride},
mendeley-tags = {stride},
title = {{Very deep convolutional networks for large-scale image recognition}},
year = {2014}
}
@article{lecun1989backpropagation,
author = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
journal = {Neural computation},
number = {4},
pages = {541--551},
publisher = {MIT Press},
title = {{Backpropagation applied to handwritten zip code recognition}},
volume = {1},
year = {1989}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
issn = {0028-0836},
journal = {Nature},
month = {may},
number = {7553},
pages = {436--444},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
title = {{Deep learning}},
url = {http://dx.doi.org/10.1038/nature14539 http://10.0.4.14/nature14539},
volume = {521},
year = {2015}
}
@techreport{rumelhart1985learning,
author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
institution = {DTIC Document},
title = {{Learning internal representations by error propagation}},
year = {1985}
}
@article{rosenblatt1958perceptron,
author = {Rosenblatt, Frank},
journal = {Psychological review},
number = {6},
pages = {386},
publisher = {American Psychological Association},
title = {{The perceptron: a probabilistic model for information storage and organization in the brain.}},
volume = {65},
year = {1958}
}
@book{Byrne1997,
author = {Byrne, J. H. and Dafny, N},
publisher = {Department of Neurobiology and Anatomy, The University of Texas Medical School at Houston (UTHealth)},
title = {{Neuroscience Online: An Electronic Textbook for the Neurosciences}},
year = {1997}
}
@misc{Markram2012,
author = {Markram, H and Gerstner, W and Sj{\"{o}}str{\"{o}}m, P J},
booktitle = {Frontiers in Synaptic Neuroscience},
doi = {10.3389/fnsyn.2012.00002},
issn = {1663-3563 (Electronic)},
language = {eng},
pmid = {22807913},
title = {{Spike-Timing-Dependent Plasticity: A Comprehensive Overview}},
volume = {4},
year = {2012}
}
@book{gerstner2014neuronal,
author = {Gerstner, W and Kistler, W M and Naud, R and Paninski, L},
isbn = {9781107060838},
publisher = {Cambridge University Press},
title = {{Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition}},
url = {https://books.google.de/books?id=D4j2AwAAQBAJ},
year = {2014}
}
@unpublished{Goodfellow-et-al-2016-Book,
annote = {Book in preparation for MIT Press},
author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
title = {{Deep Learning}},
url = {http://www.deeplearningbook.org},
year = {2016}
}
@article{maass1997networks,
author = {Maass, Wolfgang},
journal = {Neural networks},
number = {9},
pages = {1659--1671},
publisher = {Elsevier},
title = {{Networks of spiking neurons: the third generation of neural network models}},
volume = {10},
year = {1997}
}
@article{bengio2015towards,
author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Lin, Zhouhan},
journal = {arXiv preprint arXiv:1502.04156},
title = {{Towards biologically plausible deep learning}},
year = {2015}
}
@inproceedings{lee2009convolutional,
author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y},
booktitle = {Proceedings of the 26th annual international conference on machine learning},
keywords = {crbm},
mendeley-tags = {crbm},
organization = {ACM},
pages = {609--616},
title = {{Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations}},
year = {2009}
}
@article{hinton2006fast,
author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
journal = {Neural computation},
number = {7},
pages = {1527--1554},
publisher = {MIT Press},
title = {{A fast learning algorithm for deep belief nets}},
volume = {18},
year = {2006}
}
@inproceedings{chen2015deepdriving,
author = {Chen, Chenyi and Seff, Ari and Kornhauser, Alain and Xiao, Jianxiong},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
pages = {2722--2730},
title = {{Deepdriving: Learning affordance for direct perception in autonomous driving}},
year = {2015}
}
@article{levine2016learning,
author = {Levine, Sergey and Pastor, Peter and Krizhevsky, Alex and Quillen, Deirdre},
journal = {arXiv preprint arXiv:1603.02199},
title = {{Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection}},
year = {2016}
}
@article{giusti2016machine,
author = {Giusti, Alessandro and Guzzi, J{\'{e}}r{\^{o}}me and Cire$\backslash$csan, Dan C and He, Fang-Lin and Rodr$\backslash$'$\backslash$iguez, Juan P and Fontana, Flavio and Faessler, Matthias and Forster, Christian and Schmidhuber, J{\"{u}}rgen and {Di Caro}, Gianni and Others},
journal = {IEEE Robotics and Automation Letters},
number = {2},
pages = {661--667},
publisher = {IEEE},
title = {{A machine learning approach to visual perception of forest trails for mobile robots}},
volume = {1},
year = {2016}
}
@article{abdel2014convolutional,
author = {Abdel-Hamid, Ossama and Mohamed, Abdel-Rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
journal = {IEEE/ACM Transactions on audio, speech, and language processing},
number = {10},
pages = {1533--1545},
publisher = {IEEE},
title = {{Convolutional neural networks for speech recognition}},
volume = {22},
year = {2014}
}
@inproceedings{karpathy2014large,
author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh and Leung, Thomas and Sukthankar, Rahul and Fei-Fei, Li},
booktitle = {Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
pages = {1725--1732},
title = {{Large-scale video classification with convolutional neural networks}},
year = {2014}
}
@inproceedings{szegedy2015going,
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {1--9},
title = {{Going deeper with convolutions}},
year = {2015}
}
@incollection{NIPS2012_4824,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
booktitle = {Advances in Neural Information Processing Systems 25},
editor = {Pereira, F and Burges, C J C and Bottou, L and Weinberger, K Q},
pages = {1097--1105},
publisher = {Curran Associates, Inc.},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
year = {2012}
}
@article{Zhou2007,
abstract = {Probabilistic graphical models combine the graph theory and probability theory to give a multivariate statistical modeling. They provide a unified description of uncertainty using probability and complexity using the graphical model. Especially, graphical models provide the following several useful properties: - Graphical models provide a simple and intuitive interpretation of the structures of probabilistic models. On the other hand, they can be used to design and motivate new models. - Graphical models provide additional insights into the properties of the model, including the conditional independence properties. - Complex computations which are required to perform inference and learning in sophisticated models can be expressed in terms of graphical manipulations, in which the underlying mathematical expressions are carried along implicitly. The graphical models have been applied to a large number of fields, including bioinformatics, social science, control theory, image processing, marketing analysis, among others. However, structure learning for graphical models remains an open challenge, since one must cope with a combinatorial search over the space of all possible structures. In this paper, we present a comprehensive survey of the existing structure learning algorithms.},
archivePrefix = {arXiv},
arxivId = {1111.6925},
author = {Zhou, Yang},
eprint = {1111.6925},
file = {:home/david/Documents/papers/1111.6925v1.pdf:pdf},
journal = {arXiv: 1111.6925},
title = {{Structure Learning of Probabilistic Graphical Models : A Comprehensive Survey}},
url = {http://arxiv.org/abs/1111.6925},
year = {2007}
}
@article{Ghahramani2002,
author = {Ghahramani, Zoubin},
doi = {10.1561/2200000001},
file = {:home/david/Documents/papers/graphical-models02.pdf:pdf},
issn = {1935-8237},
journal = {Handbook of Brain Theory and Neural Networks},
number = {2},
pages = {486--490},
title = {{Graphical models : parameter learning}},
year = {2002}
}
@article{Sjostrom2010,
abstract = {Spike Timing Dependent Plasticity (STDP) is a temporally asymmetric form of Hebbian learning induced by tight temporal correlations between the spikes of pre- and postsynaptic neurons. As with other forms of synaptic plasticity, it is widely believed that it underlies learning and information storage in the brain, as well as the development and refinement of neuronal circuits during brain development (e.g. Bi and Poo, 2001; Sj¨ ostr¨ om et al., 2008). With STDP, repeated presynaptic spike arrival a few milliseconds before postsynaptic action potentials leads in many synapse types to long-term potentiation (LTP) of the synapses, whereas repeated spike arrival after postsynaptic spikes leads to long-term depression (LTD) of the same synapse. The change of the synapse plotted as a function of the relative timing of pre- and postsynaptic action potentials is called the STDP function or learning window and varies between synapse types. The rapid change of the STDP function with the relative timing of spikes suggests the possibility of temporal coding schemes on a millisecond time scale.},
author = {Sjostrom, Per Jesper and Gerstner, Wulfram},
doi = {10.1162/089976604773135041},
file = {:home/david/Documents/papers/STDP-Scholarpedia2010.pdf:pdf},
issn = {0899-7667},
journal = {Scholarpedia},
keywords = {Action Potentials,Action Potentials: physiology,Learning,Learning: physiology,Models,Neurological,Neuronal Plasticity,Neuronal Plasticity: physiology},
number = {2},
pages = {1362},
pmid = {15070504},
title = {{Spike-timing-dependent plasticity}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20882297},
volume = {2},
year = {2010}
}
@article{Meftah2013,
abstract = {Artificial neural networks have been well developed so far. First two generations of neural networks have had a lot of successful applications. Spiking Neuron Networks (SNNs) are often referred to as the third generation of neural net- works which have potential to solve problems related to biological stimuli. They derive their strength and interest from an accurate modeling of synaptic interactions between neurons, taking into account the time of spike emission. SNNs overcome the computational power of neural networks made of threshold or sigmoidal units. Based on dynamic event-driven processing, they open up new horizons for developing models with an exponential capacity of memorizing and a strong ability to fast adaptation.Moreover,SNNs add a newdimension, the temporal axis, to the representation capacity and the processing abilities of neural networks. In this chapter, we present how SNN can be applied with efficacy in image cluster- ing, segmentation and edge detection. Results obtained confirm the validity of the approach},
author = {Meftah, Boudjelal and L{\'{e}}zoray, Olivier and Chaturvedi, Soni and Khurshid, Aleefia A. and Benyettou, Abdelkader},
doi = {10.1007/978-3-642-29694-9-20},
file = {:home/david/Documents/papers/Meftah{\_}Turing2012.pdf:pdf},
isbn = {9783642296932},
issn = {1860949X},
journal = {Studies in Computational Intelligence},
pages = {525--544},
title = {{Image processing with spiking neuron networks}},
volume = {427},
year = {2013}
}
@article{Faltin2007,
abstract = {Bayesian networks (BNs), also known as belief net- works (or Bayes nets for short), belong to the fam- ily of probabilistic graphical models (GMs). These graphical structures are used to represent knowledge about an uncertain domain. In particular, each node in the graph represents a random variable, while the edges between the nodes represent probabilistic dependencies among the corresponding random vari- ables. These conditional dependencies in the graph are often estimated by using known statistical and computational methods. Hence, BNs combine princi- ples from graph theory, probability theory, computer science, and statistics. GMs},
author = {Faltin, F and Kenett, R},
doi = {10.1002/wics.48},
file = {:home/david/Documents/papers/BN.pdf:pdf},
isbn = {9780521884389},
issn = {18780326},
journal = {Encyclopedia of Statistics in Quality {\&} Reliability},
number = {1},
pages = {4},
pmid = {21775236},
title = {{Bayesian Networks}},
volume = {1},
year = {2007}
}
@article{Buchanan2010,
abstract = {Synaptic plasticity has historically been investigated most intensely in the hippocampus and therefore it is somewhat surprising that the majority of studies on spike timing-dependent plasticity (STDP) have focused not in the hippocampus but on synapses in the cortex. One of the major reasons for this bias is the relative ease in obtaining paired electrophysiological recordings from synaptically coupled neurons in cortical slices, in comparison to hippocampal slices. Another less obvious reason has been the difficulty in achieving reliable STDP in the hippocampal slice preparation and confusion surrounding the conditions required. The original descriptions of STDP in the hippocampus was performed on paired recordings from neurons in dissociated or slice cultures utilizing single pairs of presynaptic and postsynaptic spikes and were subsequently replicated in acute hippocampal slices. Further work in several laboratories using conditions that more closely replicate the situation in vivo revealed a requirement for multiple postsynaptic spikes that necessarily complicate the absolute timing rules for STDP. Here we review the hippocampal STDP literature focusing on data from acute hippocampal slice preparations and highlighting apparently contradictory results and the variations in experimental conditions that might account for the discrepancies. We conclude by relating the majority of the available experimental data to a model for STDP induction in the hippocampus based on a critical role for postsynaptic Ca(2+) dynamics.},
author = {Buchanan, Katherine A. and Mellor, Jack R.},
doi = {10.3389/fnsyn.2010.00011},
file = {:home/david/Documents/papers/fnsyn{\_}2010{\_}00011.pdf:pdf},
isbn = {ISSN 1663-3563},
issn = {16633563},
journal = {Frontiers in Synaptic Neuroscience},
keywords = {Hippocampus,STDP,Synaptic plasticity},
number = {JUN},
pages = {1--5},
pmid = {21423497},
title = {{The activity requirements for spike timing-dependent plasticity in the hippocampus}},
volume = {2},
year = {2010}
}
@article{Heeger2000,
abstract = {In the cortex, the timing of successive action potentials is highly irregular. The interpretation of this irregularity has led to two divergent views of cortical organization. On the one hand, the irregularity might arise from stochastic forces. If so, the irregular interspike interval reflects a random process and implies that an instantaneous estimate of the spike rate can be obtained by averaging the pooled responses of many individual neurons. In keeping with this theory, one would expect that the precise timing of individual spikes conveys little information. Alternatively, the irregular ISI may result from precise coincidences of presynaptic events. In this scenario, it is postulated that the timing of spikes, their intervals and patterns can convey information. According to this view, the irregularity of the ISI reflects a rich bandwidth for information transfer. In this handout, we take the former point of view, that the irregular spike interval reflects a random process. We assume that the generation of each spike depends only on an underlying continuous/analog driving signal, r(t),},
author = {Heeger, David},
doi = {10.1.1.37.6580},
file = {:home/david/Documents/papers/poisson.pdf:pdf},
journal = {Handout},
keywords = {Poisson Distribution,spike train variability},
pages = {1--13},
title = {{Poisson Model of Spike Generation}},
year = {2000}
}
@article{Hubel1959,
abstract = {Recordings were made in lightly anaesthesized cats whose retinas were stimulated, singly or together, with light spots of various sizes and shapes. Receptive fields, defined as restricted areas where illumination influenced the firing of a single cortical unit, usually contained mutually antagonistic excitatory and inhibitory regions. Thus a stimulus covering a whole field was relatively ineffective in driving most units. Effective driving of a unit required a stimulus specific in form, size, position, and orientation; based on the arrangement of excitatory and inhibitory areas. About 20{\{}{\%}{\}} of the cortical units studied could be activated from either eye; these were driven from roughly homologous regions of the retinas and summation and antagonism could be shown. (PsycINFO Database Record (c) 2009 APA, all rights reserved)},
author = {Hubel, D H and Wiesel, T N},
doi = {10.1113/jphysiol.2009.174151},
file = {:home/david/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hubel, Wiesel - 1959 - Receptive fields of single neurones in the cat's striate cortex.pdf:pdf},
isbn = {0022-3751 (Print)},
issn = {00223751},
journal = {The Journal of Physiology},
keywords = {CEREBRAL CORTEX/physiology,NEURONS/physiology},
pages = {574--591},
pmid = {14403679},
title = {{Receptive fields of single neurones in the cat's striate cortex.}},
volume = {148},
year = {1959}
}
@book{Petrovici2016,
abstract = {This thesis addresses one of the most fundamental challenges for modern science: how can the brain as a network of neurons process information, how can it create and store internal models of our world, and how can it infer conclusions from ambiguous data? The author addresses these questions with the rigorous language of mathematics and theoretical physics, an approach that requires a high degree of abstraction to transfer results of wet lab biology to formal models. The thesis starts with an in-depth description of the state-of-the-art in theoretical neuroscience, which it subsequently uses as a basis to develop several new and original ideas. Throughout the text, the author connects the form and function of neuronal networks. This is done in order to achieve functional performance of biological brains by transferring their form to synthetic electronics substrates, an approach referred to as neuromorphic computing. The obvious aspect that this transfer can never be perfect but necessarily leads to performance differences is substantiated and explored in detail. The author also introduces a novel interpretation of the firing activity of neurons. He proposes a probabilistic interpretation of this activity and shows by means of formal derivations that stochastic neurons can sample from internally stored probability distributions. This is corroborated by the author's recent findings, which confirm that biological features like the high conductance state of networks enable this mechanism. The author goes on to show that neural sampling can be implemented on synthetic neuromorphic circuits, paving the way for future applications in machine learning and cognitive computing, for example as energy-efficient implementations of deep learning networks. The thesis offers an essential resource for newcomers to the field and an inspiration for scientists working in theoretical neuroscience and the future of computing.},
author = {Petrovici, Mihai Alexandru},
doi = {10.1007/978-3-319-39552-4},
file = {:home/david/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Petrovici - 2016 - Form Versus Function Theory and Models for Neuronal Substrates.pdf:pdf;:home/david/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Petrovici - 2016 - Form Versus Function Theory and Models for Neuronal Substrates(2).pdf:pdf},
isbn = {978-3-319-39551-7},
pages = {374},
title = {{Form Versus Function: Theory and Models for Neuronal Substrates}},
url = {https://www.amazon.com/Form-Versus-Function-Neuronal-Substrates/dp/3319395513/ref=sr{\_}1{\_}106?ie=UTF8{\&}qid=1472180643{\&}sr=8-106{\&}keywords=neuroscience+architecture},
year = {2016}
}
@article{Norouzi2009,
abstract = {In this thesis, we present a method for learning problem-specific hierarchical features specialized for vision applications. Recently, a greedy layerwise learning mechanism has been proposed for tuning parameters of fully connected hierarchical networks. This approach views layers of a network as Restricted Boltzmann Machines (RBM), and trains them separately from the bottom layer upwards. We develop Convolutional RBM (CRBM), an extension of the RBM model in which connections are local and weights are shared to respect the spatial structure of images. We switch between the CRBM and down-sampling layers and stack them on top of each other to build a multilayer hierarchy of alternating filtering and pooling. This framework learns generic features such as oriented edges at the bottom levels and features specific to an object class such as object parts in the top layers. Afterward, we feed the extracted features into a discriminative classifier for recognition. It is experimentally demonstrated that the features automatically learned by our algorithm are effective for object detection, by using them to obtain performance comparable to the state-of-the-art on handwritten digit classification and pedestrian detection.},
archivePrefix = {arXiv},
arxivId = {1203.4416},
author = {Norouzi, Mohammad and Ranjbar, Mani and Mori, Greg},
doi = {10.1109/CVPRW.2009.5206577},
eprint = {1203.4416},
file = {:home/david/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Norouzi M - 2009 - Convolutional restricted Boltzmann machines for feature learning(2).pdf:pdf;:home/david/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Norouzi M - 2009 - Convolutional restricted Boltzmann machines for feature learning(3).pdf:pdf},
isbn = {9781424439935},
issn = {1063-6919},
journal = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
pages = {2735--2742},
title = {{Stacks of convolutional restricted boltzmann machines for shift-invariant feature learning}},
year = {2009}
}
@article{LeCun,
author = {LeCun, Yann},
file = {:home/david/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/LeCun - Unknown - 5 Years From Now, Everyone Will Learn Their Features.pdf:pdf},
title = {{5 Years From Now, Everyone Will Learn Their Features}},
url = {http://www.cs.nyu.edu/{~}yann/talks/lecun-20110823-frontiers-in-vision.pdf}
}
@article{Bengio2009,
abstract = {We study an expansion of the log likelihood in undirected graphical models such as the restricted Boltzmann machine (RBM), where each term in the expansion is associated with a sample in a Gibbs chain alternating between two random variables (the visible vector and the hidden vector in RBMs). We are particularly interested in estimators of the gradient of the log likelihood obtained through this expansion. We show that its residual term converges to zero, justifying the use of a truncation--running only a short Gibbs chain, which is the main idea behind the contrastive divergence (CD) estimator of the log-likelihood gradient. By truncating even more, we obtain a stochastic reconstruction error, related through a mean-field approximation to the reconstruction error often used to train autoassociators and stacked autoassociators. The derivation is not specific to the particular parametric forms used in RBMs and requires only convergence of the Gibbs chain. We present theoretical and empirical evidence linking the number of Gibbs steps k and the magnitude of the RBM parameters to the bias in the CD estimator. These experiments also suggest that the sign of the CD estimator is correct most of the time, even when the bias is large, so that CD-k is a good descent direction even for small k.},
archivePrefix = {arXiv},
arxivId = {submit/0500581},
author = {Bengio, Yoshua and Delalleau, Olivier},
doi = {10.1162/neco.2008.11-07-647},
eprint = {0500581},
file = {:home/david/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio, Delalleau - 2009 - Justifying and generalizing contrastive divergence.pdf:pdf},
isbn = {0262195682},
issn = {0899-7667},
journal = {Neural Computation},
number = {6},
pages = {1601--1621},
pmid = {19018704},
primaryClass = {submit},
title = {{Justifying and generalizing contrastive divergence}},
url = {papers3://publication/doi/10.1162/neco.2008.11-07-647},
volume = {21},
year = {2009}
}
@article{Nair2010,
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
author = {Nair, Vinod and Hinton, Geoffrey E},
doi = {10.1.1.165.6419},
file = {:home/david/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Nair, Hinton - 2010 - Rectified Linear Units Improve Restricted Boltzmann Machines.pdf:pdf},
isbn = {9781605589077},
issn = {1935-8237},
journal = {Proceedings of the 27th International Conference on Machine Learning},
number = {3},
pages = {807--814},
pmid = {22404682},
title = {{Rectified Linear Units Improve Restricted Boltzmann Machines}},
year = {2010}
}
@article{Woodford2002,
abstract = {These notes describe Contrastive Divergence (CD), an approximate Maximum-Likelihood (ML) learning algorithm proposed by Geoffrey Hinton.},
author = {Woodford, Oliver},
file = {:home/david/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Woodford - 2002 - Notes on Contrastive Divergence.pdf:pdf},
isbn = {978-3-642-04273-7},
journal = {Notes},
number = {Cd},
pages = {1--3},
title = {{Notes on Contrastive Divergence}},
year = {2002}
}
@article{Teh2005,
abstract = {This paper presents a novel physics-based representation of realistic character motion. The dynamical model incorporates several factors of locomotion derived from the biomechanical literature, including relative preferences for using some muscles more than others, elastic mechanisms at joints due to the mechanical properties of tendons, ligaments, and muscles, and variable stiffness at joints depending on the task. When used in a spacetime optimization framework, the parameters of this model define a wide range of styles of natural human movement. Due to the complexity of biological motion, these style parameters are too difficult to design by hand. To address this, we introduce Nonlinear Inverse Optimization, a novel algorithm for estimating optimization parameters from motion capture data. Our method can extract the physical parameters from a single short motion sequence. Once captured, this representation of style is extremely flexible: motions can be generated in the same style but performing different tasks, and styles may be edited to change the physical properties of the body.},
author = {Teh, YeeWhye and Hinton, Geoffrey E.},
doi = {10.1145/1073204.1073314},
file = {:home/david/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Teh, Hinton - 2001 - Rate-coded restricted Boltzmann machines for face recognition.pdf:pdf},
isbn = {0730-0301},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {character animation,inverse optimization,mation,motion style,physics-based ani-},
pages = {1071},
pmid = {16764513},
title = {{Rate-coded Restricted Boltzmann Machines for Face Recognition}},
url = {http://www.cs.toronto.edu/{~}hinton/absps/nips00-ywt.pdf},
volume = {24},
year = {2005}
}
@article{King2013,
abstract = {Sparse coding models of natural scenes can account for several physiological properties of primary visual cortex (V1), including the shapes of simple cell receptive fields (RFs) and the highly kurtotic firing rates of V1 neurons. Current spiking network models of pattern learning and sparse coding require direct inhibitory connections between the excitatory simple cells, in conflict with the physiological distinction between excitatory (glutamatergic) and inhibitory (GABAergic) neurons (Dale's Law). At the same time, the computational role of inhibitory neurons in cortical microcircuit function has yet to be fully explained. Here we show that adding a separate population of inhibitory neurons to a spiking model of V1 provides conformance to Dale's Law, proposes a computational role for at least one class of interneurons, and accounts for certain observed physiological properties in V1. When trained on natural images, this excitatory-inhibitory spiking circuit learns a sparse code with Gabor-like RFs as found in V1 using only local synaptic plasticity rules. The inhibitory neurons enable sparse code formation by suppressing predictable spikes, which actively decorrelates the excitatory population. The model predicts that only a small number of inhibitory cells is required relative to excitatory cells and that excitatory and inhibitory input should be correlated, in agreement with experimental findings in visual cortex. We also introduce a novel local learning rule that measures stimulus-dependent correlations between neurons to support "explaining away" mechanisms in neural coding.},
author = {King, Paul D and Zylberberg, Joel and DeWeese, Michael R},
doi = {10.1523/JNEUROSCI.4188-12.2013},
file = {:home/david/Documents/papers/king{\_}zylberberg{\_}deweese{\_}JNeuro{\_}Dale{\_}SAILnet{\_}2013.pdf:pdf},
isbn = {0270-6474},
issn = {1529-2401},
journal = {The Journal of neuroscience : the official journal of the Society for Neuroscience},
keywords = {Action Potentials,Action Potentials: physiology,Animals,Computer Simulation,Humans,Interneurons,Interneurons: physiology,Learning,Learning: physiology,Models, Neurological,Nerve Net,Nerve Net: cytology,Nerve Net: physiology,Neural Inhibition,Neural Inhibition: physiology,Neural Pathways,Neural Pathways: physiology,Nonlinear Dynamics,Predictive Value of Tests,Statistics as Topic,Visual Cortex,Visual Cortex: cytology},
number = {13},
pages = {5475--85},
pmid = {23536063},
title = {{Inhibitory interneurons decorrelate excitatory cells to drive sparse code formation in a spiking model of V1.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23536063},
volume = {33},
year = {2013}
}
@article{NorouziM2009,
abstract = {In this thesis, we present a method for learning problem-specific hierarchical features specialized for vision applications. Recently, a greedy layerwise learning mechanism has been proposed for tuning parameters of fully connected hierarchical networks. This approach views layers of a network as Restricted Boltzmann Machines (RBM), and trains them separately from the bottom layer upwards. We develop Convolutional RBM (CRBM), an extension of the RBM model in which connections are local and weights are shared to respect the spatial structure of images. We switch between the CRBM and down-sampling layers and stack them on top of each other to build a multilayer hierarchy of alternating filtering and pooling. This framework learns generic features such as oriented edges at the bottom levels and features specific to an object class such as object parts in the top layers. Afterward, we feed the extracted features into a discriminative classifier for recognition. It is experimentally demonstrated that the features automatically learned by our algorithm are effective for object detection, by using them to obtain performance comparable to the state-of-the-art on handwritten digit classification and pedestrian detection.},
author = {{Norouzi M}},
doi = {10.1109/CVPR.2009.5206577},
file = {:home/david/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Norouzi M - 2009 - Convolutional restricted Boltzmann machines for feature learning(3).pdf:pdf},
isbn = {9781424439911},
issn = {1063-6919},
journal = {School of Computing Science-Simon Fraser University},
keywords = {crbm},
mendeley-tags = {crbm},
pages = {2735--2742},
title = {{Convolutional restricted Boltzmann machines for feature learning}},
year = {2009}
}
@article{Neil2013,
author = {Neil, D},
file = {:home/david/Documents/papers/dneil{\_}thesis.pdf:pdf},
number = {October},
title = {{Online Learning in Event-based Restricted Boltzmann Machines}},
year = {2013}
}
@article{Fischer2014,
author = {Fischer, Asja and Igel, Christian},
file = {:home/david/Documents/papers/TRBMAI.pdf:pdf},
keywords = {contrastive divergence learning,gibbs sampling,markov chains,markov random fields,neural networks,parallel tempering,restricted boltzmann machines},
pages = {25--39},
title = {{Training Restricted Boltzmann Machines: An Introduction}},
year = {2014}
}
@article{Neftci2015,
abstract = {Recent studies have shown that synaptic unreliability is a robust and sufficient mechanism for inducing the stochasticity observed in cortex. Here, we introduce the Synaptic Sampling Machine (SSM), a stochastic neural network model that uses synaptic unreliability as a means to stochasticity for sampling. Synaptic unreliability plays the dual role of an efficient mechanism for sampling in neuromorphic hardware, and a regularizer during learning akin to DropConnect. Similar to the original formulation of Boltzmann machines, the SSM can be viewed as a stochastic counterpart of Hopfield networks, but where stochasticity is induced by a random mask over the connections. The SSM is trained to learn generative models with a synaptic plasticity rule implementing an event-driven form of contrastive divergence. We demonstrate this by learning a model of MNIST hand-written digit dataset, and by testing it in recognition and inference tasks. We find that SSMs outperform restricted Boltzmann machines (4.4{\%} error rate vs. 5{\%}), they are more robust to overfitting, and tend to learn sparser representations. SSMs are remarkably robust to weight pruning: removal of more than 80{\%} of the weakest connections followed by cursory re-learning causes only a negligible performance loss on the MNIST task (4.8{\%} error rate). These results show that SSMs offer substantial improvements in terms of performance, power and complexity over existing methods for unsupervised learning in spiking neural networks, and are thus promising models for machine learning in neuromorphic execution platforms.},
archivePrefix = {arXiv},
arxivId = {1511.04484},
author = {Neftci, Emre O. and Pedroni, Bruno U. and Joshi, Siddharth and Al-Shedivat, Maruan and Cauwenberghs, Gert},
eprint = {1511.04484},
file = {:home/david/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neftci et al. - 2015 - Unsupervised Learning in Synaptic Sampling Machines.pdf:pdf},
pages = {1--27},
title = {{Unsupervised Learning in Synaptic Sampling Machines}},
url = {http://arxiv.org/abs/1511.04484},
year = {2015}
}
@article{Lillicrap2014,
abstract = {The brain processes information through many layers of neurons. This deep architecture is representationally powerful, but it complicates learning by making it hard to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame to a neuron by computing exactly how it contributed to an error. To do this, it multiplies error signals by matrices consisting of all the synaptic weights on the neuron's axon and farther downstream. This operation requires a precisely choreographed transport of synaptic weight information, which is thought to be impossible in the brain. Here we present a surprisingly simple algorithm for deep learning, which assigns blame by multiplying error signals by random synaptic weights. We show that a network can learn to extract useful information from signals sent through these random feedback connections. In essence, the network learns to learn. We demonstrate that this new mechanism performs as quickly and accurately as backpropagation on a variety of problems and describe the principles which underlie its function. Our demonstration provides a plausible basis for how a neuron can be adapted using error signals generated at distal locations in the brain, and thus dispels long-held assumptions about the algorithmic constraints on learning in neural circuits.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.0247v1},
author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
eprint = {arXiv:1411.0247v1},
file = {:home/david/Documents/papers/1411.0247.pdf:pdf},
journal = {arXiv:1411.0247 [cs, q-bio]},
pages = {1--27},
title = {{Random feedback weights support learning in deep neural networks}},
url = {http://arxiv.org/abs/1411.0247{\%}5Cnhttp://www.arxiv.org/pdf/1411.0247.pdf},
year = {2014}
}
@article{Bengio2015,
abstract = {The growing probabilities of additional offspring with the beneficial reversal allele for various population sizes, {\$}N{\$}, sequence lengths, {\$}L{\$}, selective advantages, {\$}s{\$}, fitness parameters, {\$}k{\$}, and measuring parameters, {\$}C{\$}, were calculated in an asymmetric sharply-peaked landscape using the decoupled continuous-time mutation-selection model. The growing probability in the stochastic region was inversely proportional to the measuring parameter when {\$}C{\textless}1/Ns{\^{}}*{\$}, bent when {\$}C\backslashapprox 1/Ns{\^{}}*{\$} and became saturated when {\$}C{\textgreater}1/Ns{\^{}}*{\$}, where {\$}s{\^{}}*{\$} is the effective selective advantage. The saturated growing probability in the stochastic region was approximately the effective selective advantage when {\$}C\backslashgg 1/Ns{\^{}}*{\$} and {\$}s{\^{}}*\backslashll 1{\$}. The present study suggests that the growing probability in the stochastic region in an asymmetric sharply-peaked landscape in the decoupled continuous-time mutation-selection model can be described using the theoretical formula for the growing probability in the Moran two-allele model. The selective advantage ratio, which represents the ratio of the effective selective advantage to the selective advantage, does not depend on the population size, selective advantage, measuring parameter, and fitness parameter; instead the selective advantage ratio decreases with increasing the sequence length.},
archivePrefix = {arXiv},
arxivId = {1502.0415},
author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Lin, Zhouhan},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.0415},
file = {:home/david/Documents/papers/1502.04156v2.pdf:pdf},
isbn = {9781467398947},
issn = {0717-6163},
journal = {arXiv preprint arxiv:1502.0415},
pages = {18},
pmid = {15003161},
title = {{Towards Biologically Plausible Deep Learning}},
url = {http://arxiv.org/abs/1502.0415},
year = {2015}
}
@article{Scellier2016,
abstract = {This work follows Bengio and Fischer (2015) in which theoretical foundations were laid to show how iterative inference can backpropagate error signals. Neurons move their activations towards configurations corresponding to lower energy and smaller prediction error: a new observation creates a perturbation at visible neurons that propagates into hidden layers, with these propagated perturbations corresponding to the back-propagated gradient. This avoids the need for a lengthy relaxation in the positive phase of training (when both inputs and targets are observed), as was believed with previous work on fixed-point recurrent networks. We show experimentally that energy-based neural networks with several hidden layers can be trained at discriminative tasks by using iterative inference and an STDP-like learning rule. The main result of this paper is that we can train neural networks with 1, 2 and 3 hidden layers on the permutation-invariant MNIST task and get the training error down to 0.00{\%}. The results presented here make it more biologically plausible that a mechanism similar to back-propagation may take place in brains in order to achieve credit assignment in deep networks. The paper also discusses some of the remaining open problems to achieve a biologically plausible implementation of backprop in brains.},
archivePrefix = {arXiv},
arxivId = {1602.05179},
author = {Scellier, Benjamin and Bengio, Yoshua},
eprint = {1602.05179},
file = {:home/david/Documents/papers/1602.05179v2.pdf:pdf},
journal = {Arxiv},
pages = {1--14},
title = {{Towards a Biologically Plausible Backprop}},
url = {http://arxiv.org/abs/1602.05179},
year = {2016}
}
@article{Petrovici2013,
abstract = {The seemingly stochastic transient dynamics of neocortical circuits observed$\backslash$n in vivo have been hypothesized to represent a signature of ongoing stochastic$\backslash$n inference. In vitro neurons, on the other hand, exhibit a highly deterministic$\backslash$n response to various types of stimulation. We show that an ensemble of$\backslash$n deterministic leaky integrate-and-fire neurons embedded in a spiking noisy$\backslash$n environment can attain the correct firing statistics in order to sample from a$\backslash$n well-defined target distribution. We provide an analytical derivation of the$\backslash$n activation function on the single cell level; for recurrent networks, we$\backslash$n examine convergence towards stationarity in computer simulations and$\backslash$n demonstrate sample-based Bayesian inference in a mixed graphical model. This$\backslash$n establishes a rigorous link between deterministic neuron models and functional$\backslash$n stochastic dynamics on the network level.},
archivePrefix = {arXiv},
arxivId = {arXiv:1311.3211v1},
author = {Petrovici, Mihai a and Bill, Johannes and Bytschok, Ilja and Schemmel, Johannes and Meier, Karlheinz},
eprint = {arXiv:1311.3211v1},
file = {:home/david/Documents/papers/1311.3211.pdf:pdf},
journal = {arXiv:q-bio.NC},
number = {2},
pages = {1--6},
title = {{Stochastic inference with deterministic spiking neurons}},
year = {2013}
}
@article{Buesing2011,
abstract = {The organization of computations in networks of spiking neurons in the brain is still largely unknown, in particular in view of the inherently stochastic features of their firing activity and the experimentally observed trial-to-trial variability of neural systems in the brain. In principle there exists a powerful computational framework for stochastic computations, probabilistic inference by sampling, which can explain a large number of macroscopic experimental data in neuroscience and cognitive science. But it has turned out to be surprisingly difficult to create a link between these abstract models for stochastic computations and more detailed models of the dynamics of networks of spiking neurons. Here we create such a link and show that under some conditions the stochastic firing activity of networks of spiking neurons can be interpreted as probabilistic inference via Markov chain Monte Carlo (MCMC) sampling. Since common methods for MCMC sampling in distributed systems, such as Gibbs sampling, are inconsistent with the dynamics of spiking neurons, we introduce a different approach based on non-reversible Markov chains that is able to reflect inherent temporal processes of spiking neuronal activity through a suitable choice of random variables. We propose a neural network model and show by a rigorous theoretical analysis that its neural activity implements MCMC sampling of a given distribution, both for the case of discrete and continuous time. This provides a step towards closing the gap between abstract functional models of cortical computation and more detailed models of networks of spiking neurons.},
author = {Buesing, Lars and Bill, Johannes and Nessler, Bernhard and Maass, Wolfgang},
doi = {10.1371/journal.pcbi.1002211},
file = {:home/david/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Buesing et al. - 2011 - Neural dynamics as sampling A model for stochastic computation in recurrent networks of spiking neurons.pdf:pdf},
isbn = {1553-7358 (Electronic)$\backslash$n1553-734X (Linking)},
issn = {1553734X},
journal = {PLoS Computational Biology},
number = {11},
pmid = {22096452},
title = {{Neural dynamics as sampling: A model for stochastic computation in recurrent networks of spiking neurons}},
volume = {7},
year = {2011}
}
@article{Diehl2015,
abstract = {Deep neural networks such as Convolutional Networks (ConvNets) and Deep Belief Networks (DBNs) represent the state-of-the-art for many machine learning and computer vision classification problems. To overcome the large computational cost of deep networks, spiking deep networks have recently been proposed, given the specialized hardware now available for spiking neural networks (SNNs). However, this has come at the cost of performance losses due to the conversion from analog neural networks (ANNs) without a notion of time, to sparsely firing, event-driven SNNs. Here we analyze the effects of converting deep ANNs into SNNs with respect to the choice of parameters for spiking neurons such as firing rates and thresholds. We present a set of optimization techniques to minimize performance loss in the conversion process for ConvNets and fully connected deep networks. These techniques yield networks that outperform all previous SNNs on the MNIST database to date, and many networks here are close to maximum performance after only 20 ms of simulated time. The techniques include using rectified linear units (ReLUs) with zero bias during training, and using a new weight normalization method to help regulate firing rates. Our method for converting an ANN into an SNN enables low-latency classification with high accuracies already after the first output spike, and compared with previous SNN approaches it yields improved performance without increased training time. The presented analysis and optimization techniques boost the value of spiking deep networks as an attractive framework for neuromorphic computing platforms aimed at fast and efficient pattern recognition.},
author = {Diehl, Peter U. and Neil, Daniel and Binas, Jonathan and Cook, Matthew and Liu, Shih Chii and Pfeiffer, Michael},
doi = {10.1109/IJCNN.2015.7280696},
file = {:home/david/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Diehl et al. - 2015 - Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing.pdf:pdf},
isbn = {9781479919604},
journal = {Proceedings of the International Joint Conference on Neural Networks},
keywords = {Accuracy,Handheld computers,Neuromorphics,Neurons,Robustness},
title = {{Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing}},
volume = {2015-Septe},
year = {2015}
}
@article{Neftci2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1311.0966v3},
author = {Neftci, Emre and Das, Srinjoy and Pedroni, Bruno and Kreutz-delgado, Kenneth and Cauwenberghs, Gert and Jolla, La and Jolla, La and Jolla, La},
eprint = {arXiv:1311.0966v3},
file = {:home/david/Documents/papers/1311.0966v3.pdf:pdf},
title = {{Event-Driven Contrastive Divergence for Spiking Neuromorphic Systems}},
year = {2013}
}
@article{OConnor2013,
author = {O'Connor, Peter and Pfeiffer, Michael},
doi = {10.3389/fnins.2013.00178},
file = {:home/david/Documents/papers/oconnor2013.pdf:pdf},
keywords = {deep,deep belief networks,generative model,learning,sensory fusion,sili,silicon cochlea,silicon retina,spiking neural network},
number = {October},
pages = {1--13},
title = {{Real-time classification and sensor fusion with a spiking deep belief network}},
volume = {7},
year = {2013}
}
@article{Cao2014,
author = {Cao, Yongqiang and Chen, Yang and Khosla, Deepak},
doi = {10.1007/s11263-014-0788-3},
file = {:home/david/Documents/papers/Cao, Chen, Khosla - 2015 - Spiking Deep Convolutional Neural Networks for Energy-Efficient Object Recognition.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {Convolutional neural networks,Deep learning,Machine learning,Neuromorphic circuits,Object recognition,Spiking neural networks,and,aurelio ranzato,communicated by marc,convolutional neural networks,deep learning,geoffrey e,hinton,machine learning,neuromorphic circuits,object recognition,spiking neural networks},
month = {nov},
number = {1},
pages = {54--66},
publisher = {Springer US},
title = {{Spiking Deep Convolutional Neural Networks for Energy-Efficient Object Recognition}},
url = {http://dl.acm.org/citation.cfm?id=2776902.2776961 http://link.springer.com/10.1007/s11263-014-0788-3},
volume = {113},
year = {2015}
}
